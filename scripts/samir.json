[
    {
        "internal_id": 86,
        "title": "Joint Inference for Neural Network Depth and Dropout Regularization",
        "abstract": "Dropout regularization methods prune a neural network's pre-determined backbone structure to avoid overfitting. However, a deep model still tends to be poorly calibrated with high confidence on incorrect predictions. We propose a unified Bayesian model selection method to jointly infer the most plausible network depth warranted by data, and perform dropout regularization simultaneously. In particular, to infer network depth we define a beta process over the number of hidden layers which allows it to go to infinity. Layer-wise activation probabilities induced by the beta process modulate neuron activation via binary vectors of a conjugate Bernoulli process. Experiments across domains show that by adapting network depth and dropout regularization to data, our method achieves superior performance comparing to state-of-the-art methods with well-calibrated uncertainty estimates. In continual learning, our method enables neural networks to dynamically evolve their depths to accommodate incrementally available data beyond their initial structures, and alleviate catastrophic forgetting.",
        "authors": [
            "Kishan K C",
            "Rui Li",
            "Mahdi Gilany"
        ],
        "created_time": "29 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/JointInferenceforNeuralNetworkDepthandDropoutRegularization.pdf"
    },
    {
        "internal_id": 87,
        "title": "Overinterpretation reveals image classification model",
        "abstract": "Image classifiers are typically scored on their test set accuracy, but high accuracy can mask a subtle type of model failure. We find that high scoring convolutional neural networks (CNNs) on popular benchmarks exhibit troubling pathologies that allow them to display high accuracy even in the absence of semantically salient features. When a model provides a high-confidence decision without salient supporting input features, we say the classifier has overinterpreted its input, finding too much class-evidence in patterns that appear nonsensical to humans. Here, we demonstrate that neural networks trained on CIFAR-10 and ImageNet suffer from overinterpretation, and we find models on CIFAR-10 make confident predictions even when 95% of input images are masked and humans cannot discern salient features in the remaining pixel-subsets. We introduce Batched Gradient SIS, a new method for discovering sufficient input subsets for complex datasets, and use this method to show the sufficiency of border pixels in ImageNet for training and testing. Although these patterns portend potential model fragility in real-world deployment, they are in fact valid statistical patterns of the benchmark that alone suffice to attain high test accuracy. Unlike adversarial examples, overinterpretation relies upon unmodified image pixels. We find ensembling and input dropout can each help mitigate overinterpretation.",
        "authors": [
            "MIT CSAIL",
            "Siddhartha Jain",
            "MIT CSAIL",
            "Jonas Mueller",
            "David Gifford",
            "MIT CSAIL"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/Overinterpretationrevealsimageclassificationmodelpathologies.pdf"
    },
    {
        "internal_id": 88,
        "title": "Mitigating Covariate Shift in Imitation Learning via Offline Data Without Great Coverage",
        "abstract": "This paper studies offline Imitation Learning (IL) where an agent learns to imitate an expert demonstrator without additional online environment interactions. In- stead, the learner is presented with a static offline dataset of state-action-next state transition triples from a potentially less proficient behavior policy. We introduce Model-based IL from Offline data (MILO): an algorithmic framework that utilizes the static dataset to solve the offline IL problem efficiently both in theory and in practice. In theory, even if the behavior policy is highly sub-optimal compared to the expert, we show that as long as the data from the behavior policy provides sufficient coverage on the expert state-action traces (and with no necessity for a global coverage over the entire state-action space), MILO can provably combat the covariate shift issue in IL. Complementing our theory results, we also demon- strate that a practical implementation of our approach mitigates covariate shift on benchmark MuJoCo continuous control tasks. We demonstrate that with behavior policies whose performances are less than half of that of the expert, MILO still successfully imitates with an extremely low number of expert state-action pairs while traditional offline IL methods such as behavior cloning (BC) fail completely. Source code is provided at https://github.com/jdchang1/milo.",
        "authors": [
            "Jonathan D Chang",
            "Masatoshi Uehara",
            "Dhruv Sreenivas",
            "Rahul Kidambi",
            "Wen Sun"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/MitigatingCovariateShiftinImitationLearningviaOfflineDataWithPartialCoverage.pdf"
    },
    {
        "internal_id": 89,
        "title": "Terra: Imperative-Symbolic Co-Execution of Imperative Deep Learning Programs",
        "abstract": "Imperative programming allows users to implement their deep neural networks (DNNs) easily and has become an essential part of recent deep learning (DL) frame- works. Recently, several systems have been proposed to combine the usability of imperative programming with the optimized performance of symbolic graph execu- tion. Such systems convert imperative Python DL programs to optimized symbolic graphs and execute them. However, they cannot fully support the usability of imper- ative programming. For example, if an imperative DL program contains a Python feature with no corresponding symbolic representation (e.g., third-party library calls or unsupported dynamic control flows) they fail to execute the program. To overcome this limitation, we propose Terra, an imperative-symbolic co-execution system that can handle any imperative DL programs while achieving the optimized performance of symbolic graph execution. To achieve this, Terra builds a symbolic graph by decoupling DL operations from Python features. Then, Terra conducts the imperative execution to support all Python features, while delegating the decoupled operations to the symbolic execution. We evaluated Terra's performance improve- ment and coverage with ten imperative DL programs for several DNN architectures. The results show that Terra can speed up the execution of all ten imperative DL programs, whereas AutoGraph, one of the state-of-the-art systems, fails to execute five of them.",
        "authors": [
            "Taebum Kim",
            "Eunji Jeong",
            "Geon Woo Kim",
            "Yunmo Koo",
            "Sehoon Kim",
            "Gyeong In Yu",
            "Byung Gon Chun"
        ],
        "created_time": "27 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/TerraImperativeSymbolicCoExecutionofImperativeDeepLearningPrograms.pdf"
    },
    {
        "internal_id": 90,
        "title": "Targeted Neural Dynamical Modeling",
        "abstract": "Latent dynamics models have emerged as powerful tools for modeling and inter- preting neural population activity. Recently, there has been a focus on incorporating simultaneously measured behaviour into these models to further disentangle sources of neural variability in their latent space. These approaches, however, are limited in their ability to capture the underlying neural dynamics (e.g. linear) and in their ability to relate the learned dynamics back to the observed behaviour (e.g. no time lag). To this end, we introduce Targeted Neural Dynamical Modeling (TNDM), a nonlinear state-space model that jointly models the neural activity and external behavioural variables. TNDM decomposes neural dynamics into behaviourally relevant and behaviourally irrelevant dynamics; the relevant dynamics are used to reconstruct the behaviour through a flexible linear decoder and both sets of dynamics are used to reconstruct the neural activity through a linear decoder with no time lag. We implement TNDM as a sequential variational autoencoder and validate it on simulated recordings and recordings taken from the premotor and motor cortex of a monkey performing a center-out reaching task. We show that TNDM is able to learn low-dimensional latent dynamics that are highly predictive of behaviour without sacrificing its fit to the neural data.",
        "authors": [
            "Cole Hurwitz",
            "EH AB",
            "Akash Srivastava",
            "MA ",
            "Kai Xu",
            "EH AB",
            "Justin Jude",
            "EH AB",
            "Matthew G Perich",
            "New York"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/TargetedNeuralDynamicalModeling.pdf"
    },
    {
        "internal_id": 91,
        "title": "Particle Cloud Generation with Message Passing",
        "abstract": "In high energy physics (HEP), jets are collections of correlated particles produced ubiquitously in particle collisions such as those at the CERN Large Hadron Col- lider (LHC). Machine learning (ML)-based generative models, such as generative adversarial networks (GANs), have the potential to significantly accelerate LHC jet simulations. However, despite jets having a natural representation as a set of particles in momentum-space, a.k.a. a particle cloud, there exist no generative models applied to such a dataset. In this work, we introduce a new particle cloud dataset (JetNet), and apply to it existing point cloud GANs. Results are evaluated using (1) 1-Wasserstein distances between high- and low-level feature distributions, (2) a newly developed Fr\u00e9chet ParticleNet Distance, and (3) the coverage and (4) minimum matching distance metrics. Existing GANs are found to be inadequate for physics applications, hence we develop a new message passing GAN (MPGAN), which outperforms existing point cloud GANs on virtually every metric and shows promise for use in HEP. We propose JetNet as a novel point-cloud-style dataset for the ML community to experiment with, and set MPGAN as a benchmark to improve upon for future generative models. Additionally, to facilitate research and improve accessibility and reproducibility in this area, we release the open-source JETNET Python package with interfaces for particle cloud datasets, implementations for evaluation and loss metrics, and more tools for ML in HEP development.",
        "authors": [
            "Generative Adversarial Networks",
            "Raghav Kansal",
            "Javier Duarte",
            "Hao Su",
            "La Jolla",
            "CA ",
            "Breno Orzari",
            "Thiago Tomei",
            "Maurizio Pierini",
            "Mary Touranakou"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/ParticleCloudGenerationwithMessagePassingGenerativeAdversarialNetworks.pdf"
    },
    {
        "internal_id": 92,
        "title": "Uncertainty-Driven Loss for Single Image Super-Resolution",
        "abstract": "In low-level vision such as single image super-resolution (SISR), traditional MSE or L1 loss function treats every pixel equally with the assumption that the importance of all pixels is the same. However, it has been long recognized that texture and edge areas carry more important visual information than smooth areas in photographic images. How to achieve such spatial adaptation in a principled manner has been an open problem in both traditional model-based and modern learning-based approaches toward SISR. In this paper, we propose a new adaptive weighted loss for SISR to train deep networks focusing on challenging situations such as textured and edge pixels with high uncertainty. Specifically, we introduce variance estimation characterizing the uncertainty on a pixel-by-pixel basis into SISR solutions so the targeted pixels in a high-resolution image (mean) and their corresponding uncertainty (variance) can be learned simultaneously. Moreover, uncertainty estimation allows us to leverage conventional wisdom such as sparsity prior for regularizing SISR solutions. Ultimately, pixels with large certainty (e.g., texture and edge pixels) will be prioritized for SISR according to their importance to visual quality. For the first time, we demonstrate that such uncertainty-driven loss can achieve better results than MSE or L1 loss for a wide range of network architectures. Experimental results on three popular SISR networks show that our proposed uncertainty-driven loss has achieved better PSNR performance than traditional loss functions without any increased computation during testing.",
        "authors": [
            "Qian Ning",
            "Weisheng Dong",
            "Xin Li",
            "Jinjian Wu",
            "Guangming Shi",
            "Xi an ",
            "Morgantown WV "
        ],
        "created_time": "25 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/UncertaintyDrivenLossforSingleImageSuperResolution.pdf"
    },
    {
        "internal_id": 93,
        "title": "Reliable and Trustworthy Machine Learning for",
        "abstract": "Unpredictable ML model behavior on unseen data, especially in the health domain, raises serious concerns about its safety as repercussions for mistakes can be fatal. In this paper, we explore the feasibility of using state-of-the-art out-of-distribution detectors for reliable and trustworthy diagnostic predictions. We select publicly available deep learning models relating to various health conditions (e.g., skin cancer, lung sound, and Parkinson's disease) using various input data types (e.g., image, audio, and motion data). We demonstrate that these models show unrea- sonable predictions on out-of-distribution datasets. We show that Mahalanobis distance- and Gram matrices-based out-of-distribution detection methods are able to detect out-of-distribution data with high accuracy for the health models that operate on different modalities. We then translate the out-of-distribution score into a human interpretable CONFIDENCE SCORE to investigate its effect on the users' interaction with health ML applications. Our user study shows that the CONFIDENCE SCORE helped the participants only trust the results with a high score to make a medical decision and disregard results with a low score. Through this work, we demonstrate that dataset shift is a critical piece of information for high-stake ML applications, such as medical diagnosis and healthcare, to provide reliable and trustworthy predictions to the users.",
        "authors": [
            "Chunjong Park",
            "Anas Awadalla",
            "Tadayoshi Kohno",
            "Shwetak Patel"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/ReliableandTrustworthyMachineLearningforHealthUsingDatasetShiftDetection.pdf"
    },
    {
        "internal_id": 94,
        "title": "OctField: Hierarchical Implicit Functions for 3D Modeling",
        "abstract": "Recent advances in localized implicit functions have enabled neural implicit rep- resentation to be scalable to large scenes. However, the regular subdivision of 3D space employed by these approaches fails to take into account the sparsity of the surface occupancy and the varying granularities of geometric details. As a result, its memory footprint grows cubically with the input volume, leading to a prohibitive computational cost even at a moderately dense decomposition. In this work, we present a learnable hierarchical implicit representation for 3D surfaces, coded OctField, that allows high-precision encoding of intricate surfaces with low memory and computational budget. The key to our approach is an adaptive de- composition of 3D scenes that only distributes local implicit functions around the surface of interest. We achieve this goal by introducing a hierarchical octree struc- ture to adaptively subdivide the 3D space according to the surface occupancy and the richness of part geometry. As octree is discrete and non-differentiable, we further propose a novel hierarchical network that models the subdivision of octree cells as a probabilistic process and recursively encodes and decodes both octree structure and surface geometry in a differentiable manner. We demonstrate the value of OctField for a range of shape modeling and reconstruction tasks, show- ing superiority over alternative approaches.",
        "authors": [
            "Weikai Chen ",
            "Jie Yang",
            "Bo Wang",
            "Songrun Liu",
            "Bo Yang"
        ],
        "created_time": "28 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/OctFieldHierarchicalImplicitFunctionsfor3DModeling.pdf"
    },
    {
        "internal_id": 95,
        "title": "DualNet: Continual Learning, Fast and Slow",
        "abstract": "According to Complementary Learning Systems (CLS) theory [37] in neuroscience, humans do effective continual learning through two complementary systems: a fast learning system centered on the hippocampus for rapid learning of the specifics and individual experiences, and a slow learning system located in the neocortex for the gradual acquisition of structured knowledge about the environment. Motivated by this theory, we propose a novel continual learning framework named \"DualNet\", which comprises a fast learning system for supervised learning of pattern-separated representation from specific tasks and a slow learning system for unsupervised rep- resentation learning of task-agnostic general representation via a Self-Supervised Learning (SSL) technique. The two fast and slow learning systems are complemen- tary and work seamlessly in a holistic continual learning framework. Our extensive experiments on two challenging continual learning benchmarks of CORE50 and miniImageNet show that DualNet outperforms state-of-the-art continual learning methods by a large margin. We further conduct ablation studies of different SSL ob- jectives to validate DualNet's efficacy, robustness, and scalability. Code is publicly available at https://github.com/phquang/DualNet.",
        "authors": [
            "Quang Pham",
            "Chenghao Liu"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/DualNetContinualLearningFastandSlow.pdf"
    },
    {
        "internal_id": 96,
        "title": "Minimax Optimal Quantile and Semi-Adversarial Regret via Root-Logarithmic Regularizers",
        "abstract": "Quantile (and, more generally, KL) regret bounds, such as those achieved by NormalHedge (Chaudhuri, Freund, and Hsu 2009) and its variants, relax the goal of competing against the best individual expert to only competing against a majority of experts on adversarial data. More recently, the semi-adversarial paradigm (Bilodeau, Negrea, and Roy 2020) provides an alternative relaxation of adversarial online learning by considering data that may be neither fully adversarial nor stochastic (i.i.d.). We achieve the minimax optimal regret in both paradigms using FTRL with separate, novel, root-logarithmic regularizers, both of which can be interpreted as yielding variants of NormalHedge. We extend existing KL regret upper bounds, which hold uniformly over target distributions, to possibly uncountable expert classes with arbitrary priors; provide the first full-information lower bounds for quantile regret on finite expert classes (which are tight); and provide an adaptively minimax optimal algorithm for the semi-adversarial paradigm that adapts to the true, unknown constraint faster, leading to uniformly improved regret bounds over existing methods.",
        "authors": [
            "Jeffrey Negrea",
            "Blair Bilodeau",
            "Nicol Campolongo",
            "Francesco Orabona",
            "Daniel M Roy"
        ],
        "created_time": "",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/MinimaxOptimalQuantileandSemiAdversarialRegretviaRootLogarithmicRegularizers.pdf"
    },
    {
        "internal_id": 97,
        "title": "When Are Solutions Connected in Deep Networks?",
        "abstract": "The question of how and why the phenomenon of mode connectivity occurs in training deep neural networks has gained remarkable attention in the research community. From a theoretical perspective, two possible explanations have been proposed: (i) the loss function has connected sublevel sets, and (ii) the solutions found by stochastic gradient descent are dropout stable. While these explanations provide insights into the phenomenon, their assumptions are not always satisfied in practice. In particular, the first approach requires the network to have one layer with order of N neurons (N being the number of training samples), while the second one requires the loss to be almost invariant after removing half of the neurons at each layer (up to some rescaling of the remaining ones). In this work, we improve both conditions by exploiting the quality of the features at every intermediate layer together with a milder over-parameterization requirement. More specifically, we show that: (i) under generic assumptions on the features of intermediate layers, it suffices that the last two hidden layers have order of \u221a",
        "authors": [
            "Quynh Nguyen",
            "MPI MIS",
            "Pierre Br chet",
            "MPI MIS",
            "Marco Mondelli",
            "IST Austria"
        ],
        "created_time": "21 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/WhenAreSolutionsConnectedinDeepNetworks.pdf"
    },
    {
        "internal_id": 98,
        "title": "Tight High Probability Bounds for Linear Stochastic",
        "abstract": "This paper provides a non-asymptotic analysis of linear stochastic approximation (LSA) algorithms with fixed stepsize. This family of methods arises in many machine learning tasks and is used to obtain approximate solutions of a linear system \u00afA\u2713 = \u00afb for which \u00afA and \u00afb can only be accessed through random estimates {(An, bn) : n 2 N\u21e4}. Our analysis is based on new results regarding moments and high probability bounds for products of matrices which are shown to be tight. We derive high probability bounds on the performance of LSA under weaker conditions on the sequence {(An, bn) : n 2 N\u21e4} than previous works. However, in contrast, we establish polynomial concentration bounds with order depending on the stepsize. We show that our conclusions cannot be improved without additional assumptions on the sequence of random matrices {An : n 2 N\u21e4}, and in particular that no Gaussian or exponential high probability bounds can hold. Finally, we pay a particular attention to establishing bounds with sharp order with respect to the number of iterations and the stepsize and whose leading terms contain the covariance matrices appearing in the central limit theorems.",
        "authors": [
            "Alain Durmus",
            "Universit Paris Saclay",
            "ENS Paris Saclay",
            "Eric Moulines",
            "Alexey Naumov",
            "Sergey Samsonov",
            "Kevin Scaman",
            "Hoi To Wai"
        ],
        "created_time": "04 November 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/TightHighProbabilityBoundsforLinearStochasticApproximationwithFixedStepsize.pdf"
    },
    {
        "internal_id": 99,
        "title": "Latent Execution for Neural Program Synthesis",
        "abstract": "Program synthesis from input-output (IO) examples has been a long-standing challenge. While recent works demonstrated limited success on domain-specific languages (DSL), it remains highly challenging to apply them to real-world pro- gramming languages, such as C. Due to complicated syntax and token variation, there are three major challenges: (1) unlike many DSLs, programs in languages like C need to compile first and are not executed via interpreters; (2) the program search space grows exponentially when the syntax and semantics of the program- ming language become more complex; and (3) collecting a large-scale dataset of real-world programs is non-trivial. As a first step to address these challenges, we propose LaSynth and show its efficacy in a restricted-C domain (i.e., C code with tens of tokens, with sequential, branching, loop and simple arithmetic operations but no library call). More specifically, LaSynth learns the latent representation to approximate the execution of partially generated programs, even if they are in- complete in syntax (addressing (1)). The learned execution significantly improves the performance of next token prediction over existing approaches, facilitating search (addressing (2)). Finally, once trained with randomly generated ground- truth programs and their IO pairs, LaSynth can synthesize more concise programs that resemble human-written code. Furthermore, retraining our model with these synthesized programs yields better performance with fewer samples for both Karel and C program synthesis, indicating the promise of leveraging the learned program synthesizer to improve the dataset quality for input-output program synthesis (ad- dressing (3)). When evaluating on whether the program execution outputs match the IO pairs, LaSynth achieves 55.2% accuracy on generating simple C code with tens of tokens including loops and branches, outperforming existing approaches without executors by around 20%.",
        "authors": [
            "Xinyun Chen",
            "Dawn Song"
        ],
        "created_time": "14 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/LatentExecutionforNeuralProgramSynthesisBeyondDomainSpecificLanguages.pdf"
    },
    {
        "internal_id": 100,
        "title": "Foundations of Symbolic Languages for Model Interpretability",
        "abstract": "Several queries and scores have been proposed to explain individual predictions made by ML models. Examples include queries based on \"anchors\", which are parts of an instance that are sufficient to justify its classification, and \"feature- perturbation\" scores such as SHAP. Given the need for flexible, reliable, and easy-to- apply interpretability methods for ML models, we foresee the need for developing declarative languages to naturally specify different explainability queries. We do this in a principled way by rooting such a language in a logic called FOIL, that allows for expressing many simple but important explainability queries, and might serve as a core for more expressive interpretability languages. We study the computational complexity of FOIL queries over classes of ML models often deemed to be easily interpretable: decision trees and more general decision diagrams. Since the number of possible inputs for an ML model is exponential in its dimension, tractability of the FOIL evaluation problem is delicate, but can be achieved by either restricting the structure of the models, or the fragment of FOIL being evaluated. We also present a prototype implementation of FOIL wrapped in a high-level declarative language, and perform experiments showing that such a language can be used in practice.",
        "authors": [
            "Marcelo Arenas",
            "Daniel Baez",
            "Pablo Barcel ",
            "Jorge P rez",
            "Bernardo Subercaseaux",
            "PUC Chile",
            "PUC Chile",
            "Universidad de Chile"
        ],
        "created_time": "01 November 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/FoundationsofSymbolicLanguagesforModelInterpretability.pdf"
    },
    {
        "internal_id": 101,
        "title": "Adaptive Sampling for Minimax Fair Classification",
        "abstract": "Machine learning models trained on uncurated datasets can often end up adversely affecting inputs belonging to underrepresented groups. To address this issue, we consider the problem of adaptively constructing training sets which allow us to learn classifiers that are fair in a minimax sense. We first propose an adaptive sampling algorithm based on the principle of optimism, and derive theoretical bounds on its performance. We also propose heuristic extensions of this algorithm suitable for application to large scale, practical problems. Next, by deriving algorithm independent lower-bounds for a specific class of problems, we show that the performance achieved by our adaptive scheme cannot be improved in general. We then validate the benefits of adaptively constructing training sets via experiments on synthetic tasks with logistic regression classifiers, as well as on several real-world tasks using convolutional neural networks (CNNs).",
        "authors": [
            "Shubhanshu Shekhar",
            "Greg Fields",
            "Mohammad Ghavamzadeh",
            "Tara Javidi"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/AdaptiveSamplingforMinimaxFairClassification.pdf"
    },
    {
        "internal_id": 102,
        "title": "You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection",
        "abstract": "Can Transformer perform 2D object- and region-level recognition from a pure sequence-to-sequence perspective with minimal knowledge about the 2D spatial structure? To answer this question, we present You Only Look at One Sequence (YOLOS), a series of object detection models based on the vanilla Vision Trans- former with the fewest possible modifications, region priors, as well as induc- tive biases of the target task. We find that YOLOS pre-trained on the mid-sized ImageNet-1k dataset only can already achieve quite competitive performance on the challenging COCO object detection benchmark, e.g., YOLOS-Base directly adopted from BERT-Base architecture can obtain 42.0 box AP on COCO val. We also discuss the impacts as well as limitations of current pre-train schemes and model scaling strategies for Transformer in vision through YOLOS. Code and pre-trained models are available at https://github.com/hustvl/YOLOS.",
        "authors": [
            "Yuxin Fang ",
            "Bencheng Liao ",
            "Xinggang Wang ",
            "Jiemin Fang ",
            "Jiyang Qi ",
            "Rui Wu ",
            "Jianwei Niu ",
            "Wenyu Liu ",
            " Horizon Robotics"
        ],
        "created_time": "27 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/YouOnlyLookatOneSequenceRethinkingTransformerinVisionthroughObjectDetection.pdf"
    },
    {
        "internal_id": 103,
        "title": "Derivative-Free Policy Optimization for Linear",
        "abstract": "Policy-based model-free reinforcement learning (RL) methods have shown great promise for continuous control applications. However, their performances on risk-sensitive/robust control tasks have not been fully understood, which has been generally considered to be one important open problem in the seminal work (Fazel et al., 2018). We make a step toward addressing this open problem, by providing the first sample complexity results for policy gradient (PG) methods in two fundamental risk-sensitive/robust control settings: the linear exponential quadratic Gaussian, and the linear-quadratic (LQ) disturbance attenuation problems. The optimization landscapes for these problems are by nature more challenging than that of the LQ regulator problem, due to lack of coercivity of their objective functions. To overcome this challenge, we obtain the first implicit regularization results for model-free PG methods, certifying that the controller remains robust during the learning process, which further lead to the sample complexity guarantees. As a by-product, our results also provide the first sample complexity of PG methods in two-player zero-sum LQ dynamic games, a baseline in multi-agent RL.",
        "authors": [
            "Kaiqing Zhang",
            "Xiangyuan Zhang",
            "Bin Hu",
            "Tamer Ba sar"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/DerivativeFreePolicyOptimizationforLinearRiskSensitiveandRobustControlDesignImplicitRegularizationandSampleComplexity.pdf"
    },
    {
        "internal_id": 104,
        "title": "Residual Relaxation for Multi-view Representation Learning",
        "abstract": "Multi-view methods learn representations by aligning multiple views of the same image and their performance largely depends on the choice of data augmentation. In this paper, we notice that some other useful augmentations, such as image ro- tation, are harmful for multi-view methods because they cause a semantic shift that is too large to be aligned well. This observation motivates us to relax the ex- act alignment objective to better cultivate stronger augmentations. Taking image rotation as a case study, we develop a generic approach, Pretext-aware Resid- ual Relaxation (Prelax), that relaxes the exact alignment by allowing an adaptive residual vector between different views and encoding the semantic shift through pretext-aware learning. Extensive experiments on different backbones show that our method can not only improve multi-view methods with existing augmenta- tions, but also benefit from stronger image augmentations like rotation.",
        "authors": [
            "Yifei Wang",
            "Zhengyang Geng",
            "Feng Jiang",
            "Chuming Li",
            "Yisen Wang",
            "Jiansheng Yang",
            "Zhouchen Lin",
            "Peking Univesity",
            "Guangzhou "
        ],
        "created_time": "27 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/ResidualRelaxationforMultiviewRepresentationLearning.pdf"
    },
    {
        "internal_id": 105,
        "title": "Why Generalization in RL is Difficult: Epistemic POMDPs and Implicit Partial Observability",
        "abstract": "Generalization is a central challenge for the deployment of reinforcement learning (RL) systems in the real world. In this paper, we show that the sequential structure of the RL problem necessitates new approaches to generalization beyond the well-studied techniques used in supervised learning. While supervised learning methods can generalize effectively without explicitly accounting for epistemic uncertainty, we show that, perhaps surprisingly, this is not the case in RL. We show that generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, effectively turning even fully- observed MDPs into POMDPs. Informed by this observation, we recast the problem of generalization in RL as solving the induced partially observed Markov decision process, which we call the epistemic POMDP. We demonstrate the failure modes of algorithms that do not appropriately handle this partial observability, and suggest a simple ensemble-based technique for approximately solving the partially observed problem. Empirically, we demonstrate that our simple algorithm derived from the epistemic POMDP achieves significant gains in generalization over current methods on the Procgen benchmark suite.",
        "authors": [
            "Dibya Ghosh",
            "Jad Rahme",
            "Aviral Kumar",
            "Amy Zhang",
            "Ryan P Adams",
            "Sergey Levine"
        ],
        "created_time": "",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/WhyGeneralizationinRLisDifficultEpistemicPOMDPsandImplicitPartialObservability.pdf"
    },
    {
        "internal_id": 106,
        "title": "On the Frequency Bias of Generative Models",
        "abstract": "The key objective of Generative Adversarial Networks (GANs) is to generate new data with the same statistics as the provided training data. However, multiple recent works show that state-of-the-art architectures yet struggle to achieve this goal. In particular, they report an elevated amount of high frequencies in the spectral statistics which makes it straightforward to distinguish real and generated images. Explanations for this phenomenon are controversial: While most works attribute the artifacts to the generator, other works point to the discriminator. We take a sober look at those explanations and provide insights on what makes proposed measures against high-frequency artifacts effective. To achieve this, we first independently assess the architectures of both the generator and discriminator and investigate if they exhibit a frequency bias that makes learning the distribution of high-frequency content particularly problematic. Based on these experiments, we make the following four observations: 1) Different upsampling operations bias the generator towards different spectral properties. 2) Checkerboard artifacts introduced by upsampling cannot explain the spectral discrepancies alone as the generator is able to compensate for these artifacts. 3) The discriminator does not struggle with detecting high frequencies per se but rather struggles with frequencies of low magnitude. 4) The downsampling operations in the discriminator can impair the quality of the training signal it provides. In light of these findings, we analyze proposed measures against high-frequency artifacts in state-of-the-art GAN training but find that none of the existing approaches can fully resolve spectral artifacts yet. Our results suggest that there is great potential in improving the discriminator and that this could be key to match the distribution of the training data more closely.",
        "authors": [
            "Katja Schwarz",
            "Yiyi Liao",
            "Autonomous Vision Group"
        ],
        "created_time": "27 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/OntheFrequencyBiasofGenerativeModels.pdf"
    },
    {
        "internal_id": 107,
        "title": "Grad2Task: Improved Few-shot Text Classification Using Gradients for Task Representation",
        "abstract": "Large pretrained language models (LMs) like BERT have improved performance in many disparate natural language processing (NLP) tasks. However, fine tuning such models requires a large number of training examples for each target task. Simultaneously, many realistic NLP problems are \"few shot\", without a sufficiently large training set. In this work, we propose a novel conditional neural process-based approach for few-shot text classification that learns to transfer from other diverse tasks with rich annotation. Our key idea is to represent each task using gradient information from a base model and to train an adaptation network that modulates a text classifier conditioned on the task representation. While previous task-aware few-shot learners represent tasks by input encoding, our novel task representation is more powerful, as the gradient captures input-output relationships of a task. Experimental results show that our approach outperforms traditional fine-tuning, sequential transfer learning, and state-of-the-art meta learning approaches on a collection of diverse few-shot tasks. We further conducted analysis and ablations to justify our design choices.",
        "authors": [
            "Jixuan Wang",
            "Kuan Chieh Wang",
            "Frank Rudzicz",
            "Michael Brudno",
            "Unity Health Toronto"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/Grad2TaskImprovedFewshotTextClassificationUsingGradientsforTaskRepresentation.pdf"
    },
    {
        "internal_id": 108,
        "title": "Breaking the Linear Iteration Cost Barrier for Some Well-known Conditional Gradient Methods Using MaxIP Data-structures",
        "abstract": "Conditional gradient methods (CGM) are widely used in modern machine learn- ing. CGM's overall running time usually consists of two parts: the number of iterations and the cost of each iteration. Most efforts focus on reducing the num- ber of iterations as a means to reduce the overall running time. In this work, we focus on improving the per iteration cost of CGM. The bottleneck step in most CGM is maximum inner product search (MaxIP), which requires a linear scan over the parameters. In practice, approximate MaxIP data-structures are found to be helpful heuristics. However, theoretically, nothing is known about the combi- nation of approximate MaxIP data-structures and CGM. In this work, we answer this question positively by providing a formal framework to combine the locality sensitive hashing type approximate MaxIP data-structures with CGM algorithms. As a result, we show the first algorithm, where the cost per iteration is sublinear in the number of parameters, for many fundamental optimization algorithms, e.g., Frank-Wolfe, Herding algorithm, and policy gradient.",
        "authors": [
            "Zhaozhuo Xu",
            "Zhao Song",
            "Anshumali Shrivastava"
        ],
        "created_time": "",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/BreakingtheLinearIterationCostBarrierforSomeWellknownConditionalGradientMethodsUsingMaxIPDatastructures.pdf"
    },
    {
        "internal_id": 109,
        "title": "Aligning Pretraining for Detection via Object-Level Contrastive Learning",
        "abstract": "Image-level contrastive representation learning has proven to be highly effective as a generic model for transfer learning. Such generality for transfer learning, however, sacrifices specificity if we are interested in a certain downstream task. We argue that this could be sub-optimal and thus advocate a design principle which encourages alignment between the self-supervised pretext task and the downstream task. In this paper, we follow this principle with a pretraining method specifically designed for the task of object detection. We attain alignment in the following three aspects: 1) object-level representations are introduced via selective search bounding boxes as object proposals; 2) the pretraining network architecture incorporates the same dedicated modules used in the detection pipeline (e.g. FPN); 3) the pretraining is equipped with object detection properties such as object-level translation invariance and scale invariance. Our method, called Selective Object COntrastive learning (SoCo), achieves state-of-the-art results for transfer performance on COCO detection using a Mask R-CNN framework. Code is available at https://github.com/hologerry/SoCo.",
        "authors": [
            "Fangyun Wei",
            "Yue Gao",
            "Zhirong Wu",
            "Han Hu",
            "Stephen Lin"
        ],
        "created_time": "22 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/AligningPretrainingforDetectionviaObjectLevelContrastiveLearning.pdf"
    },
    {
        "internal_id": 110,
        "title": "Continuous-time edge modelling using non-parametric point processes",
        "abstract": "The mutually-exciting Hawkes process (ME-HP) is a natural choice to model reciprocity, which is an important attribute of continuous-time edge (dyadic) data. However, existing ways of implementing the ME-HP for such data are either inflexible, as the exogenous (background) rate functions are typically constant and the endogenous (excitation) rate functions are specified parametrically, or inefficient, as inference usually relies on Markov chain Monte Carlo methods with high computational costs. To address these limitations, we discuss various approaches to model design, and develop three variants of non-parametric point processes for continuous-time edge modelling (CTEM). The resulting models are highly adaptable as they generate intensity functions through sigmoidal Gaussian processes, and so provide greater modelling flexibility than parametric forms. The models are implemented via a fast variational inference method enabled by a novel edge modelling construction. The superior performance of the proposed CTEM models is demonstrated through extensive experimental evaluations on four real-world continuous-time edge data sets.",
        "authors": [
            "Xuhui Fan",
            "Bin Li",
            "Feng Zhou"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/Continuoustimeedgemodellingusingnonparametricpointprocesses.pdf"
    },
    {
        "internal_id": 111,
        "title": "Best-case lower bounds in online learning",
        "abstract": "Much of the work in online learning focuses on the study of sublinear upper bounds on the regret. In this work, we initiate the study of best-case lower bounds in online convex optimization, wherein we bound the largest improvement an algorithm can obtain relative to the single best action in hindsight. This problem is motivated by the goal of better understanding the adaptivity of a learning algorithm. Another motivation comes from fairness: it is known that best-case lower bounds are instrumental in obtaining algorithms for decision-theoretic online learning (DTOL) that satisfy a notion of group fairness. Our contributions are a general method to provide best-case lower bounds in Follow the Regularized Leader (FTRL) algorithms with time-varying regularizers, which we use to show that best-case lower bounds are of the same order as existing upper regret bounds: this includes situations with a fixed learning rate, decreasing learning rates, timeless methods, and adaptive gradient methods. In stark contrast, we show that the linearized version of FTRL can attain negative linear regret. Finally, in DTOL with two experts and binary losses, we fully characterize the best-case sequences, which provides a finer understanding of the best-case lower bounds.",
        "authors": [
            "Ali Mortazavi "
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/Bestcaselowerboundsinonlinelearning.pdf"
    },
    {
        "internal_id": 112,
        "title": "Generalization Guarantee of SGD for Pairwise Learning",
        "abstract": "Recently, there is a growing interest in studying pairwise learning since it includes many important machine learning tasks as specific examples, e.g., metric learning, AUC maximization and ranking. While stochastic gradient descent (SGD) is an efficient method, there is a lacking study on its generalization behavior for pairwise learning. In this paper, we present a systematic study on the generalization analysis of SGD for pairwise learning to understand the balance between generalization and optimization. We develop a novel high-probability generalization bound for uniformly-stable algorithms to incorporate the variance information for better generalization, based on which we establish the first nonsmooth learning algorithm to achieve almost optimal high-probability and dimension-independent excess risk bounds with O(n) gradient computations. We consider both convex and nonconvex pairwise learning problems. Our stability analysis for convex problems shows how the interpolation can help generalization. We establish a uniform convergence of gradients, and apply it to derive the first excess risk bounds on population gradients for nonconvex pairwise learning. Finally, we extend our stability analysis to pairwise learning with gradient-dominated problems.",
        "authors": [
            "Yunwen Lei",
            "Mingrui Liu",
            "Yiming Ying",
            "Birmingham B TT",
            "VA "
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/GeneralizationGuaranteeofSGDforPairwiseLearning.pdf"
    },
    {
        "internal_id": 113,
        "title": "Cross-Modal Domain Adaptation for Cost-Efficient Visual Reinforcement Learning",
        "abstract": "In visual-input sim-to-real scenarios, to overcome the reality gap between images rendered in simulators and those from the real world, domain adaptation, i.e., learning an aligned representation space between simulators and the real world, then training and deploying policies in the aligned representation, is a promising direction. Previous methods focus on same-modal domain adaptation. However, those methods require building and running simulators that render high-quality images, which can be difficult and costly. In this paper, we consider a more cost- efficient setting of visual-input sim-to-real where only low-dimensional states are simulated. We first point out that the objective of learning mapping functions in previous methods that align the representation spaces is ill-posed, prone to yield an incorrect mapping. When the mapping crosses modalities, previous methods are easier to fail. Our algorithm, Cross-mOdal Domain Adaptation with Sequential structure (CODAS), mitigates the ill-posedness by utilizing the sequential nature of the data sampling process in RL tasks. Experiments on MuJoCo and Hand Manipulation Suite tasks show that the agents deployed with our method achieve similar performance as it has in the source domain, while those deployed with previous methods designed for same-modal domain adaptation suffer a larger performance gap.",
        "authors": [
            "Xiong Hui Chen",
            "Shengyi Jiang",
            "Feng Xu",
            "Zongzhang Zhang",
            "Yang Yu",
            "Nanjing ",
            "Guangzhou "
        ],
        "created_time": "31 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/CrossmodalDomainAdaptationforCostEfficientVisualReinforcementLearning.pdf"
    },
    {
        "internal_id": 114,
        "title": "Coresets for Classification \u2013 Simplified and Strengthened",
        "abstract": "We give relative error coresets for training linear classifiers with a broad class of loss functions, including the logistic loss and hinge loss. Our construction achieves (1 \u00b1 \u03f5) relative error with \u02dcO(d \u00b7 \u00b5y(X)2/\u03f52) points, where \u00b5y(X) is a natural complexity measure of the data matrix X \u2208 Rn\u00d7d and label vector y \u2208 {\u22121, 1}n, introduced in [MSSW18]. Our result is based on subsampling data points with probabilities proportional to their \u21131 Lewis weights. It significantly improves on existing theoretical bounds and performs well in practice, outperforming uniform subsampling along with other importance sampling methods. Our sampling dis- tribution does not depend on the labels, so can be used for active learning. It also does not depend on the specific loss function, so a single coreset can be used in multiple training scenarios.",
        "authors": [
            "Tung Mai",
            "Cameron Musco",
            "Anup Rao"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/CoresetsforClassificationSimplifiedandStrengthened.pdf"
    },
    {
        "internal_id": 115,
        "title": "Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning",
        "abstract": "Recently, there has been great success in applying deep neural networks on graph structured data. Most work, however, focuses on either node- or graph-level supervised learning, such as node, link or graph classification or node-level un- supervised learning (e.g., node clustering). Despite its wide range of possible applications, graph-level unsupervised representation learning has not received much attention yet. This might be mainly attributed to the high representation complexity of graphs, which can be represented by n! equivalent adjacency ma- trices, where n is the number of nodes. In this work we address this issue by proposing a permutation-invariant variational autoencoder for graph structured data. Our proposed model indirectly learns to match the node order of input and output graph, without imposing a particular node order or performing expensive graph matching. We demonstrate the effectiveness of our proposed model for graph reconstruction, generation and interpolation and evaluate the expressive power of extracted representations for downstream graph-level classification and regression.",
        "authors": [
            "Robin Winter",
            "Bayer AG",
            "Frank No",
            "Djork Arn Clevert",
            "Bayer AG"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/PermutationInvariantVariationalAutoencoderforGraphLevelRepresentationLearning.pdf"
    },
    {
        "internal_id": 116,
        "title": "What training reveals about neural network complexity",
        "abstract": "This work explores the Benevolent Training Hypothesis (BTH) which argues that the complexity of the function a deep neural network (NN) is learning can be deduced by its training dynamics. Our analysis provides evidence for BTH by relating the NN's Lipschitz constant at different regions of the input space with the behavior of the stochastic training procedure. We first observe that the Lipschitz constant close to the training data affects various aspects of the parameter trajectory, with more complex networks having a longer trajectory, bigger variance, and often veering further from their initialization. We then show that NNs whose 1st layer bias is trained more steadily (i.e., slowly and with little variation) have bounded complexity even in regions of the input space that are far from any training point. Finally, we find that steady training with Dropout implies a training- and data- dependent generalization bound that grows poly-logarithmically with the number of parameters. Overall, our results support the intuition that good training behavior can be a useful bias towards good generalization.",
        "authors": [
            "Marinos Poiitis",
            "Stefanie Jegelka"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/Whattrainingrevealsaboutneuralnetworkcomplexity.pdf"
    },
    {
        "internal_id": 117,
        "title": "Why Spectral Normalization Stabilizes GANs: Analysis and Improvements",
        "abstract": "Spectral normalization (SN) [30] is a widely-used technique for improving the stability and sample quality of Generative Adversarial Networks (GANs). However, current understanding of SN's efficacy is limited. In this work, we show that SN controls two important failure modes of GAN training: exploding and vanishing gradients. Our proofs illustrate a (perhaps unintentional) connection with the suc- cessful LeCun initialization [25]. This connection helps to explain why the most popular implementation of SN for GANs [30] requires no hyper-parameter tuning, whereas stricter implementations of SN [15, 12] have poor empirical performance out-of-the-box. Unlike LeCun initialization which only controls gradient vanishing at the beginning of training, SN preserves this property throughout training. Build- ing on this theoretical understanding, we propose a new spectral normalization technique: Bidirectional Scaled Spectral Normalization (BSSN), which incorpo- rates insights from later improvements to LeCun initialization: Xavier initialization [13] and Kaiming initialization [17]. Theoretically, we show that BSSN gives better gradient control than SN. Empirically, we demonstrate that it outperforms SN in sample quality and training stability on several benchmark datasets.",
        "authors": [
            "Zinan Lin",
            "PA ",
            "Vyas Sekar",
            "PA ",
            "Giulia Fanti",
            "PA "
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/WhySpectralNormalizationStabilizesGANsAnalysisandImprovements.pdf"
    },
    {
        "internal_id": 118,
        "title": "Understanding Bandits with Graph Feedback",
        "abstract": "The bandit problem with graph feedback, proposed in [Mannor and Shamir, NeurIPS 2011], is modeled by a directed graph G = (V, E) where V is the collection of bandit arms, and once an arm is triggered, all its incident arms are observed. A fundamental question is how the structure of the graph affects the min-max regret. We propose the notions of the fractional weak domination number \u03b4\u2217 and the k-packing independence number capturing upper bound and lower bound for the regret respectively. We show that the two notions are inherently connected via aligning them with the linear program of the weakly dominating set and its dual \u2014 the fractional vertex packing set respectively. Based on this connection, we utilize the strong duality theorem to prove a general regret upper bound O \ufffd (\u03b4\u2217 log |V |)",
        "authors": [
            "Houshuang Chen",
            "Zengfeng Huang",
            "Shuai Li",
            "Chihao Zhang"
        ],
        "created_time": "27 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/UnderstandingBanditswithGraphFeedback.pdf"
    },
    {
        "internal_id": 119,
        "title": "NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction",
        "abstract": "We present a novel neural surface reconstruction method, called NeuS, for recon- structing objects and scenes with high fidelity from 2D image inputs. Existing neural surface reconstruction approaches, such as DVR [Niemeyer et al., 2020] and IDR [Yariv et al., 2020], require foreground mask as supervision, easily get trapped in local minima, and therefore struggle with the reconstruction of objects with severe self-occlusion or thin structures. Meanwhile, recent neural methods for novel view synthesis, such as NeRF [Mildenhall et al., 2020] and its variants, use volume rendering to produce a neural scene representation with robustness of optimization, even for highly complex objects. However, extracting high-quality surfaces from this learned implicit representation is difficult because there are not sufficient surface constraints in the representation. In NeuS, we propose to represent a surface as the zero-level set of a signed distance function (SDF) and develop a new volume rendering method to train a neural SDF representation. We observe that the conventional volume rendering method causes inherent geometric errors (i.e. bias) for surface reconstruction, and therefore propose a new formula- tion that is free of bias in the first order of approximation, thus leading to more accurate surface reconstruction even without the mask supervision. Experiments on the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the state-of-the-arts in high-quality surface reconstruction, especially for objects and scenes with complex structures and self-occlusion.",
        "authors": [
            "Peng Wang",
            "Lingjie Liu",
            "Yuan Liu",
            "Christian Theobalt",
            "Taku Komura",
            "Wenping Wang"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/NeuSLearningNeuralImplicitSurfacesbyVolumeRenderingforMultiviewReconstruction.pdf"
    },
    {
        "internal_id": 120,
        "title": "Chebyshev-Cantelli PAC-Bayes-Bennett Inequality for the Weighted Majority Vote",
        "abstract": "We present a new second-order oracle bound for the expected risk of a weighted majority vote. The bound is based on a novel parametric form of the Chebyshev- Cantelli inequality (a.k.a. one-sided Chebyshev's), which is amenable to efficient minimization. The new form resolves the optimization challenge faced by prior oracle bounds based on the Chebyshev-Cantelli inequality, the C-bounds [Germain et al., 2015], and, at the same time, it improves on the oracle bound based on second order Markov's inequality introduced by Masegosa et al. [2020]. We also derive a new concentration of measure inequality, which we name PAC-Bayes-Bennett, since it combines PAC-Bayesian bounding with Bennett's inequality. We use it for empirical estimation of the oracle bound. The PAC-Bayes-Bennett inequality improves on the PAC-Bayes-Bernstein inequality of Seldin et al. [2012]. We provide an empirical evaluation demonstrating that the new bounds can improve on the work of Masegosa et al. [2020]. Both the parametric form of the Chebyshev- Cantelli inequality and the PAC-Bayes-Bennett inequality may be of independent interest for the study of concentration of measure in other domains.",
        "authors": [
            "Yi Shan Wu",
            "Stephan S Lorenzen",
            "Christian Igel",
            "Yevgeny Seldin"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/ChebyshevCantelliPACBayesBennettInequalityfortheWeightedMajorityVote.pdf"
    },
    {
        "internal_id": 121,
        "title": "Do Wider Neural Networks Really Help Adversarial Robustness?",
        "abstract": "Adversarial training is a powerful type of defense against adversarial examples. Previous empirical results suggest that adversarial training requires wider networks for better performances. However, it remains elusive how does neural network width affect model robustness. In this paper, we carefully examine the relationship between network width and model robustness. Specifically, we show that the model robustness is closely related to the tradeoff between natural accuracy and perturbation stability, which is controlled by the robust regularization parameter \u03bb. With the same \u03bb, wider networks can achieve better natural accuracy but worse perturbation stability, leading to a potentially worse overall model robustness. To understand the origin of this phenomenon, we further relate the perturbation stability with the network's local Lipschitzness. By leveraging recent results on neural tangent kernels, we theoretically show that wider networks tend to have worse perturbation stability. Our analyses suggest that: 1) the common strategy of first fine-tuning \u03bb on small networks and then directly use it for wide model training could lead to deteriorated model robustness; 2) one needs to properly enlarge \u03bb to unleash the robustness potential of wider models fully. Finally, we propose a new Width Adjusted Regularization (WAR) method that adaptively enlarges \u03bb on wide models and significantly saves the tuning time.",
        "authors": [
            "Boxi Wu",
            "Jinghui Chen",
            "PA ",
            "Deng Cai",
            "Xiaofei He",
            "Quanquan Gu"
        ],
        "created_time": "25 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/DoWiderNeuralNetworksReallyHelpAdversarialRobustness.pdf"
    },
    {
        "internal_id": 122,
        "title": "Look at the Variance! Efficient Black-box Explanations with Sobol-based Sensitivity Analysis",
        "abstract": "We describe a novel attribution method which is grounded in Sensitivity Analysis and uses Sobol indices. Beyond modeling the individual contributions of image regions, Sobol indices provide an efficient way to capture higher-order interactions between image regions and their contributions to a neural network's prediction through the lens of variance. We describe an approach that makes the computation of these indices efficient for high-dimensional problems by using perturbation masks coupled with efficient estimators to handle the high dimensionality of images. Importantly, we show that the proposed method leads to favorable scores on standard benchmarks for vision (and language models) while drastically reducing the computing time compared to other black-box methods \u2013 even surpassing the accuracy of state-of-the-art white-box methods which require access to internal representations. Our code is freely available: github.com/fel-thomas/ Sobol-Attribution-Method.",
        "authors": [
            "Thomas Fel",
            "Mathieu Chalvidal",
            "Matthieu Cord",
            "David Vigouroux",
            "Thomas Serre",
            "Sorbonne Universit",
            "Universit de Toulouse",
            "Valeo ai"
        ],
        "created_time": "27 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/LookattheVarianceEfficientBlackboxExplanationswithSobolbasedSensitivityAnalysis.pdf"
    },
    {
        "internal_id": 123,
        "title": "Smooth Normalizing Flows",
        "abstract": "Normalizing flows are a promising tool for modeling probability distributions in physical systems. While state-of-the-art flows accurately approximate distributions and energies, applications in physics additionally require smooth energies to com- pute forces and higher-order derivatives. Furthermore, such densities are often defined on non-trivial topologies. A recent example are Boltzmann Generators for generating 3D-structures of peptides and small proteins. These generative models leverage the space of internal coordinates (dihedrals, angles, and bonds), which is a product of hypertori and compact intervals. In this work, we introduce a class of smooth mixture transformations working on both compact intervals and hypertori. Mixture transformations employ root-finding methods to invert them in practice, which has so far prevented bi-directional flow training. To this end, we show that parameter gradients and forces of such inverses can be computed from forward evaluations via the inverse function theorem. We demonstrate two advantages of such smooth flows: they allow training by force matching to simulation data and can be used as potentials in molecular dynamics simulations.",
        "authors": [
            "Jonas K hler",
            "Frank No"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/SmoothNormalizingFlows.pdf"
    },
    {
        "internal_id": 124,
        "title": "High-probability bounds for Non-Convex Stochastic Optimization with Heavy Tails",
        "abstract": "We consider non-convex stochastic optimization using first-order algorithms for which the gradient estimates may have heavy tails. We show that a combination of gradient clipping, momentum, and normalized gradient descent yields convergence to critical points in high-probability with best-known rates for smooth losses when the gradients only have bounded pth moments for some p \u2208 (1, 2]. We then consider the case of second-order smooth losses, which to our knowledge have not been studied in this setting, and again obtain high-probability bounds for any p. Moreover, our results hold for arbitrary smooth norms, in contrast to the typical SGD analysis which requires a Hilbert space norm. Further, we show that after a suitable \"burn-in\" period, the objective value will monotonically decrease whenever the current iterate is not a critical point, which provides intuition behind the popular practice of learning rate \"warm-up\" and also yields a last-iterate guarantee.",
        "authors": [
            "Ashok Cutkosky",
            "Harsh Mehta"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/HighprobabilityBoundsforNonConvexStochasticOptimizationwithHeavyTails.pdf"
    },
    {
        "internal_id": 125,
        "title": "Training Feedback Spiking Neural Networks by Implicit Differentiation on the Equilibrium State",
        "abstract": "Spiking neural networks (SNNs) are brain-inspired models that enable energy- efficient implementation on neuromorphic hardware. However, the supervised training of SNNs remains a hard problem due to the discontinuity of the spiking neuron model. Most existing methods imitate the backpropagation framework and feedforward architectures for artificial neural networks, and use surrogate derivatives or compute gradients with respect to the spiking time to deal with the problem. These approaches either accumulate approximation errors or only propa- gate information limitedly through existing spikes, and usually require information propagation along time steps with large memory costs and biological implausibility. In this work, we consider feedback spiking neural networks, which are more brain- like, and propose a novel training method that does not rely on the exact reverse of the forward computation. First, we show that the average firing rates of SNNs with feedback connections would gradually evolve to an equilibrium state along time, which follows a fixed-point equation. Then by viewing the forward computation of feedback SNNs as a black-box solver for this equation, and leveraging the implicit differentiation on the equation, we can compute the gradient for parameters without considering the exact forward procedure. In this way, the forward and backward procedures are decoupled and therefore the problem of non-differentiable spiking functions is avoided. We also briefly discuss the biological plausibility of implicit differentiation, which only requires computing another equilibrium. Extensive experiments on MNIST, Fashion-MNIST, N-MNIST, CIFAR-10, and CIFAR-100 demonstrate the superior performance of our method for feedback models with fewer neurons and parameters in a small number of time steps. Our code is available at https://github.com/pkuxmq/IDE-FSNN.",
        "authors": [
            "Mingqing Xiao",
            "Qingyan Meng",
            "Zongpeng Zhang",
            "Yisen Wang",
            "Zhouchen Lin",
            "Guangzhou "
        ],
        "created_time": "24 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/TrainingFeedbackSpikingNeuralNetworksbyImplicitDifferentiationontheEquilibriumState.pdf"
    },
    {
        "internal_id": 126,
        "title": "On the Sample Complexity of Learning under Invariance and Geometric Stability",
        "abstract": "Many supervised learning problems involve high-dimensional data such as images, text, or graphs. In order to make efficient use of data, it is often useful to leverage certain geometric priors in the problem at hand, such as invariance to translations, permutation subgroups, or stability to small deformations. We study the sample complexity of learning problems where the target function presents such invariance and stability properties, by considering spherical harmonic decompositions of such functions on the sphere. We provide non-parametric rates of convergence for kernel methods, and show improvements in sample complexity by a factor equal to the size of the group when using an invariant kernel over the group, compared to the corresponding non-invariant kernel. These improvements are valid when the sample size is large enough, with an asymptotic behavior that depends on spectral properties of the group. Finally, these gains are extended beyond invariance groups to also cover geometric stability to small deformations, modeled here as subsets (not necessarily subgroups) of permutations.",
        "authors": [
            "Alberto Bietti",
            "Luca Venturi",
            "Joan Bruna"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/OntheSampleComplexityofLearningunderGeometricStability.pdf"
    },
    {
        "internal_id": 127,
        "title": "Dirichlet Energy Constrained Learning for Deep Graph Neural Networks",
        "abstract": "Graph neural networks (GNNs) integrate deep architectures and topological struc- ture modeling in an effective way. However, the performance of existing GNNs would decrease significantly when they stack many layers, because of the over- smoothing issue. Node embeddings tend to converge to similar vectors when GNNs keep recursively aggregating the representations of neighbors. To enable deep GNNs, several methods have been explored recently. But they are developed from either techniques in convolutional neural networks or heuristic strategies. There is no generalizable and theoretical principle to guide the design of deep GNNs. To this end, we analyze the bottleneck of deep GNNs by leveraging the Dirichlet energy of node embeddings, and propose a generalizable principle to guide the training of deep GNNs. Based on it, a novel deep GNN framework \u2013 Energetic Graph Neural Networks (EGNN) is designed. It could provide lower and upper constraints in terms of Dirichlet energy at each layer to avoid over-smoothing. Experimental results demonstrate that EGNN achieves state-of-the-art performance by using deep layers.",
        "authors": [
            "Kaixiong Zhou",
            "Xiao Huang",
            "Daochen Zha",
            "Rui Chen",
            "Li Li",
            "Soo Hyun Choi",
            "Xia Hu"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/DirichletEnergyConstrainedLearningforDeepGraphNeuralNetworks.pdf"
    },
    {
        "internal_id": 128,
        "title": "Exploiting Opponents under Utility Constraints in Sequential Games",
        "abstract": "Recently, game-playing agents based on AI techniques have demonstrated super- human performance in several sequential games, such as chess, Go, and poker. Sur- prisingly, the multi-agent learning techniques that allowed to reach these achieve- ments do not take into account the actual behavior of the human player, potentially leading to an impressive gap in performances. In this paper, we address the problem of designing artificial agents that learn how to effectively exploit unknown human opponents while playing repeatedly against them in an online fashion. We study the case in which the agent's strategy during each repetition of the game is subject to constraints ensuring that the human's expected utility is within some lower and upper thresholds. Our framework encompasses several real-world problems, such as human engagement in repeated game playing and human education by means of serious games. As a first result, we formalize a set of linear inequalities encoding the conditions that the agent's strategy must satisfy at each iteration in order to do not violate the given bounds for the human's expected utility. Then, we use such formulation in an upper confidence bound algorithm, and we prove that the resulting procedure suffers from sublinear regret and guarantees that the constraints are satisfied with high probability at each iteration. Finally, we empirically evaluate the convergence of our algorithm on standard testbeds of sequential games.",
        "authors": [
            "Politecnico di Milano",
            "Federico Cacciamani",
            "Politecnico di Milano",
            "Simone Fioravanti",
            "Nicola Gatti",
            "Politecnico di Milano",
            "Alberto Marchesi",
            "Politecnico di Milano",
            "Francesco Trov",
            "Politecnico di Milano"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/ExploitingOpponentsUnderUtilityConstraintsinSequentialGames.pdf"
    },
    {
        "internal_id": 129,
        "title": "Learning Treatment Effects in Panels with General Intervention Patterns",
        "abstract": "The problem of causal inference with panel data is a central econometric question. The following is a fundamental version of this problem: Let M \u2217 be a low rank matrix and E be a zero-mean noise matrix. For a 'treatment' matrix Z with entries in {0, 1} we observe the matrix O with entries Oij := M \u2217 ij + Eij + TijZij where Tij are unknown, heterogenous treatment effects. The problem requires we estimate the average treatment effect \u03c4 \u2217 := \ufffd",
        "authors": [
            "Vivek F Farias",
            "MA ",
            "PA ",
            "Tianyi Peng",
            "MA "
        ],
        "created_time": "25 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/LearningTreatmentEffectsinPanelswithGeneralInterventionPatterns.pdf"
    },
    {
        "internal_id": 130,
        "title": "HELP: Hardware-Adaptive Efficient Latency Prediction for NAS via Meta-Learning",
        "abstract": "For deployment, neural architecture search should be hardware-aware, in order to satisfy the device-specific constraints (e.g., memory usage, latency and energy con- sumption) and enhance the model efficiency. Existing methods on hardware-aware NAS collect a large number of samples (e.g., accuracy and latency) from a target device, either builds a lookup table or a latency estimator. However, such approach is impractical in real-world scenarios as there exist numerous devices with different hardware specifications, and collecting samples from such a large number of de- vices will require prohibitive computational and monetary cost. To overcome such limitations, we propose Hardware-adaptive Efficient Latency Predictor (HELP), which formulates the device-specific latency estimation problem as a meta-learning problem, such that we can estimate the latency of a model's performance for a given task on an unseen device with a few samples. To this end, we introduce novel hardware embeddings to embed any devices considering them as black-box func- tions that output latencies, and meta-learn the hardware-adaptive latency predictor in a device-dependent manner, using the hardware embeddings. We validate the proposed HELP for its latency estimation performance on unseen platforms, on which it achieves high estimation performance with as few as 10 measurement samples, outperforming all relevant baselines. We also validate end-to-end NAS frameworks using HELP against ones without it, and show that it largely reduces the total time cost of the base NAS method, in latency-constrained settings. Code is available at https://github.com/HayeonLee/HELP.",
        "authors": [],
        "created_time": "",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/HardwareadaptiveEfficientLatencyPredictionforNASviaMetaLearning.pdf"
    },
    {
        "internal_id": 131,
        "title": "Towards Understanding Why Lookahead Generalizes Better Than SGD and Beyond",
        "abstract": "To train networks, lookahead algorithm [1] updates its fast weights k times via an inner-loop optimizer before updating its slow weights once by using the latest fast weights. Any optimizer, e.g. SGD, can serve as the inner-loop optimizer, and the derived lookahead generally enjoys remarkable test performance improvement over the vanilla optimizer. But theoretical understandings on the test performance improvement of lookahead remain absent yet. To solve this issue, we theoretically justify the advantages of lookahead in terms of the excess risk error which mea- sures the test performance. Specifically, we prove that lookahead using SGD as its inner-loop optimizer can better balance the optimization error and generalization error to achieve smaller excess risk error than vanilla SGD on (strongly) convex problems and nonconvex problems with Polyak-\u0141ojasiewicz condition which has been observed/proved in neural networks. Moreover, we show the stagewise op- timization strategy [2] which decays learning rate several times during training can also benefit lookahead in improving its optimization and generalization errors on strongly convex problems. Finally, we propose a stagewise locally-regularized lookahead (SLRLA) algorithm which sums up the vanilla objective and a local regularizer to minimize at each stage and provably enjoys optimization and gener- alization improvement over the conventional (stagewise) lookahead. Experimental results on CIFAR10/100 and ImageNet testify its advantages. Codes is available at https://github.com/sail-sg/SLRLA-optimizer.",
        "authors": [
            "Pan Zhou",
            "Hanshu Yan",
            "Xiao Tong Yuan",
            "Jiashi Feng",
            "Shuicheng Yan"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/TowardsUnderstandingWhyLookaheadGeneralizesBetterThanSGDandBeyond.pdf"
    },
    {
        "internal_id": 132,
        "title": "Littlestone Classes are Privately Online Learnable",
        "abstract": "We consider the problem of online classification under a privacy constraint. In this setting a learner observes sequentially a stream of labelled examples (\ud835\udc65\ud835\udc61, \ud835\udc66\ud835\udc61), for 1 \u2264 \ud835\udc61 \u2264 \ud835\udc47, and returns at each iteration \ud835\udc61 a hypothesis \u210e\ud835\udc61 which is used to predict the label of each new example \ud835\udc65\ud835\udc61. The learner's performance is measured by her regret against a known hypothesis class H. We require that the algorithm satisfies the following privacy constraint: the sequence \u210e1, . . . , \u210e\ud835\udc47 of hypotheses output by the algorithm needs to be an (\u03f5, \u03b4)-differentially private function of the whole input sequence (\ud835\udc651, \ud835\udc661), . . . , (\ud835\udc65\ud835\udc47 , \ud835\udc66\ud835\udc47 ). We provide the first non-trivial regret bound for the realizable setting. Specifically, we show that if the class H has constant Littlestone dimension then, given an oblivious sequence of labelled examples, there is a private learner that makes in expectation at most \ud835\udc42(log\ud835\udc47) mistakes \u2013 comparable to the optimal mistake bound in the non-private case, up to a logarithmic factor. Moreover, for general values of the Littlestone dimension \ud835\udc51, the same mistake bound holds but with a doubly-exponential in \ud835\udc51 factor. A recent line of work has demonstrated a strong connection between classes that are online learnable and those that are differentially-private learnable. Our results strengthen this connection and show that an online learning algorithm can in fact be directly privatized (in the realizable setting). We also discuss an adaptive setting and provide a sublinear regret bound of \ud835\udc42( \u221a",
        "authors": [
            "Noah Golowich",
            "MIT CSAIl",
            "Roi Livni"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/LittlestoneClassesarePrivatelyOnlineLearnable.pdf"
    },
    {
        "internal_id": 133,
        "title": "Functionally Regionalized Knowledge Transfer for Low-resource Drug Discovery",
        "abstract": "More recently, there has been a surge of interest in employing machine learning approaches to expedite the drug discovery process where virtual screening for hit discovery and ADMET prediction for lead optimization play essential roles. One of the main obstacles to the wide success of machine learning approaches in these two tasks is that the number of compounds labeled with activities or ADMET properties is too small to build an effective predictive model. This paper seeks to remedy the problem by transferring the knowledge from previous assays, namely in-vivo experiments, by different laboratories and against various target proteins. To accommodate these wildly different assays and capture the similarity between assays, we propose a functional rationalized meta-learning algorithm FRML for such knowledge transfer. FRML constructs the predictive model with layers of neural sub-networks or so-called functional regions. Building on this, FRML shares an initialization for the weights of the predictive model across all assays, while customizes it to each assay with a region localization network choosing the pertinent regions. The compositionality of the model improves the capacity of generalization to various and even out-of-distribution tasks. Empirical results on both virtual screening and ADMET prediction validate the superiority of FRML over state-of-the-art baselines powered with interpretability in assay relationship.",
        "authors": [
            "Huaxiu Yao",
            "Ying Wei",
            "Long Kai Huang",
            "Ding Xue",
            "Junzhou Huang",
            "Zhenhui Li"
        ],
        "created_time": "27 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/FunctionallyRegionalizedKnowledgeTransferforLowresourceDrugDiscovery.pdf"
    },
    {
        "internal_id": 134,
        "title": "Forster Decomposition and Learning Halfspaces with Noise",
        "abstract": "A Forster transform is an operation that turns a distribution into one with good anti- concentration properties. While a Forster transform does not always exist, we show that any distribution can be efficiently decomposed as a disjoint mixture of few distributions for which a Forster transform exists and can be computed efficiently. As the main application of this result, we obtain the first polynomial-time algorithm for distribution-independent PAC learning of halfspaces in the Massart noise model with strongly polynomial sample complexity, i.e., independent of the bit complexity of the examples. Previous algorithms for this learning problem incurred sample complexity scaling polynomially with the bit complexity, even though such a dependence is not information-theoretically necessary.",
        "authors": [
            "Ilias Diakonikolas",
            "Daniel M Kane",
            "Christos Tzamos"
        ],
        "created_time": "",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/ForsterDecompositionandLearningHalfspaceswithNoise.pdf"
    },
    {
        "internal_id": 135,
        "title": "FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention",
        "abstract": "We propose FMMformers, a class of efficient and flexible transformers inspired by the celebrated fast multipole method (FMM) for accelerating interacting parti- cle simulation. FMM decomposes particle-particle interaction into near-field and far-field components and then performs direct and coarse-grained computation, respectively. Similarly, FMMformers decompose the attention into near-field and far-field attention, modeling the near-field attention by a banded matrix and the far-field attention by a low-rank matrix. Computing the attention matrix for FMM- formers requires linear complexity in computational time and memory footprint with respect to the sequence length. In contrast, standard transformers suffer from quadratic complexity. We analyze and validate the advantage of FMMformers over the standard transformer on the Long Range Arena and language modeling benchmarks. FMMformers can even outperform the standard transformer in terms of accuracy by a significant margin. For instance, FMMformers achieve an average classification accuracy of 60.74% over the five Long Range Arena tasks, which is significantly better than the standard transformer's average accuracy of 58.70%.",
        "authors": [
            "Tan M Nguyen",
            "Vai Suliafu",
            "Salt Lake City",
            "Stanley J Osher",
            "Long Chen",
            "Bao Wang",
            "Salt Lake City"
        ],
        "created_time": "",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/FMMformerEfficientandFlexibleTransformerviaDecomposedNearfieldandFarfieldAttention.pdf"
    },
    {
        "internal_id": 136,
        "title": "A nonparametric method for gradual change",
        "abstract": "We consider the detection and localization of gradual changes in the distribution of a sequence of time-ordered observations. Existing literature focuses mostly on the simpler abrupt setting which assumes a discontinuity jump in distribution, and is unrealistic for some applied settings. We propose a general method for detecting and localizing gradual changes that does not require a specific data generating model, a particular data type, or prior knowledge about which features of the distribution are subject to change. Despite relaxed assumptions, the proposed method possesses proven theoretical guarantees for both detection and localization.",
        "authors": [
            "Lizhen Nie",
            "Dan Nicolae"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/Anonparametricmethodforgradualchangeproblemswithstatisticalguarantees.pdf"
    },
    {
        "internal_id": 137,
        "title": "Unfolding Taylor's Approximations for Image Restoration",
        "abstract": "Deep learning provides a new avenue for image restoration, which demands a deli- cate balance between fine-grained details and high-level contextualized information during recovering the latent clear image. In practice, however, existing methods em- pirically construct encapsulated end-to-end mapping networks without deepening into the rationality, and neglect the intrinsic prior knowledge of restoration task. To solve the above problems, inspired by Taylor's Approximations, we unfold Taylor's Formula to construct a novel framework for image restoration. We find the main part and the derivative part of Taylor's Approximations take the same effect as the two competing goals of high-level contextualized information and spatial details of image restoration respectively. Specifically, our framework consists of two steps, which are correspondingly responsible for the mapping and derivative functions. The former first learns the high-level contextualized information and the later com- bines it with the degraded input to progressively recover local high-order spatial details. Our proposed framework is orthogonal to existing methods and thus can be easily integrated with them for further improvement, and extensive experiments demonstrate the effectiveness and scalability of our proposed framework.",
        "authors": [
            "Man Zhou",
            "Xueyang Fu",
            "Zeyu Xiao",
            "Gang Yang",
            "Aiping Liu",
            "Zhiwei Xiong"
        ],
        "created_time": "24 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/UnfoldingTaylorsApproximationsforImageRestoration.pdf"
    },
    {
        "internal_id": 138,
        "title": "Asymptotically Exact Error Characterization of Offline Policy Evaluation with Misspecified Linear Models",
        "abstract": "We consider the problem of offline policy evaluation (OPE) with Markov decision processes (MDPs), where the goal is to estimate the utility of given decision- making policies based on static datasets. Recently, theoretical understanding of OPE has been rapidly advanced under (approximate) realizability assumptions, i.e., where the environments of interest are well approximated with the given hypothetical models. On the other hand, the OPE under unrealizability has not been well understood as much as in the realizable setting despite its importance in real-world applications. To address this issue, we study the behavior of a simple existing OPE method called the linear direct method (DM) under the unrealizability. Consequently, we obtain an asymptotically exact characterization of the OPE error in a doubly robust form. Leveraging this result, we also establish the nonparametric consistency of the tile-coding estimators under quite mild assumptions.",
        "authors": [
            "Kohei Miyaguchi"
        ],
        "created_time": "21 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/AsymptoticallyExactErrorCharacterizationofOfflinePolicyEvaluationwithMisspecifiedLinearModels.pdf"
    },
    {
        "internal_id": 139,
        "title": "NEO: Non Equilibrium Sampling on the Orbit of a Deterministic Transform",
        "abstract": "Sampling from a complex distribution \u03c0 and approximating its intractable nor- malizing constant Z are challenging problems. In this paper, a novel family of importance samplers (IS) and Markov chain Monte Carlo (MCMC) samplers is de- rived. Given an invertible map T, these schemes combine (with weights) elements from the forward and backward Orbits through points sampled from a proposal distribution \u03c1. The map T does not leave the target \u03c0 invariant, hence the name NEO, standing for Non-Equilibrium Orbits. NEO-IS provides unbiased estimators of the normalizing constant and self-normalized IS estimators of expectations under \u03c0 while NEO-MCMC combines multiple NEO-IS estimates of the normalizing constant and an iterated sampling-importance resampling mechanism to sample from \u03c0. For T chosen as a discrete-time integrator of a conformal Hamiltonian system, NEO-IS achieves state-of-the art performance on difficult benchmarks and NEO-MCMC is able to explore highly multimodal targets. Additionally, we provide detailed theoretical results for both methods. In particular, we show that NEO-MCMC is uniformly geometrically ergodic and establish explicit mixing time estimates under mild conditions.",
        "authors": [
            "Achille Thin",
            "cole polytechnique",
            "Yazid Janati",
            "D partement CITI",
            "Sylvain Le Corff",
            "D partement CITI",
            "Charles Ollion",
            "cole polytechnique",
            "Arnaud Doucet",
            "Alain Durmus"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/NEONonEquilibriumSamplingontheOrbitsofaDeterministicTransform.pdf"
    },
    {
        "internal_id": 141,
        "title": "Fast Training of Neural Lumigraph Representations using Meta Learning",
        "abstract": "Novel view synthesis is a long-standing problem in machine learning and computer vision. Significant progress has recently been made in developing neural scene representations and rendering techniques that synthesize photorealistic images from arbitrary views. These representations, however, are extremely slow to train and often also slow to render. Inspired by neural variants of image-based rendering, we develop a new neural rendering approach with the goal of quickly learning a high-quality representation which can also be rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a unique combination of a neural shape representation and 2D CNN-based image feature extraction, aggregation, and re-projection. To push representation convergence times down to minutes, we leverage meta learning to learn neural shape and image feature priors which accelerate training. The optimized shape and image features can then be extracted using traditional graphics techniques and rendered in real time. We show that MetaNLR++ achieves similar or better novel view synthesis results in a fraction of the time that competing methods require.",
        "authors": [
            "Petr Kellnhofer",
            "Gordon Wetzstein"
        ],
        "created_time": "24 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/FastTrainingofNeuralLumigraphRepresentationsusingMetaLearning.pdf"
    },
    {
        "internal_id": 142,
        "title": "Sageflow: Robust Federated Learning against Both Stragglers and Adversaries",
        "abstract": "While federated learning (FL) allows efficient model training with local data at edge devices, among major issues still to be resolved are: slow devices known as stragglers and malicious attacks launched by adversaries. While the presence of both of these issues raises serious concerns in practical FL systems, no known schemes or combinations of schemes effectively address them at the same time. We propose Sageflow, staleness-aware grouping with entropy-based filtering and loss-weighted averaging, to handle both stragglers and adversaries simultaneously. Model grouping and weighting according to staleness (arrival delay) provides robustness against stragglers, while entropy-based filtering and loss-weighted averaging, working in a highly complementary fashion at each grouping stage, counter a wide range of adversary attacks. A theoretical bound is established to provide key insights into the convergence behavior of Sageflow. Extensive experimental results show that Sageflow outperforms various existing methods aiming to handle stragglers/adversaries.",
        "authors": [
            "Dong Jun Han",
            "Minseok Choi",
            "Jaekyun Moon"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/SageflowRobustFederatedLearningagainstBothStragglersandAdversaries.pdf"
    },
    {
        "internal_id": 143,
        "title": "Model, sample, and epoch-wise descents: exact solution of gradient flow in the random feature model",
        "abstract": "Recent evidence has shown the existence of a so-called double-descent and even triple-descent behavior for the generalization error of deep-learning models. This important phenomenon commonly appears in implemented neural network ar- chitectures, and also seems to emerge in epoch-wise curves during the training process. A recent line of research has highlighted that random matrix tools can be used to obtain precise analytical asymptotics of the generalization (and training) errors of the random feature model. In this contribution, we analyze the whole temporal behavior of the generalization and training errors under gradient flow for the random feature model. We show that in the asymptotic limit of large system size the full time-evolution path of both errors can be calculated analytically. This allows us to observe how the double and triple descents develop over time, if and when early stopping is an option, and also observe time-wise descent structures. Our techniques are based on Cauchy complex integral representations of the errors together with recent random matrix methods based on linear pencils.",
        "authors": [
            "Antoine Bodin",
            "Nicolas Macris"
        ],
        "created_time": "22 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/Modelsampleandepochwisedescentsexactsolutionofgradientflowintherandomfeaturemodel.pdf"
    },
    {
        "internal_id": 144,
        "title": "SimiGrad: Fine-Grained Adaptive Batching for Large Scale Training using Gradient Similarity Measurement",
        "abstract": "Large scale training requires massive parallelism to finish the training within a reasonable amount of time. To support massive parallelism, large batch training is the key enabler but often at the cost of generalization performance. Existing works explore adaptive batching or hand-tuned static large batching, in order to strike a balance between the computational efficiency and the performance. However, these methods can provide only coarse-grained adaption (e.g., at a epoch level) due to the intrinsic expensive calculation or hand tuning requirements. In this paper, we propose a fully automated and lightweight adaptive batching methodology to enable fine-grained batch size adaption (e.g., at a mini-batch level) that can achieve state- of-the-art performance with record breaking batch sizes. The core component of our method is a lightweight yet efficient representation of the critical gradient noise information. We open-source the proposed methodology by providing a plugin tool that supports mainstream machine learning frameworks. Extensive evaluations on popular benchmarks (e.g., CIFAR10, ImageNet, and BERT-Large) demonstrate that the proposed methodology outperforms state-of-the-art methodologies using adaptive batching approaches or hand-tuned static strategies in both performance and batch size. Particularly, we achieve a new state-of-the-art batch size of 78k in BERT-Large pretraining with SQuAD score 90.69 compared to 90.58 reported in previous state-of-the-art with 59k batch size.",
        "authors": [
            "Heyang Qin",
            "Olatunji Ruwase",
            "Feng Yan",
            "Lei Yang",
            "Yuxiong He"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/SimiGradFineGrainedAdaptiveBatchingforLargeScaleTrainingusingGradientSimilarityMeasurement.pdf"
    },
    {
        "internal_id": 145,
        "title": "Adversarially Robust 3D Point Cloud Recognition Using Self-Supervisions",
        "abstract": "3D point cloud data is increasingly used in safety-critical applications such as autonomous driving. Thus, the robustness of 3D deep learning models against adversarial attacks becomes a major consideration. In this paper, we systemati- cally study the impact of various self-supervised learning proxy tasks on different architectures and threat models for 3D point clouds with adversarial training. Specifically, we study MLP-based (PointNet), convolution-based (DGCNN), and transformer-based (PCT) 3D architectures. Through extensive experimentation, we demonstrate that appropriate applications of self-supervision can significantly enhance the robustness in 3D point cloud recognition, achieving considerable improvements compared to the standard adversarial training baseline. Our analysis reveals that local feature learning is desirable for adversarial robustness in point clouds since it limits the adversarial propagation between the point-level input perturbations and the model's final output. This insight also explains the success of DGCNN and the jigsaw proxy task in achieving stronger 3D adversarial robustness.",
        "authors": [
            "Jiachen Sun ",
            "Yulong Cao ",
            "Christopher Choy ",
            "Zhiding Yu ",
            " NVIDIA",
            " Caltech",
            " ASU"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/AdversariallyRobust3DPointCloudRecognitionUsingSelfSupervisions.pdf"
    },
    {
        "internal_id": 146,
        "title": "TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification",
        "abstract": "Multiple instance learning (MIL) is a powerful tool to solve the weakly super- vised classification in whole slide image (WSI) based pathology diagnosis. How- ever, the current MIL methods are usually based on independent and identical distribution hypothesis, thus neglect the correlation among different instances. To address this problem, we proposed a new framework, called correlated MIL, and provided a proof for convergence. Based on this framework, we devised a Transformer based MIL (TransMIL), which explored both morphological and spatial information. The proposed TransMIL can effectively deal with unbal- anced/balanced and binary/multiple classification with great visualization and in- terpretability. We conducted various experiments for three different computational pathology problems and achieved better performance and faster convergence com- pared with state-of-the-art methods. The test AUC for the binary tumor classi- fication can be up to 93.09% over CAMELYON16 dataset. And the AUC over the cancer subtypes classification can be up to 96.03% and 98.82% over TCGA- NSCLC dataset and TCGA-RCC dataset, respectively. Implementation is avail- able at: https://github.com/szc19990412/TransMIL.",
        "authors": [
            "Zhuchen Shao",
            "Hao Bian",
            "Yang Chen",
            "Yifeng Wang",
            "Jian Zhang",
            "Xiangyang Ji",
            "Yongbing Zhang"
        ],
        "created_time": "18 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/TransMILTransformerbasedCorrelatedMultipleInstanceLearningforWholeSlideImageClassification.pdf"
    },
    {
        "internal_id": 147,
        "title": "Latent Matters: Learning Deep State-Space Models",
        "abstract": "Deep state-space models (DSSMs) enable temporal predictions by learning the underlying dynamics of observed sequence data. They are often trained by max- imising the evidence lower bound. However, as we show, this does not ensure the model actually learns the underlying dynamics. We therefore propose a constrained optimisation framework as a general approach for training DSSMs. Building upon this, we introduce the extended Kalman VAE (EKVAE), which combines amortised variational inference with classic Bayesian filtering/smoothing to model dynamics more accurately than RNN-based DSSMs. Our results show that the constrained optimisation framework significantly improves system identification and prediction accuracy on the example of established state-of-the-art DSSMs. The EKVAE out- performs previous models w.r.t. prediction accuracy, achieves remarkable results in identifying dynamical systems, and can furthermore successfully learn state-space representations where static and dynamic features are disentangled.",
        "authors": [
            "Alexej Klushyn ",
            "Richard Kurle ",
            "Maximilian Soelch",
            "Botond Cseke",
            "Volkswagen Group"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/LatentMattersLearningDeepStateSpaceModels.pdf"
    },
    {
        "internal_id": 148,
        "title": "On Riemannian Optimization over Positive Definite Matrices with the Bures-Wasserstein Geometry",
        "abstract": "In this paper, we comparatively analyze the Bures-Wasserstein (BW) geometry with the popular Affine-Invariant (AI) geometry for Riemannian optimization on the symmetric positive definite (SPD) matrix manifold. Our study begins with an observation that the BW metric has a linear dependence on SPD matrices in contrast to the quadratic dependence of the AI metric. We build on this to show that the BW metric is a more suitable and robust choice for several Riemannian optimization problems over ill-conditioned SPD matrices. We show that the BW geometry has a non-negative curvature, which further improves convergence rates of algorithms over the non-positively curved AI geometry. Finally, we verify that several popular cost functions, which are known to be geodesic convex under the AI geometry, are also geodesic convex under the BW geometry. Extensive experiments on various applications support our findings.",
        "authors": [
            "Bamdev Mishra",
            "Pratik Jawanpuria",
            "Junbin Gao"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/OnRiemannianOptimizationoverPositiveDefiniteMatriceswiththeBuresWassersteinGeometry.pdf"
    },
    {
        "internal_id": 150,
        "title": "Replay-Guided Adversarial Environment Design",
        "abstract": "Deep reinforcement learning (RL) agents may successfully generalize to new settings if trained on an appropriately diverse set of environment and task configurations. Unsupervised Environment Design (UED) is a promising self- supervised RL paradigm, wherein the free parameters of an underspecified environment are automatically adapted during training to the agent's capabilities, leading to the emergence of diverse training environments. Here, we cast Prioritized Level Replay (PLR), an empirically successful but theoretically unmotivated method that selectively samples randomly-generated training levels, as UED. We argue that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training. This insight reveals a natural class of UED methods we call Dual Curriculum Design (DCD). Crucially, DCD includes both PLR and a popular UED algorithm, PAIRED, as special cases and inherits similar theoretical guarantees. This connection allows us to develop novel theory for PLR, providing a version with a robustness guarantee at Nash equilibria. Furthermore, our theory suggests a highly counterintuitive improvement to PLR: by stopping the agent from updating its policy on uncurated levels (training on less data), we can improve the convergence to Nash equilibria. Indeed, our experiments confirm that our new method, PLR\u22a5, obtains better results on a suite of out-of-distribution, zero-shot transfer tasks, in addition to demonstrating that PLR\u22a5 improves the performance of PAIRED, from which it inherited its theoretical framework.",
        "authors": [
            "Minqi Jiang",
            "Michael Dennis",
            "Jack Parker Holder",
            "Jakob Foerster",
            "Edward Grefenstette",
            "Tim Rockt schel"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/ReplayGuidedAdversarialEnvironmentDesign.pdf"
    },
    {
        "internal_id": 151,
        "title": "Entropy-based adaptive Hamiltonian Monte Carlo",
        "abstract": "Hamiltonian Monte Carlo (HMC) is a popular Markov Chain Monte Carlo (MCMC) algorithm to sample from an unnormalized probability distribution. A leapfrog integrator is commonly used to implement HMC in practice, but its per- formance can be sensitive to the choice of mass matrix used therein. We develop a gradient-based algorithm that allows for the adaptation of the mass matrix by encouraging the leapfrog integrator to have high acceptance rates while also ex- ploring all dimensions jointly. In contrast to previous work that adapt the hyper- parameters of HMC using some form of expected squared jumping distance, the adaptation strategy suggested here aims to increase sampling efficiency by maxi- mizing an approximation of the proposal entropy. We illustrate that using multiple gradients in the HMC proposal can be beneficial compared to a single gradient- step in Metropolis-adjusted Langevin proposals. Empirical evidence suggests that the adaptation method can outperform different versions of HMC schemes by ad- justing the mass matrix to the geometry of the target distribution and by providing some control on the integration time.",
        "authors": [
            "Marcel Hirt",
            "Michalis K Titsias",
            "Petros Dellaportas"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/EntropybasedadaptiveHamiltonianMonteCarlo.pdf"
    },
    {
        "internal_id": 152,
        "title": "S3: Sign-Sparse-Shift Reparametrization for Effective Training of Low-bit Shift Networks",
        "abstract": "Shift neural networks reduce computation complexity by removing expensive multiplication operations and quantizing continuous weights into low-bit discrete values, which are fast and energy-efficient compared to conventional neural net- works. However, existing shift networks are sensitive to the weight initialization and yield a degraded performance caused by vanishing gradient and weight sign freezing problem. To address these issues, we propose S3 re-parameterization, a novel technique for training low-bit shift networks. Our method decomposes a discrete parameter in a sign-sparse-shift 3-fold manner. This way, it efficiently learns a low-bit network with weight dynamics similar to full-precision networks and insensitive to weight initialization. Our proposed training method pushes the boundaries of shift neural networks and shows 3-bit shift networks compete with their full-precision counterparts in terms of top-1 accuracy on ImageNet.",
        "authors": [
            "Xinlin Li",
            "Bang Liu",
            "Yaoliang Yu",
            "Wulong Liu",
            "Chunjing Xu"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/S3SignSparseShiftReparametrizationforEffectiveTrainingofLowbitShiftNetworks.pdf"
    },
    {
        "internal_id": 153,
        "title": "INDIGO: GNN-Based Inductive Knowledge Graph Completion Using Pair-Wise Encoding",
        "abstract": "The aim of knowledge graph (KG) completion is to extend an incomplete KG with missing triples. Popular approaches based on graph embeddings typically work by first representing the KG in a vector space, and then applying a predefined scoring function to the resulting vectors to complete the KG. These approaches work well in transductive settings, where predicted triples involve only constants seen during training; however, they are not applicable in inductive settings, where the KG on which the model was trained is extended with new constants or merged with other KGs. The use of Graph Neural Networks (GNNs) has recently been proposed as a way to overcome these limitations; however, existing approaches do not fully exploit the capabilities of GNNs and still rely on heuristics and ad- hoc scoring functions. In this paper, we propose a novel approach, where the KG is fully encoded into a GNN in a transparent way, and where the predicted triples can be read out directly from the last layer of the GNN without the need for additional components or scoring functions. Our experiments show that our model outperforms state-of-the-art approaches on inductive KG completion benchmarks.",
        "authors": [
            "Shuwen Liu",
            "Bernardo Cuenca Grau",
            "Ian Horrocks"
        ],
        "created_time": "30 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/INDIGOGNNBasedInductiveKnowledgeGraphCompletionUsingPairWiseEncoding.pdf"
    },
    {
        "internal_id": 154,
        "title": "Directional Message Passing on Molecular Graphs via Synthetic Coordinates",
        "abstract": "Graph neural networks that leverage coordinates via directional message passing have recently set the state of the art on multiple molecular property prediction tasks. However, they rely on atom position information that is often unavailable, and obtaining it is usually prohibitively expensive or even impossible. In this paper we propose synthetic coordinates that enable the use of advanced GNNs without requiring the true molecular configuration. We propose two distances as synthetic coordinates: Distance bounds that specify the rough range of molecular configurations, and graph-based distances using a symmetric variant of personalized PageRank. To leverage both distance and angular information we propose a method of transforming normal graph neural networks into directional MPNNs. We show that with this transformation we can reduce the error of a normal graph neural network by 55 % on the ZINC benchmark. We furthermore set the state of the art on ZINC and coordinate-free QM9 by incorporating synthetic coordinates in the SMP and DimeNet++ models. Our implementation is available online. 1",
        "authors": [
            "Johannes Klicpera",
            "Stephan G nnemann"
        ],
        "created_time": "07 November 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/DirectionalMessagePassingonMolecularGraphsviaSyntheticCoordinates.pdf"
    },
    {
        "internal_id": 155,
        "title": "Photonic Differential Privacy with Direct Feedback Alignment",
        "abstract": "Optical Processing Units (OPUs) \u2013 low-power photonic chips dedicated to large scale random projections \u2013 have been used in previous work to train deep neural networks using Direct Feedback Alignment (DFA), an effective alternative to backpropagation. Here, we demonstrate how to leverage the intrinsic noise of optical random projections to build a differentially private DFA mechanism, making OPUs a solution of choice to provide a private-by-design training. We provide a theoretical analysis of our adaptive privacy mechanism, carefully measuring how the noise of optical random projections propagates in the process and gives rise to provable Differential Privacy. Finally, we conduct experiments demonstrating the ability of our learning procedure to achieve solid end-task performance.",
        "authors": [
            "Ruben Ohana ",
            "Julien Launay ",
            "Iacopo Poli",
            "Liva Ralaivola",
            "Alain Rakotomamonjy"
        ],
        "created_time": "25 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/PhotonicDifferentialPrivacywithDirectFeedbackAlignment.pdf"
    },
    {
        "internal_id": 156,
        "title": "Scatterbrain: Unifying",
        "abstract": "Recent advances in efficient Transformers have exploited either the sparsity or low-rank properties of attention matrices to reduce the computational and memory bottlenecks of modeling long sequences. However, it is still challenging to balance the trade-off between model quality and efficiency to perform a one-size-fits-all approximation for different tasks. To better understand this trade-off, we observe that sparse and low-rank approximations excel in different regimes, determined by the softmax temperature in attention, and sparse + low-rank can outperform each individually. Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, we propose Scatterbrain, a novel way to unify sparse (via locality sensitive hashing) and low-rank (via kernel feature map) attention for accurate and efficient approximation. The estimation is unbiased with provably low error. We empirically show that Scatterbrain can achieve 2.1\u21e5 lower error than baselines when serving as a drop-in replacement in BigGAN image generation and pre-trained T2T-ViT. On a pre-trained T2T Vision transformer, even without fine-tuning, Scatterbrain can reduce 98% of attention memory at the cost of only 1% drop in accuracy. We demonstrate Scatterbrain for end-to-end training with up to 4 points better perplexity and 5 points better average accuracy than sparse or low-rank efficient transformers on language modeling and long-range-arena tasks.",
        "authors": [
            "Beidi Chen",
            "Tri Dao",
            "Eric Winsor",
            "Zhao Song",
            "Atri Rudra",
            "Christopher R"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/ScatterbrainUnifyingSparseandLowrankAttention.pdf"
    },
    {
        "internal_id": 157,
        "title": "Video Instance Segmentation using Inter-Frame Communication Transformers",
        "abstract": "We propose a novel end-to-end solution for video instance segmentation (VIS) based on transformers. Recently, the per-clip pipeline shows superior performance over per-frame methods leveraging richer information from multiple frames. How- ever, previous per-clip models require heavy computation and memory usage to achieve frame-to-frame communications, limiting practicality. In this work, we pro- pose Inter-frame Communication Transformers (IFC), which significantly reduces the overhead for information-passing between frames by efficiently encoding the context within the input clip. Specifically, we propose to utilize concise memory tokens as a mean of conveying information as well as summarizing each frame scene. The features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens. We validate our method on the latest benchmark sets and achieved the state-of-the- art performance (AP 42.6 on YouTube-VIS 2019 val set using the offline inference) while having a considerably fast runtime (89.4 FPS). Our method can also be applied to near-online inference for processing a video in real-time with only a small delay. The code is available at https://github.com/sukjunhwang/IFC.",
        "authors": [
            "Miran Heo",
            "Seoung Wug Oh",
            "Seon Joo Kim"
        ],
        "created_time": "25 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/VideoInstanceSegmentationusingInterFrameCommunicationTransformers.pdf"
    },
    {
        "internal_id": 158,
        "title": "An Efficient Pessimistic-Optimistic Algorithm for Stochastic Linear Bandits with General Constraints",
        "abstract": "This paper considers stochastic linear bandits with general nonlinear constraints. The objective is to maximize the expected cumulative reward over horizon T subject to a set of constraints in each round \u03c4 \u010f T. We propose a pessimistic- optimistic algorithm for this problem, which is efficient in two aspects. First, the algorithm yields \u02dcO \u00b4\u00b4 K0.75",
        "authors": [
            "Xin Liu",
            "Ann Arbor",
            "Bin Li",
            "Pengyi Shi",
            "Lei Ying",
            "Ann Arbor"
        ],
        "created_time": "25 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/AnEfficientPessimisticOptimisticAlgorithmforStochasticLinearBanditswithGeneralConstraints.pdf"
    },
    {
        "internal_id": 159,
        "title": "Open Rule Induction",
        "abstract": "Rules have a number of desirable properties. It is easy to understand, infer new knowledge, and communicate with other inference systems. One weakness of the previous rule induction systems is that they only find rules within a knowledge base (KB) and therefore cannot generalize to more open and complex real-world rules. Recently, the language model (LM)-based rule generation are proposed to enhance the expressive power of the rules. In this paper, we revisit the differences between KB-based rule induction and LM-based rule generation. We argue that, while KB-based methods inducted rules by discovering data commonalities, the current LM-based methods are \"learning rules from rules\". This limits these methods to only produce \"canned\" rules whose patterns are constrained by the annotated rules, while discarding the rich expressive power of LMs for free text. Therefore, in this paper, we propose the open rule induction problem, which aims to induce open rules utilizing the knowledge in LMs. Besides, we propose the Orion (open rule induction) system to automatically mine open rules from LMs without supervision of annotated rules. We conducted extensive experiments to verify the quality and quantity of the inducted open rules. Surprisingly, when applying the open rules in downstream tasks (i.e. relation extraction), these automatically inducted rules even outperformed the manually annotated rules. 2",
        "authors": [
            "Wanyun Cui",
            "Xingran Chen"
        ],
        "created_time": "05 November 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/OpenRuleInduction.pdf"
    },
    {
        "internal_id": 160,
        "title": "Self-Paced Contrastive Learning for Semi-supervised Medical Image Segmentation with Meta-labels",
        "abstract": "The contrastive pre-training of a recognition model on a large dataset of unlabeled data often boosts the model's performance on downstream tasks like image classifi- cation. However, in domains such as medical imaging, collecting unlabeled data can be challenging and expensive. In this work, we consider the task of medical image segmentation and adapt contrastive learning with meta-label annotations to scenarios where no additional unlabeled data is available. Meta-labels, such as the location of a 2D slice in a 3D MRI scan, often come for free during the acquisition process. We use these meta-labels to pre-train the image encoder, as well as in a semi-supervised learning step that leverages a reduced set of annotated data. A self-paced learning strategy exploiting the weak annotations is proposed to further help the learning process and discriminate useful labels from noise. Results on five medical image segmentation datasets show that our approach: i) highly boosts the performance of a model trained on a few scans, ii) outperforms previous contrastive and semi-supervised approaches, and iii) reaches close to the performance of a model trained on the full data.",
        "authors": [
            "Jizong Peng",
            "ETS Montreal",
            "Ping Wang",
            "ETS Montreal",
            "Christian Desrosiers",
            "ETS Montreal",
            "Marco Pedersoli",
            "ETS Montreal"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/SelfPacedContrastiveLearningforSemisupervisedMedicalImageSegmentationwithMetalabels.pdf"
    },
    {
        "internal_id": 161,
        "title": "Addressing Algorithmic Disparity and Performance Inconsistency in Federated Learning",
        "abstract": "Federated learning (FL) has gain growing interests for its capability of learning from distributed data sources collectively without the need of accessing the raw data samples across different sources. So far FL research has mostly focused on improving the performance, how the algorithmic disparity will be impacted for the model learned from FL and the impact of algorithmic disparity on the utility inconsistency are largely unexplored. In this paper, we propose an FL framework to jointly consider performance consistency and algorithmic fairness across different local clients (data sources). We derive our framework from a constrained multi- objective optimization perspective, in which we learn a model satisfying fairness constraints on all clients with consistent performance. Specifically, we treat the algorithm prediction loss at each local client as an objective and maximize the worst-performing client with fairness constraints through optimizing a surrogate maximum function with all objectives involved. A gradient-based procedure is employed to achieve the Pareto optimality of this optimization problem. Theoretical analysis is provided to prove that our method can converge to a Pareto solution that achieves the min-max performance with fairness constraints on all clients. Comprehensive experiments on synthetic and real-world datasets demonstrate the superiority that our approach over baselines and its effectiveness in achieving both fairness and consistency across all local clients.",
        "authors": [
            "Sen Cui",
            "Weishen Pan",
            "Jian Liang",
            "Changshui Zhang",
            "Fei Wang",
            " Alibaba Group",
            "Weill Cornell Medicine"
        ],
        "created_time": "20 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/AddressingAlgorithmicDisparityandPerformanceInconsistencyinFederatedLearning.pdf"
    },
    {
        "internal_id": 162,
        "title": "DOCTOR: A Simple Method for Detecting Misclassification Errors",
        "abstract": "Deep neural networks (DNNs) have shown to perform very well on large scale object recognition problems and lead to widespread use for real-world applications, including situations where DNN are implemented as \"black boxes\". A promising approach to secure their use is to accept decisions that are likely to be correct while discarding the others. In this work, we propose DOCTOR, a simple method that aims to identify whether the prediction of a DNN classifier should (or should not) be trusted so that, consequently, it would be possible to accept it or to reject it. Two scenarios are investigated: Totally Black Box (TBB) where only the soft-predictions are available and Partially Black Box (PBB) where gradient-propagation to perform input pre-processing is allowed. Empirically, we show that DOCTOR outperforms all state-of-the-art methods on various well-known images and sentiment analysis datasets. In particular, we observe a reduction of up to 4% of the false rejection rate (FRR) in the PBB scenario. DOCTOR can be applied to any pre-trained model, it does not require prior information about the underlying dataset and is as simple as the simplest available methods in the literature.",
        "authors": [
            "Federica Granese",
            "Marco Romanelli",
            "CentraleSup lec",
            "Universit Paris Saclay",
            "Daniele Gorla",
            "Catuscia Palamidessi",
            "Pablo Piantanida",
            "CentraleSup lec",
            "Universit Paris Saclay"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/DOCTORASimpleMethodforDetectingMisclassificationErrors.pdf"
    },
    {
        "internal_id": 163,
        "title": "Long Short-Term Transformer for Online Action Detection",
        "abstract": "We present Long Short-term TRansformer (LSTR), a temporal modeling algo- rithm for online action detection, which employs a long- and short-term memory mechanism to model prolonged sequence data. It consists of an LSTR encoder that dynamically leverages coarse-scale historical information from an extended temporal window (e.g., 2048 frames spanning of up to 8 minutes), together with an LSTR decoder that focuses on a short time window (e.g., 32 frames spanning 8 seconds) to model the fine-scale characteristics of the data. Compared to prior work, LSTR provides an effective and efficient method to model long videos with fewer heuristics, which is validated by extensive empirical analysis. LSTR achieves state-of-the-art performance on three standard online action detection benchmarks, THUMOS'14, TVSeries, and HACS Segment.",
        "authors": [
            "Mingze Xu",
            "Yuanjun Xiong",
            "Hao Chen",
            "Xinyu Li",
            "Wei Xia",
            "Zhuowen Tu",
            "Stefano Soatto"
        ],
        "created_time": "28 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/LongShortTermTransformerforOnlineActionDetection.pdf"
    },
    {
        "internal_id": 164,
        "title": "Minimax Regret for Stochastic Shortest Path",
        "abstract": "We study the Stochastic Shortest Path (SSP) problem in which an agent has to reach a goal state in minimum total expected cost. In the learning formulation of the problem, the agent has no prior knowledge about the costs and dynamics of the model. She repeatedly interacts with the model for K episodes, and has to minimize her regret. In this work we show that the minimax regret for this setting is \ufffdO( \ufffd",
        "authors": [
            "Alon Cohen",
            "Tel Aviv",
            "Yonathan Efroni",
            "New York",
            "Yishay Mansour",
            "Tel Aviv",
            "Aviv Rosenberg"
        ],
        "created_time": "",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/MinimaxRegretforStochasticShortestPath.pdf"
    },
    {
        "internal_id": 165,
        "title": "Learning a Single Neuron with Bias Using Gradient Descent",
        "abstract": "We theoretically study the fundamental problem of learning a single neuron with a bias term (x \ufffd\u2192 \u03c3(\u27e8w, x\u27e9 + b)) in the realizable setting with the ReLU activation, using gradient descent. Perhaps surprisingly, we show that this is a significantly different and more challenging problem than the bias-less case (which was the focus of previous works on single neurons), both in terms of the optimization geometry as well as the ability of gradient methods to succeed in some scenarios. We provide a detailed study of this problem, characterizing the critical points of the objective, demonstrating failure cases, and providing positive convergence guarantees under different sets of assumptions. To prove our results, we develop some tools which may be of independent interest, and improve previous results on learning single neurons.",
        "authors": [
            "Gal Vardi",
            "Gilad Yehudai",
            "Ohad Shamir"
        ],
        "created_time": "",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/LearningaSingleNeuronwithBiasUsingGradientDescent.pdf"
    },
    {
        "internal_id": 166,
        "title": "Graph Differentiable Architecture Search with Structure Learning",
        "abstract": "Discovering ideal Graph Neural Networks (GNNs) architectures for different tasks is labor intensive and time consuming. To save human efforts, Neural Architecture Search (NAS) recently has been used to automatically discover adequate GNN architectures for certain tasks in order to achieve competitive or even better perfor- mance compared with manually designed architectures. However, existing works utilizing NAS to search GNN structures fail to answer the question How NAS is able to select the desired GNN architectures. In this paper, we investigate this question to solve the problem, for the first time. We conduct theoretical analysis and measurement study with experiments to discover that gradient based NAS methods tend to select proper architectures based on the usefulness of different types of information with respect to the target task. Our explorations further show that gradient based NAS also suffers from noises hidden in the graph, resulting in searching suboptimal GNN architectures. Based on our findings, we propose a Graph differentiable Architecture Search model with Structure Optimization (GASSO), which allows differentiable search of the architecture with gradient descent and is able to discover graph neural architectures with better performance through employing graph structure learning as a denoising process in the search procedure. Extensive experiments on real-world graph datasets demonstrate that our proposed GASSO model is able to achieve the state-of-the-art performance compared with existing baselines.",
        "authors": [
            "Yijian Qin",
            "Xin Wang",
            "Zeyang Zhang",
            "Wenwu Zhu"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/GraphDifferentiableArchitectureSearchwithStructureLearning.pdf"
    },
    {
        "internal_id": 167,
        "title": "Large-Scale Unsupervised Object Discovery",
        "abstract": "Existing approaches to unsupervised object discovery (UOD) do not scale up to large datasets without approximations that compromise their performance. We propose a novel formulation of UOD as a ranking problem, amenable to the arsenal of distributed methods available for eigenvalue problems and link analysis. Through the use of self-supervised features, we also demonstrate the first effective fully unsupervised pipeline for UOD. Extensive experiments on COCO [42] and OpenImages [35] show that, in the single-object discovery setting where a single prominent object is sought in each image, the proposed LOD (Large-scale Object Discovery) approach is on par with, or better than the state of the art for medium- scale datasets (up to 120K images), and over 37% better than the only other algorithms capable of scaling up to 1.7M images. In the multi-object discovery setting where multiple objects are sought in each image, the proposed LOD is over 14% better in average precision (AP) than all other methods for datasets ranging from 20K to 1.7M images. Using self-supervised features, we also show that the proposed method obtains state-of-the-art UOD performance on OpenImages1.",
        "authors": [
            "Huy V Vo",
            "Elena Sizikova",
            "Cordelia Schmid",
            "Patrick P rez",
            "Jean Ponce",
            "Valeo ai"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/LargeScaleUnsupervisedObjectDiscovery.pdf"
    },
    {
        "internal_id": 168,
        "title": "Optimizing Conditional Value-At-Risk of Black-Box Functions",
        "abstract": "This paper presents two Bayesian optimization (BO) algorithms with theoretical performance guarantee to maximize the conditional value-at-risk (CVaR) of a black-box function: CV-UCB and CV-TS which are based on the well-established principle of optimism in the face of uncertainty and Thompson sampling, respec- tively. To achieve this, we develop an upper confidence bound of CVaR and prove the no-regret guarantee of CV-UCB by utilizing an interesting connection between CVaR and value-at-risk (VaR). For CV-TS, though it is straightforwardly performed with Thompson sampling, bounding its Bayesian regret is non-trivial because it re- quires a tail expectation bound for the distribution of CVaR of a black-box function, which has not been shown in the literature. The performances of both CV-UCB and CV-TS are empirically evaluated in optimizing CVaR of synthetic benchmark functions and simulated real-world optimization problems.",
        "authors": [
            "Quoc Phong Nguyen",
            "Zhongxiang Dai"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/OptimizingConditionalValueAtRiskofBlackBoxFunctions.pdf"
    },
    {
        "internal_id": 169,
        "title": "Improved Transformer for High-Resolution GANs",
        "abstract": "Attention-based models, exemplified by the Transformer, can effectively model long range dependency, but suffer from the quadratic complexity of self-attention operation, making them difficult to be adopted for high-resolution image generation based on Generative Adversarial Networks (GANs). In this paper, we introduce two key ingredients to Transformer to address this challenge. First, in low-resolution stages of the generative process, standard global self-attention is replaced with the proposed multi-axis blocked self-attention which allows efficient mixing of local and global attention. Second, in high-resolution stages, we drop self-attention while only keeping multi-layer perceptrons reminiscent of the implicit neural function. To further improve the performance, we introduce an additional self-modulation component based on cross-attention. The resulting model, denoted as HiT, has a nearly linear computational complexity with respect to the image size and thus directly scales to synthesizing high definition images. We show in the experiments that the proposed HiT achieves state-of-the-art FID scores of 31.87 and 2.95 on unconditional ImageNet 128 \u00d7 128 and FFHQ 256 \u00d7 256, respectively, with a reasonable throughput. We believe the proposed HiT is an important milestone for generators in GANs which are completely free of convolutions.",
        "authors": [
            "Long Zhao",
            "Zizhao Zhang",
            "Ting Chen",
            "Dimitris N Metaxas",
            "Han Zhang",
            "Google Cloud AI"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/ImprovedTransformerforHighResolutionGANs.pdf"
    },
    {
        "internal_id": 170,
        "title": "What Matters for Adversarial Imitation Learning?",
        "abstract": "Adversarial imitation learning has become a popular framework for imitation in continuous control. Over the years, several variations of its components were proposed to enhance the performance of the learned policies as well as the sample complexity of the algorithm. In practice, these choices are rarely tested all together in rigorous empirical studies. It is therefore difficult to discuss and understand what choices, among the high-level algorithmic options as well as low-level im- plementation details, matter. To tackle this issue, we implement more than 50 of these choices in a generic adversarial imitation learning framework and investigate their impacts in a large-scale study (>500k trained agents) with both synthetic and human-generated demonstrations. We analyze the key results and highlight the most surprising findings.",
        "authors": [
            "Manu Orsini",
            "L onard Hussenot",
            "Robert Dadashi",
            "Sertan Girgin",
            "Matthieu Geist",
            "Olivier Bachem",
            "Olivier Pietquin"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/WhatMattersforAdversarialImitationLearning.pdf"
    },
    {
        "internal_id": 171,
        "title": "Subgame solving without common knowledge",
        "abstract": "In imperfect-information games, subgame solving is significantly more challenging than in perfect-information games, but in the last few years, such techniques have been developed. They were the key ingredient to the milestone of superhuman play in no-limit Texas hold'em poker. Current subgame-solving techniques analyze the entire common-knowledge closure of the player's current information set, that is, the smallest set of nodes within which it is common knowledge that the current node lies. While this is acceptable in games like poker where the common- knowledge closure is relatively small, many practical games have more complex information structure, which renders the common-knowledge closure impractically large to enumerate or even reasonably approximate. We introduce an approach that overcomes this obstacle, by instead working with only low-order knowledge. Our approach allows an agent, upon arriving at an infoset, to basically prune any node that is no longer reachable, thereby massively reducing the game tree size relative to the common-knowledge subgame. We prove that, as is, our approach can increase exploitability compared to the blueprint strategy. However, we develop three avenues by which safety can be guaranteed. First, safety is guaranteed if the results of subgame solves are incorporated back into the blueprint. Second, we provide a method where safety is achieved by limiting the infosets at which subgame solving is performed. Third, we prove that our approach, when applied at every infoset reached during play, achieves a weaker notion of equilibrium, which we coin affine equilibrium, and which may be of independent interest. We show that affine equilibria cannot be exploited by any Nash strategy of the opponent, so an opponent who wishes to exploit must open herself to counter-exploitation. Even without the safety-guaranteeing additions, experiments on medium-sized games show that our approach always reduced exploitability in practical games even when applied at every infoset, and a depth-limited version of it led to\u2014to our knowledge\u2014the first strong AI for the challenge problem dark chess.",
        "authors": [
            "Brian Hu Zhang",
            "Strategy Robot",
            "Optimized Markets"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/Subgamesolvingwithoutcommonknowledge.pdf"
    },
    {
        "internal_id": 172,
        "title": "Alignment Attention by Matching Key and Query Distributions",
        "abstract": "The neural attention mechanism has been incorporated into deep neural networks to achieve state-of-the-art performance in various domains. Most such models use multi-head self-attention which is appealing for the ability to attend to informa- tion from different perspectives. This paper introduces alignment attention that explicitly encourages self-attention to match the distributions of the key and query within each head. The resulting alignment attention networks can be optimized as an unsupervised regularization in the existing attention framework. It is simple to convert any models with self-attention, including pre-trained ones, to the proposed alignment attention. On a variety of language understanding tasks, we show the effectiveness of our method in accuracy, uncertainty estimation, generalization across domains, and robustness to adversarial attacks. We further demonstrate the general applicability of our approach on graph attention and visual question answering, showing the great potential of incorporating our alignment method into various attention-related tasks.",
        "authors": [
            "Shujian Zhang",
            "Xinjie Fan",
            "Huangjie Zheng",
            "Korawat Tanwisuth",
            "Mingyuan Zhou"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/AlignmentAttentionbyMatchingKeyandQueryDistributions.pdf"
    },
    {
        "internal_id": 174,
        "title": "Multiwavelet-based Operator Learning for Differential Equations",
        "abstract": "The solution of a partial differential equation can be obtained by computing the inverse operator map between the input and the solution space. Towards this end, we introduce a multiwavelet-based neural operator learning scheme that com- presses the associated operator's kernel using fine-grained wavelets. By explicitly embedding the inverse multiwavelet filters, we learn the projection of the kernel onto fixed multiwavelet polynomial bases. The projected kernel is trained at mul- tiple scales derived from using repeated computation of multiwavelet transform. This allows learning the complex dependencies at various scales and results in a resolution-independent scheme. Compare to the prior works, we exploit the fundamental properties of the operator's kernel which enable numerically efficient representation. We perform experiments on the Korteweg-de Vries (KdV) equation, Burgers' equation, Darcy Flow, and Navier-Stokes equation. Compared with the existing neural operator approaches, our model shows significantly higher accuracy and achieves state-of-the-art in a range of datasets. For the time-varying equations, the proposed method exhibits a (2X\u221210X) improvement (0.0018 (0.0033) relative L2 error for Burgers' (KdV) equation). By learning the mappings between function spaces, the proposed method has the ability to find the solution of a high-resolution input after learning from lower-resolution data.",
        "authors": [
            "Gaurav Gupta",
            "Xiongye Xiao",
            "Paul Bogdan",
            "CA "
        ],
        "created_time": "22 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/MultiwaveletbasedOperatorLearningforDifferentialEquations.pdf"
    },
    {
        "internal_id": 175,
        "title": "Towards understanding retrosynthesis by energy-based models",
        "abstract": "Retrosynthesis is the process of identifying a set of reactants to synthesize a target molecule. It is critical to material design and drug discovery. Existing machine learning approaches based on language models and graph neural networks have achieved encouraging results. However, the inner connections of these models are rarely discussed, and rigorous evaluations of these models are largely in need. In this paper, we propose a framework that unifies sequence- and graph-based methods as energy-based models (EBMs) with different energy functions. This unified view establishes connections and reveals the differences between models, thereby enhances our understanding of model design. We also provide a comprehensive assessment of performance to the community. Additionally, we present a novel dual variant within the framework that performs consistent training to induce the agreement between forward- and backward-prediction. This model improves the state-of-the-art of template-free methods with or without reaction types.",
        "authors": [
            "Ruoxi Sun",
            "Hanjun Dai",
            "Li Li",
            "Steven Kearnes"
        ],
        "created_time": "",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/Towardsunderstandingretrosynthesisbyenergybasedmodels.pdf"
    },
    {
        "internal_id": 176,
        "title": "Implicit Transformer Network for Screen Content Image Continuous Super-Resolution",
        "abstract": "Nowadays, there is an explosive growth of screen contents due to the wide ap- plication of screen sharing, remote cooperation, and online education. To match the limited terminal bandwidth, high-resolution (HR) screen contents may be downsampled and compressed. At the receiver side, the super-resolution (SR) of low-resolution (LR) screen content images (SCIs) is highly demanded by the HR display or by the users to zoom in for detail observation. However, image SR methods mostly designed for natural images do not generalize well for SCIs due to the very different image characteristics as well as the requirement of SCI browsing at arbitrary scales. To this end, we propose a novel Implicit Transformer Super-Resolution Network (ITSRN) for SCISR. For high-quality continuous SR at arbitrary ratios, pixel values at query coordinates are inferred from image features at key coordinates by the proposed implicit transformer and an implicit position encoding scheme is proposed to aggregate similar neighboring pixel values to the query one. We construct benchmark SCI1K and SCI1K-compression datasets with LR and HR SCI pairs. Extensive experiments show that the proposed ITSRN significantly outperforms several competitive continuous and discrete SR methods for both compressed and uncompressed SCIs.",
        "authors": [
            "Jingyu Yang",
            "Sheng Shen",
            "Huanjing Yue",
            "Kun Li"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/ImplicitTransformerNetworkforScreenContentImageContinuousSuperResolution.pdf"
    },
    {
        "internal_id": 177,
        "title": "Beyond BatchNorm: Towards a Unified Understanding of Normalization in Deep Learning",
        "abstract": "Inspired by BatchNorm, there has been an explosion of normalization layers in deep learning. Recent works have identified a multitude of beneficial properties in BatchNorm to explain its success. However, given the pursuit of alternative normalization layers, these properties need to be generalized so that any given layer's success/failure can be accurately predicted. In this work, we take a first step towards this goal by extending known properties of BatchNorm in randomly initialized deep neural networks (DNNs) to several recently proposed normalization layers. Our primary findings follow: (i) similar to BatchNorm, activations-based normalization layers can prevent exponential growth of activations in ResNets, but parametric techniques require explicit remedies; (ii) use of GroupNorm can ensure an informative forward propagation, with different samples being assigned dissimi- lar activations, but increasing group size results in increasingly indistinguishable activations for different samples, explaining slow convergence speed in models with LayerNorm; and (iii) small group sizes result in large gradient norm in earlier layers, hence explaining training instability issues in Instance Normalization and illustrating a speed-stability tradeoff in GroupNorm. Overall, our analysis reveals a unified set of mechanisms that underpin the success of normalization methods in deep learning, providing us with a compass to systematically explore the vast design space of DNN normalization layers.",
        "authors": [
            "Ekdeep Singh Lubana",
            "Robert P Dick",
            "Hidenori Tanaka"
        ],
        "created_time": "20 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/BeyondBatchNormTowardsaUnifiedUnderstandingofNormalizationinDeepLearning.pdf"
    },
    {
        "internal_id": 178,
        "title": "Fitting summary statistics of neural data with a differentiable spiking network simulator",
        "abstract": "Fitting network models to neural activity is an important tool in neuroscience. A popular approach is to model a brain area with a probabilistic recurrent spiking net- work whose parameters maximize the likelihood of the recorded activity. Although this is widely used, we show that the resulting model does not produce realistic neural activity. To correct for this, we suggest to augment the log-likelihood with terms that measure the dissimilarity between simulated and recorded activity. This dissimilarity is defined via summary statistics commonly used in neuroscience and the optimization is efficient because it relies on back-propagation through the stochastically simulated spike trains. We analyze this method theoretically and show empirically that it generates more realistic activity statistics. We find that it improves upon other fitting algorithms for spiking network models like GLMs (Generalized Linear Models) which do not usually rely on back-propagation. This new fitting algorithm also enables the consideration of hidden neurons which is otherwise notoriously hard, and we show that it can be crucial when trying to infer the network connectivity from spike recordings.",
        "authors": [
            "Guillaume Bellec",
            "Shuqi Wang",
            "Alireza Modirshanechi",
            "Johanni Brea",
            "Wulfram Gerstner"
        ],
        "created_time": "27 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/Fittingsummarystatisticsofneuraldatawithadifferentiablespikingnetworksimulator.pdf"
    },
    {
        "internal_id": 179,
        "title": "Implicit Regularization in Matrix Sensing via Mirror Descent",
        "abstract": "We study discrete-time mirror descent applied to the unregularized empirical risk in matrix sensing. In both the general case of rectangular matrices and the particular case of positive semidefinite matrices, a simple potential-based analysis in terms of the Bregman divergence allows us to establish convergence of mirror descent\u2014with different choices of the mirror maps\u2014to a matrix that, among all global minimizers of the empirical risk, minimizes a quantity explicitly related to the nuclear norm, the Frobenius norm, and the von Neumann entropy. In both cases, this characterization implies that mirror descent, a first-order algorithm minimizing the unregularized empirical risk, recovers low-rank matrices under the same set of assumptions that are sufficient to guarantee recovery for nuclear-norm minimization. When the sensing matrices are symmetric and commute, we show that gradient descent with full-rank factorized parametrization is a first-order approximation to mirror descent, in which case we obtain an explicit characterization of the implicit bias of gradient flow as a by-product.",
        "authors": [
            "Fan Wu",
            "Patrick Rebeschini"
        ],
        "created_time": "27 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/ImplicitRegularizationinMatrixSensingviaMirrorDescent.pdf"
    },
    {
        "internal_id": 181,
        "title": "Robustness between the worst and average case",
        "abstract": "Several recent works in machine learning have focused on evaluating the test-time robustness of a classifier: how well the classifier performs not just on the target do- main it was trained upon, but upon perturbed examples. In these settings, the focus has largely been on two extremes of robustness: the robustness to perturbations drawn at random from within some distribution (i.e., robustness to random perturba- tions), and the robustness to the worst case perturbation in some set (i.e., adversarial robustness). In this paper, we argue that a sliding scale between these two extremes provides a valuable additional metric by which to gauge robustness. Specifically, we illustrate that each of these two extremes is naturally characterized by a (func- tional) q-norm over perturbation space, with q = 1 corresponding to robustness to random perturbations and q = \u221e corresponding to adversarial perturbations. We then present the main technical contribution of our paper: a method for efficiently estimating the value of these norms by interpreting them as the partition function of a particular distribution, then using path sampling with MCMC methods to estimate this partition function (either traditional Metropolis-Hastings for non-differentiable perturbations, or Hamiltonian Monte Carlo for differentiable perturbations). We show that our approach provides substantially better estimates than simple random sampling of the actual \"intermediate-q\" robustness of standard, data-augmented, and adversarially-trained classifiers, illustrating a clear tradeoff between classifiers that optimize different metrics. Code for reproducing experiments can be found at https://github.com/locuslab/intermediate_robustness.",
        "authors": [
            "Leslie Rice",
            "Anna Bair",
            "Huan Zhang",
            "J Zico Kolter"
        ],
        "created_time": "26 October 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/Robustnessbetweentheworstandaveragecase.pdf"
    },
    {
        "internal_id": 182,
        "title": "Overparameterization Improves Robustness to",
        "abstract": "A significant obstacle in the development of robust machine learning models is covariate shift, a form of distribution shift that occurs when the input distributions of the training and test sets differ while the conditional label distributions remain the same. Despite the prevalence of covariate shift in real-world applications, a theoretical understanding in the context of modern machine learning has remained lacking. In this work, we examine the exact high-dimensional asymptotics of ran- dom feature regression under covariate shift and present a precise characterization of the limiting test error, bias, and variance in this setting. Our results motivate a natural partial order over covariate shifts that provides a sufficient condition for determining when the shift will harm (or even help) test performance. We find that overparameterized models exhibit enhanced robustness to covariate shift, providing one of the first theoretical explanations for this ubiquitous empirical phe- nomenon. Additionally, our analysis reveals an exact linear relationship between the in-distribution and out-of-distribution generalization performance, offering an explanation for this surprising recent observation.",
        "authors": [
            "Nilesh Tripuraneni",
            "Ben Adlam",
            "Jeffrey Pennington"
        ],
        "created_time": "09 November 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/OverparameterizationImprovesRobustnesstoCovariateShiftinHighDimensions.pdf"
    },
    {
        "internal_id": 183,
        "title": "Modality-Agnostic Topology Aware Localization",
        "abstract": "This work presents a data-driven approach for the indoor localization of an observer on a 2D topological map of the environment. State-of-the-art techniques may yield accurate estimates only when they are tailor-made for a specific data modality like camera-based system that prevents their applicability to broader domains. Here, we establish a modality-agnostic framework (called OT-Isomap) and formulate the localization problem in the context of parametric manifold learning while leveraging optimal transportation. This framework allows jointly learning a low- dimensional embedding as well as correspondences with a topological map. We examine the generalizability of the proposed algorithm by applying it to data from diverse modalities such as image sequences and radio frequency signals. The experimental results demonstrate decimeter-level accuracy for localization using different sensory inputs.",
        "authors": [
            "Farhad G Zanjani",
            "Ilia Karmanov",
            "Hanno Ackermann",
            "Daniel Dijkman",
            "Simone Merlin",
            "Max Welling",
            "Fatih Porikli"
        ],
        "created_time": "02 November 2021",
        "conference": "NeurIPS",
        "filepath": "/Users/adit/papers/NeurIPS/ModalityAgnosticTopologyAwareLocalization.pdf"
    },
    {
        "internal_id": 184,
        "title": "On the Sample Complexity of Adversarial Multi-Source PAC Learning",
        "abstract": "We study the problem of learning from multi- ple untrusted data sources, a scenario of increas- ing practical relevance given the recent emer- gence of crowdsourcing and collaborative learn- ing paradigms. Specifically, we analyze the situa- tion in which a learning system obtains datasets from multiple sources, some of which might be bi- ased or even adversarially perturbed. It is known that in the single-source case, an adversary with the power to corrupt a fixed fraction of the training data can prevent PAC-learnability, that is, even in the limit of infinitely much training data, no learning system can approach the optimal test er- ror. In this work we show that, surprisingly, the same is not true in the multi-source setting, where the adversary can arbitrarily corrupt a fixed frac- tion of the data sources. Our main results are a generalization bound that provides finite-sample guarantees for this learning setting, as well as corresponding lower bounds. Besides establish- ing PAC-learnability our results also show that in a cooperative learning setting sharing data with other parties has provable benefits, even if some participants are malicious.",
        "authors": [],
        "created_time": "",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/On the Sample Complexity of Adversarial Multi Source PAC Learning.pdf"
    },
    {
        "internal_id": 185,
        "title": "Complexity of Finding Stationary Points of Nonsmooth Nonconvex Functions",
        "abstract": "We provide the first non-asymptotic analysis for finding stationary points of nonsmooth, noncon- vex functions. In particular, we study the class of Hadamard semi-differentiable functions, per- haps the largest class of nonsmooth functions for which the chain rule of calculus holds. This class contains examples such as ReLU neural networks and others with non-differentiable activation func- tions. We first show that finding an \u03f5-stationary point with first-order methods is impossible in finite time. We then introduce the notion of (\u03b4, \u03f5)- stationarity, which allows for an \u03f5-approximate gradient to be the convex combination of general- ized gradients evaluated at points within distance \u03b4 to the solution. We propose a series of random- ized first-order methods and analyze their com- plexity of finding a (\u03b4, \u03f5)-stationary point. Fur- thermore, we provide a lower bound and show that our stochastic algorithm has min-max opti- mal dependence on \u03b4. Empirically, our methods perform well for training ReLU neural networks.",
        "authors": [],
        "created_time": "",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Complexity of Finding Stationary Points of Nonconvex Nonsmooth Functions.pdf"
    },
    {
        "internal_id": 186,
        "title": "Q-value Path Decomposition for Deep Multiagent Reinforcement Learning",
        "abstract": "Recently, deep multiagent reinforcement learning (MARL) has become a highly active research area as many real-world problems can be inherently viewed as multiagent systems. A particularly in- teresting and widely applicable class of problems is the partially observable cooperative multiagent setting, in which a team of agents learns to coor- dinate their behaviors conditioning on their pri- vate observations and commonly shared global reward signals. One natural solution is to resort to the centralized training and decentralized execu- tion paradigm and during centralized training, one key challenge is the multiagent credit assignment: how to allocate the global rewards for individ- ual agent policies for better coordination towards maximizing system-level's benefits. In this paper, we propose a new method called Q-value Path Decomposition (QPD) to decompose the system's global Q-values into individual agents' Q-values. Unlike previous works which restrict the represen- tation relation of the individual Q-values and the global one, we leverage the integrated gradient attribution technique into deep MARL to directly decompose global Q-values along trajectory paths to assign credits for agents. We evaluate QPD on the challenging StarCraft II micromanagement tasks and show that QPD achieves the state-of- the-art performance in both homogeneous and het- erogeneous multiagent scenarios compared with existing cooperative MARL algorithms.",
        "authors": [],
        "created_time": "15 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Q value Path Decomposition for Deep Multiagent Reinforcement Learning.pdf"
    },
    {
        "internal_id": 188,
        "title": "Involutive MCMC: a Unifying Framework",
        "abstract": "Markov Chain Monte Carlo (MCMC) is a compu- tational approach to fundamental problems such as inference, integration, optimization, and simu- lation. The field has developed a broad spectrum of algorithms, varying in the way they are moti- vated, the way they are applied and how efficiently they sample. Despite all the differences, many of them share the same core principle, which we unify as the Involutive MCMC (iMCMC) frame- work. Building upon this, we describe a wide range of MCMC algorithms in terms of iMCMC, and formulate a number of \"tricks\" which one can use as design principles for developing new MCMC algorithms. Thus, iMCMC provides a unified view of many known MCMC algorithms, which facilitates the derivation of powerful ex- tensions. We demonstrate the latter with two examples where we transform known reversible MCMC algorithms into more efficient irreversible ones.",
        "authors": [],
        "created_time": "15 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Involutive MCMC  a Unifying Framework.pdf"
    },
    {
        "internal_id": 189,
        "title": "Hierarchical Verification for Adversarial Robustness",
        "abstract": "We introduce a new framework for the exact point- wise \u2113p robustness verification problem that ex- ploits the layer-wise geometric structure of deep feed-forward networks with rectified linear acti- vations (ReLU networks). The activation regions of the network partition the input space, and one can verify the \u2113p robustness around a point by checking all the activation regions within the de- sired radius. The GeoCert algorithm (Jordan et al., 2019) treats this partition as a generic polyhedral complex in order to detect which region to check next. In contrast, our LayerCert framework con- siders the nested hyperplane arrangement struc- ture induced by the layers of the ReLU network and explores regions in a hierarchical manner. We show that, under certain conditions on the al- gorithm parameters, LayerCert provably reduces the number and size of the convex programs that one needs to solve compared to GeoCert. Fur- thermore, our LayerCert framework allows the incorporation of lower bounding routines based on convex relaxations to further improve perfor- mance. Experimental results demonstrate that LayerCert can significantly reduce both the num- ber of convex programs solved and the running time over the state-of-the-art.",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Hierarchical Verification for Adversarial Robustness.pdf"
    },
    {
        "internal_id": 190,
        "title": "Adversarial Attacks on Copyright Detection Systems",
        "abstract": "It is well-known that many machine learning mod- els are susceptible to adversarial attacks, in which an attacker evades a classifier by making small perturbations to inputs. This paper discusses how industrial copyright detection tools, which serve a central role on the web, are susceptible to adver- sarial attacks. As proof of concept, we describe a well-known music identification method and im- plement this system in the form of a neural net. We then attack this system using simple gradient methods and show that it is easily broken with white-box attacks. By scaling these perturbations up, we can create transfer attacks on industrial systems, such as the AudioTag copyright detector and YouTube's Content ID system, using pertur- bations that are audible but significantly smaller than a random baseline. Our goal is to raise aware- ness of the threats posed by adversarial examples in this space and to highlight the importance of hardening copyright detection systems to attacks.",
        "authors": [],
        "created_time": "08 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Adversarial Attacks on Copyright Detection Systems.pdf"
    },
    {
        "internal_id": 191,
        "title": "Deep Reasoning Networks for Unsupervised Pattern De-mixing with Constraint Reasoning",
        "abstract": "We introduce Deep Reasoning Networks (DR- Nets), an end-to-end framework that combines deep learning with constraint reasoning for solv- ing pattern de-mixing problems, typically in an unsupervised or very-weakly-supervised set- ting. DRNets exploit problem structure and prior knowledge by tightly combining constraint rea- soning with stochastic-gradient-based neural net- work optimization. Our motivating task is from materials discovery and concerns inferring crystal structures of materials from X-ray diffraction data (Crystal-Structure-Phase-Mapping). Given the complexity of its underlying scientific domain, we start by introducing DRNets on an analogous but much simpler task: de-mixing overlapping hand- written Sudokus (Multi-MNIST-Sudoku). On Multi-MNIST-Sudoku, DRNets almost perfectly recovered the mixed Sudokus' digits, with 100% digit accuracy, outperforming the supervised state- of-the-art MNIST de-mixing models. On Crystal- Structure-Phase-Mapping, DRNets significantly outperform the state of the art and experts' capa- bilities, recovering more precise and physically meaningful crystal structures.",
        "authors": [],
        "created_time": "01 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Deep Reasoning Networks for Unsupervised Pattern De mixing with Constraint Reasoning.pdf"
    },
    {
        "internal_id": 192,
        "title": "An Accelerated DFO Algorithm for Finite-sum Convex Functions",
        "abstract": "Derivative-free optimization (DFO) has recently gained a lot of momentum in machine learning, spawning interest in the community to design faster methods for problems where gradients are not accessible. While some attention has been given to the concept of acceleration in the DFO literature, existing stochastic algorithms for ob- jective functions with a finite-sum structure have not been shown theoretically to achieve an accel- erated rate of convergence. Algorithms that use acceleration in such a setting are prone to instabil- ities, making it difficult to reach convergence. In this work, we exploit the finite-sum structure of the objective in order to design a variance-reduced DFO algorithm that provably yields acceleration. We prove rates of convergence for both smooth convex and strongly-convex finite-sum objective functions. Finally, we validate our theoretical results empirically on several tasks and datasets.",
        "authors": [],
        "created_time": "",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/An Accelerated DFO Algorithm for Finite sum Convex Functions.pdf"
    },
    {
        "internal_id": 193,
        "title": "Simple and Deep Graph Convolutional Networks",
        "abstract": "Graph convolutional networks (GCNs) are a pow- erful deep learning approach for graph-structured data. Recently, GCNs and subsequent variants have shown superior performance in various ap- plication areas on real-world datasets. Despite their success, most of the current GCN models are shallow, due to the over-smoothing problem. In this paper, we study the problem of design- ing and analyzing deep graph convolutional net- works. We propose the GCNII, an extension of the vanilla GCN model with two simple yet ef- fective techniques: Initial residual and Identity mapping. We provide theoretical and empirical evidence that the two techniques effectively re- lieves the problem of over-smoothing. Our ex- periments show that the deep GCNII model out- performs the state-of-the-art methods on various semi- and full-supervised tasks.",
        "authors": [],
        "created_time": "15 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Simple and Deep Graph Convolutional Networks.pdf"
    },
    {
        "internal_id": 194,
        "title": "Multi-objective Bayesian Optimization using Pareto-frontier Entropy",
        "abstract": "This paper studies an entropy-based multi- objective Bayesian optimization (MBO). Exist- ing entropy-based MBO methods need compli- cated approximations to evaluate entropy or em- ploy over-simplification that ignores trade-off among objectives. We propose a novel entropy- based MBO called Pareto-frontier entropy search (PFES), which is based on the information gain of Pareto-frontier. We show that our entropy evaluation can be reduced to a closed form whose computation is quite simple while capturing the trade-off relation in Pareto-frontier. We further propose an extension for the \"decoupled\" set- ting, in which each objective function can be ob- served separately, and show that the PFES-based approach derives a natural extension of the origi- nal acquisition function which can also be evalu- ated simply. Our numerical experiments show ef- fectiveness of PFES through several benchmark datasets, and real-word datasets from materials science.",
        "authors": [],
        "created_time": "11 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Multi objective Bayesian Optimization using Pareto frontier Entropy.pdf"
    },
    {
        "internal_id": 195,
        "title": "Adaptive Estimator Selection for Off-Policy Evaluation",
        "abstract": "We develop a generic data-driven method for es- timator selection in off-policy policy evaluation settings. We establish a strong performance guar- antee for the method, showing that it is compet- itive with the oracle estimator, up to a constant factor. Via in-depth case studies in contextual ban- dits and reinforcement learning, we demonstrate the generality and applicability of the method. We also perform comprehensive experiments, demon- strating the empirical efficacy of our approach and comparing with related approaches. In both case studies, our method compares favorably with existing methods.",
        "authors": [],
        "created_time": "10 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Adaptive Estimator Selection for Off Policy Evaluation.pdf"
    },
    {
        "internal_id": 196,
        "title": "Lorentz Group Equivariant Neural Network for Particle Physics",
        "abstract": "We present a neural network architecture that is fully equivariant with respect to transforma- tions under the Lorentz group, a fundamental symmetry of space and time in physics. The architecture is based on the theory of the finite- dimensional representations of the Lorentz group and the equivariant nonlinearity involves the ten- sor product. For classification tasks in particle physics, we demonstrate that such an equivariant architecture leads to drastically simpler models that have relatively few learnable parameters and are much more physically interpretable than lead- ing approaches that use CNNs and point cloud approaches. The competitive performance of the network is demonstrated on a public classifica- tion dataset (Kasieczka et al., 2019) for tagging top quark decays given energy-momenta of jet constituents produced in proton-proton collisions.",
        "authors": [],
        "created_time": "13 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Lorentz Group Equivariant Neural Network for Particle Physics.pdf"
    },
    {
        "internal_id": 197,
        "title": "Differentiable Likelihoods for Fast Inversion of 'Likelihood-Free' Dynamical Systems",
        "abstract": "Likelihood-free (a.k.a. simulation-based) infer- ence problems are inverse problems with expen- sive, or intractable, forward models. ODE inverse problems are commonly treated as likelihood-free, as their forward map has to be numerically approx- imated by an ODE solver. This, however, is not a fundamental constraint but just a lack of function- ality in classic ODE solvers, which do not return a likelihood but a point estimate. To address this shortcoming, we employ Gaussian ODE filtering (a probabilistic numerical method for ODEs) to construct a local Gaussian approximation to the likelihood. This approximation yields tractable es- timators for the gradient and Hessian of the (log-) likelihood. Insertion of these estimators into ex- isting gradient-based optimization and sampling methods engenders new solvers for ODE inverse problems. We demonstrate that these methods outperform standard likelihood-free approaches on three benchmark-systems.",
        "authors": [],
        "created_time": "06 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Differentiable Likelihoods for Fast Inversion of  Likelihood Free  Dynamical Systems.pdf"
    },
    {
        "internal_id": 198,
        "title": "Spread Divergence",
        "abstract": "For distributions P and Q with different supports or undefined densities, the divergence D(P||Q) may not exist. We define a Spread Divergence \u02dcD(P||Q) on modified P and Q and describe suf- ficient conditions for the existence of such a di- vergence. We demonstrate how to maximize the discriminatory power of a given divergence by parameterizing and learning the spread. We also give examples of using a Spread Divergence to train implicit generative models, including linear models (Independent Components Analysis) and non-linear models (Deep Generative Networks).",
        "authors": [],
        "created_time": "",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Spread Divergence.pdf"
    },
    {
        "internal_id": 199,
        "title": "Universal Equivariant Multilayer Perceptrons",
        "abstract": "Group invariant and equivariant Multilayer Per- ceptrons (MLP), also known as Equivariant Net- works and Group Group Convolutional Neural Networks (G-CNN) have achieved remarkable success in learning on a variety of data structures, such as sequences, images, sets, and graphs. This paper proves the universality of a broad class of equivariant MLPs with a single hidden layer. In particular, it is shown that having a hidden layer on which the group acts regularly is sufficient for universal equivariance (invariance). For example, some types of steerable-CNNs become universal. Another corollary is the unconditional universal- ity of equivariant MLPs for all Abelian groups. A third corollary is the universality of equivari- ant MLPs with a high-order hidden layer, where we give both group-agnostic bounds and group- specific bounds on the order of the hidden layer that guarantees universal equivariance.",
        "authors": [],
        "created_time": "06 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Universal Equivariant Multilayer Perceptrons.pdf"
    },
    {
        "internal_id": 200,
        "title": "Simultaneous Inference for Massive Data: Distributed Bootstrap",
        "abstract": "In this paper, we propose a bootstrap method ap- plied to massive data processed distributedly in a large number of machines. This new method is computationally efficient in that we bootstrap on the master machine without over-resampling, typically required by existing methods (Kleiner et al., 2014; Sengupta et al., 2016), while prov- ably achieving optimal statistical efficiency with minimal communication. Our method does not require repeatedly re-fitting the model but only ap- plies multiplier bootstrap in the master machine on the gradients received from the worker ma- chines. Simulations validate our theory.",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Simultaneous Inference for Massive Data  Distributed Bootstrap.pdf"
    },
    {
        "internal_id": 201,
        "title": "Tight Bounds on Minimax Regret under Logarithmic Loss via Self-Concordance",
        "abstract": "We consider the classical problem of sequential probability assignment under logarithmic loss while competing against an arbitrary, potentially nonparametric class of experts. We obtain tight bounds on the minimax regret via a new approach that exploits the self-concordance property of the logarithmic loss. We show that for any expert class with (sequential) metric entropy O(\u03b3\u2212p) at scale \u03b3, the minimax regret is O(n",
        "authors": [],
        "created_time": "31 July 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Tight Bounds on Minimax Regret under Logarithmic Loss via Self Concordance.pdf"
    },
    {
        "internal_id": 202,
        "title": "Accountable Off-Policy Evaluation With Kernel Bellman Statistics",
        "abstract": "We consider off-policy evaluation (OPE), which evaluates the performance of a new policy from observed data collected from previous experi- ments, without requiring the execution of the new policy. This finds important applications in areas with high execution cost or safety concerns, such as medical diagnosis, recommendation systems and robotics. In practice, due to the limited infor- mation from off-policy data, it is highly desirable to construct rigorous confidence intervals, not just point estimation, for the policy performance. In this work, we propose a new variational frame- work which reduces the problem of calculating tight confidence bounds in OPE into an optimiza- tion problem on a feasible set that catches the true state-action value function with high probability. The feasible set is constructed by leveraging sta- tistical properties of a recently proposed kernel Bellman loss (Feng et al., 2019). We design an efficient computational approach for calculating our bounds, and extend it to perform post-hoc diagnosis and correction for existing estimators. Empirical results show that our method yields tight confidence intervals in different settings.",
        "authors": [],
        "created_time": "",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Accountable Off Policy Evaluation With Kernel Bellman Statistics.pdf"
    },
    {
        "internal_id": 203,
        "title": "Computational-Statistical Tradeoffs",
        "abstract": "We study the computational and statistical trade- offs in inferring combinatorial structures of high dimensional simple zero-field ferromagnetic Ising model. Under the framework of oracle com-",
        "authors": [],
        "created_time": "30 July 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Computational and Statistical Tradeoffs in Inferring Combinatorial Structures of Ising Model.pdf"
    },
    {
        "internal_id": 204,
        "title": "Interferometric Graph Transform: a Deep Unsupervised Graph Representation",
        "abstract": "We propose the Interferometric Graph Transform (IGT), which is a new class of deep unsupervised graph convolutional neural network for building graph representations. Our first contribution is to propose a generic, complex-valued spectral graph architecture obtained from a generalization of the Euclidean Fourier transform. We show that our learned representation consists of both discrim- inative and invariant features, thanks to a novel greedy concave objective. From our experiments, we conclude that our learning procedure exploits the topology of the spectral domain, which is nor- mally a flaw of spectral methods, and in particular our method can recover an analytic operator for vision tasks. We test our algorithm on various and challenging tasks such as image classifica- tion (MNIST, CIFAR-10), community detection (Authorship, Facebook graph) and action recog- nition from 3D skeletons videos (SBU, NTU), exhibiting a new state-of-the-art in spectral graph unsupervised settings.",
        "authors": [
            "Edouard Oyallon "
        ],
        "created_time": "22 June 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Interferometric Graph Transform  a Deep Unsupervised Graph Representation.pdf"
    },
    {
        "internal_id": 205,
        "title": "Optimally Solving Two-Agent Decentralized POMDPs Under One-Sided Information Sharing",
        "abstract": "Optimally solving decentralized partially observ- able Markov decision processes (Dec-POMDPs) under either full or no information sharing re- ceived significant attention in recent years. How- ever, little is known about how partial informa- tion sharing affects existing theory and algorithms. This paper addresses this question for a team of two agents, with one-sided information sharing, i.e. both agents have imperfect information about the state of the world, but only one has access to what the other sees and does. From the perspec- tive of a central planner, we show that the original problem can be reformulated into an equivalent information-state Markov decision process and solved as such. Besides, we prove that the optimal value function exhibits a specific form of uniform continuity. We also present heuristic search algo- rithms utilizing this property and providing the first results for this family of problems.",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Optimally Solving Two Agent Decentralized POMDPs Under One Sided Information Sharing.pdf"
    },
    {
        "internal_id": 206,
        "title": "Analytic Marching: An Analytic Meshing Solution from Deep Implicit Surface Networks",
        "abstract": "This paper studies a problem of learning sur- face mesh via implicit functions in an emerg- ing field of deep learning surface reconstruction, where implicit functions are popularly imple- mented as multi-layer perceptrons (MLPs) with rectified linear units (ReLU). To achieve meshing from learned implicit functions, existing methods adopt the de-facto standard algorithm of march- ing cubes; while promising, they suffer from loss of precision learned in the MLPs, due to the discretization nature of marching cubes. Moti- vated by the knowledge that a ReLU based MLP partitions its input space into a number of lin- ear regions, we identify from these regions ana- lytic cells and analytic faces that are associated with zero-level isosurface of the implicit func- tion, and characterize the theoretical conditions under which the identified analytic faces are guar- anteed to connect and form a closed, piecewise planar surface. Based on our theorem, we pro- pose a naturally parallelizable algorithm of an- alytic marching, which marches among analytic cells to exactly recover the mesh captured by a learned MLP. Experiments on deep learning mesh reconstruction verify the advantages of our algo- rithm over existing ones.",
        "authors": [],
        "created_time": "13 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Analytic Marching  An Analytic Meshing Solution from Deep Implicit Surface Networks.pdf"
    },
    {
        "internal_id": 207,
        "title": "On Second-Order Group Influence Functions for Black-Box Predictions",
        "abstract": "With the rapid adoption of machine learning sys- tems in sensitive applications, there is an increas- ing need to make black-box models explainable. Often we want to identify an influential group of training samples in a particular test prediction for a given machine learning model. Existing influ- ence functions tackle this problem by using first- order approximations of the effect of removing a sample from the training set on model parameters. To compute the influence of a group of training samples (rather than an individual point) in model predictions, the change in optimal model parame- ters after removing that group from the training set can be large. Thus, in such cases, the first-order approximation can be loose. In this paper, we address this issue and propose second-order influ- ence functions for identifying influential groups in test-time predictions. For linear models, across different sizes and types of groups, we show that using the proposed second-order influence func- tion improves the correlation between the com- puted influence values and the ground truth ones. We also show that second-order influence func- tions could be used with optimization techniques to improve the selection of the most influential group for a test-sample.",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/On Second Order Group Influence Functions for Black Box Predictions.pdf"
    },
    {
        "internal_id": 208,
        "title": "Improved Sleeping Bandits with Stochastic Actions Sets and Adversarial Rewards",
        "abstract": "In this paper, we consider the problem of sleeping bandits with stochastic action sets and adversarial rewards. In this setting, in contrast to most work in bandits, the actions may not be available at all times. For instance, some products might be out of stock in item recommendation. The best exist- ing efficient (i.e., polynomial-time) algorithms for this problem only guarantee an O(T 2/3) upper- bound on the regret. Yet, inefficient algorithms based on EXP4 can achieve O( \u221a",
        "authors": [],
        "created_time": "",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Improved Sleeping Bandits with Stochastic Action Sets and Adversarial Rewards.pdf"
    },
    {
        "internal_id": 209,
        "title": "\"Other-Play\" for Zero-Shot Coordination",
        "abstract": "We consider the problem of zero-shot coordina- tion - constructing AI agents that can coordinate with novel partners they have not seen before (e.g. humans). Standard Multi-Agent Reinforcement Learning (MARL) methods typically focus on the self-play (SP) setting where agents construct strategies by playing the game with themselves repeatedly. Unfortunately, applying SP naively to the zero-shot coordination problem can produce agents that establish highly specialized conven- tions that do not carry over to novel partners they have not been trained with. We introduce a novel learning algorithm called other-play (OP), that enhances self-play by looking for more robust strategies, exploiting the presence of known sym- metries in the underlying problem. We character- ize OP theoretically as well as experimentally. We study the cooperative card game Hanabi and show that OP agents achieve higher scores when paired with independently trained agents. In preliminary results we also show that our OP agents obtains higher average scores when paired with human players, compared to state-of-the-art SP agents.",
        "authors": [],
        "created_time": "12 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/ Other Play  for Zero Shot Coordination.pdf"
    },
    {
        "internal_id": 210,
        "title": "Efficiently Solving MDPs with Stochastic Mirror Descent",
        "abstract": "We present a unified framework based on primal- dual stochastic mirror descent for approximately solving infinite-horizon Markov decision pro- cesses (MDPs) given a generative model. When applied to an average-reward MDP with Atot total actions and mixing time bound tmix our method computes an \u03f5-optimal policy with an expected \ufffdO(t2 mixAtot\u03f5\u22122) samples from the state- transition matrix, removing the ergodicity depen- dence of prior art. When applied to a \u03b3-discounted MDP with Atot total actions our method com- putes an \u03f5-optimal policy with an expected \ufffdO((1\u2212 \u03b3)\u22124Atot\u03f5\u22122) samples, improving over previous primal-dual methods and matching the state-of- the-art up to a (1\u2212\u03b3)\u22121 factor. Both methods are model-free, update state values and policies simul- taneously, and run in time linear in the number of samples taken. We achieve these results through a more general stochastic mirror descent framework for solving bilinear saddle-point problems with simplex and box domains and we demonstrate the flexibility of this framework by providing further applications to constrained MDPs.",
        "authors": [],
        "created_time": "",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Efficiently Solving MDPs with Stochastic Mirror Descent.pdf"
    },
    {
        "internal_id": 211,
        "title": "Reinforcement Learning for Molecular Design Guided by Quantum Mechanics",
        "abstract": "Automating molecular design using deep rein- forcement learning (RL) holds the promise of accelerating the discovery of new chemical com- pounds. Existing approaches work with molecu- lar graphs and thus ignore the location of atoms in space, which restricts them to 1) generating single organic molecules and 2) heuristic reward functions. To address this, we present a novel RL formulation for molecular design in Carte- sian coordinates, thereby extending the class of molecules that can be built. Our reward function is directly based on fundamental physical prop- erties such as the energy, which we approximate via fast quantum-chemical methods. To enable progress towards de-novo molecular design, we introduce MOLGYM, an RL environment com- prising several challenging molecular design tasks along with baselines. In our experiments, we show that our agent can efficiently learn to solve these tasks from scratch by working in a transla- tion and rotation invariant state-action space.",
        "authors": [],
        "created_time": "13 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Reinforcement Learning for Molecular Design Guided by Quantum Mechanics.pdf"
    },
    {
        "internal_id": 212,
        "title": "Why bigger is not always better: on finite and infinite neural networks",
        "abstract": "Recent work has argued that neural networks can be understood theoretically by taking the number of channels to infinity, at which point the out- puts become Gaussian process (GP) distributed. However, we note that infinite Bayesian neural networks lack a key facet of the behaviour of real neural networks: the fixed kernel, determined only by network hyperparameters, implies that they cannot do any form of representation learn- ing. The lack of representation or equivalently kernel learning leads to less flexibility and hence worse performance, giving a potential explana- tion for the inferior performance of infinite net- works observed in the literature (e.g. Novak et al. 2019). We give analytic results characterising the prior over representations and representation learning in finite deep linear networks. We show empirically that the representations in SOTA ar- chitectures such as ResNets trained with SGD are much closer to those suggested by our deep linear results than by the corresponding infinite network. This motivates the introduction of a new class of network: infinite networks with bottle- necks, which inherit the theoretical tractability of infinite networks while at the same time allowing representation learning.",
        "authors": [
            "Laurence Aitchison "
        ],
        "created_time": "24 June 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Why bigger is not always better  on finite and infinite neural networks.pdf"
    },
    {
        "internal_id": 213,
        "title": "Distribution Augmentation for Generative Modeling",
        "abstract": "We present distribution augmentation (DistAug), a simple and powerful method of regularizing generative models. Our approach applies augmen- tation functions to data and, importantly, condi- tions the generative model on the specific function used. Unlike typical data augmentation, DistAug allows usage of functions which modify the target density, enabling aggressive augmentations more commonly seen in supervised and self-supervised learning. We demonstrate this is a more effective regularizer than standard methods, and use it to train a 152M parameter autoregressive model on CIFAR-10 to 2.56 bits per dim (relative to the state-of-the-art 2.80). Samples from this model attain FID 12.75 and IS 8.40, outperforming the majority of GANs. We further demonstrate the technique is broadly applicable across model ar- chitectures and problem domains.",
        "authors": [
            "Ilya Sutskever "
        ],
        "created_time": "30 June 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Distribution Augmentation for Generative Modeling.pdf"
    },
    {
        "internal_id": 214,
        "title": "Learning Discrete Structured Representations by Adversarially Maximizing Mutual Information",
        "abstract": "We propose learning discrete structured represen- tations from unlabeled data by maximizing the mutual information between a structured latent variable and a target variable. Calculating mutual information is intractable in this setting. Our key technical contribution is an adversarial objective that can be used to tractably estimate mutual in- formation assuming only the feasibility of cross entropy calculation. We develop a concrete real- ization of this general formulation with Markov distributions over binary encodings. We report critical and unexpected findings on practical as- pects of the objective such as the choice of varia- tional priors. We apply our model on document hashing and show that it outperforms current best baselines based on discrete and vector quantized variational autoencoders. It also yields highly compressed interpretable representations.",
        "authors": [],
        "created_time": "12 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Learning Discrete Structured Representations by Adversarially Maximizing Mutual Information.pdf"
    },
    {
        "internal_id": 215,
        "title": "Robust Learning with the Hilbert-Schmidt Independence Criterion",
        "abstract": "We investigate the use of a non-parametric in- dependence measure, the Hilbert-Schmidt Inde- pendence Criterion (HSIC), as a loss-function for learning robust regression and classification mod- els. This loss-function encourages learning mod- els where the distribution of the residuals between the label and the model prediction is statistically independent of the distribution of the instances themselves. This loss-function was first proposed by Mooij et al. (2009) in the context of learning causal graphs. We adapt it to the task of learn- ing for unsupervised covariate shift: learning on a source domain without access to any instances or labels from the unknown target domain, but with the assumption that p(y|x) (the conditional probability of labels given instances) remains the same in the target domain. We show that the pro- posed loss is expected to give rise to models that generalize well on a class of target domains char- acterised by the complexity of their description within a reproducing kernel Hilbert space. Ex- periments on unsupervised covariate shift tasks demonstrate that models learned with the pro- posed loss-function outperform models learned with standard loss functions, achieving state-of- the-art results on a challenging cell-microscopy unsupervised covariate shift task.",
        "authors": [],
        "created_time": "10 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Robust Learning with the Hilbert Schmidt Independence Criterion.pdf"
    },
    {
        "internal_id": 216,
        "title": "LEEP: A New Measure to Evaluate Transferability of Learned Representations",
        "abstract": "We introduce a new measure to evaluate the trans- ferability of representations learned by classifiers. Our measure, the Log Expected Empirical Pre- diction (LEEP), is simple and easy to compute: when given a classifier trained on a source data set, it only requires running the target data set through this classifier once. We analyze the prop- erties of LEEP theoretically and demonstrate its effectiveness empirically. Our analysis shows that LEEP can predict the performance and conver- gence speed of both transfer and meta-transfer learning methods, even for small or imbalanced data. Moreover, LEEP outperforms recently pro- posed transferability measures such as negative conditional entropy and H scores. Notably, when transferring from ImageNet to CIFAR100, LEEP can achieve up to 30% improvement compared to the best competing method in terms of the corre- lations with actual transfer accuracy.",
        "authors": [],
        "created_time": "13 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/LEEP  A New Measure to Evaluate Transferability of Learned Representations.pdf"
    },
    {
        "internal_id": 217,
        "title": "Sharp Statistical Guarantees for Adversarially Robust Gaussian Classification",
        "abstract": "Adversarial robustness has become a fundamental requirement in modern machine learning appli- cations. Yet, there has been surprisingly little statistical understanding so far. In this paper, we provide the first result of the optimal minimax guarantees for the excess risk for adversarially ro- bust classification, under Gaussian mixture model proposed by (Schmidt et al., 2018). The results are stated in terms of the Adversarial Signal-to- Noise Ratio (AdvSNR), which generalizes a simi- lar notion for standard linear classification to the adversarial setting. For the Gaussian mixtures with AdvSNR value of r, we establish an excess risk lower bound of order \u0398(e\u2212( 1",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Sharp Statistical Guaratees for Adversarially Robust Gaussian Classification.pdf"
    },
    {
        "internal_id": 218,
        "title": "Linear Bandits with Stochastic Delayed Feedback",
        "abstract": "Stochastic linear bandits are a natural and well-studied model for structured explo- ration/exploitation problems and are widely used in applications such as online marketing and recommendation. One of the main challenges faced by practitioners hoping to apply existing algorithms is that usually the feedback is randomly delayed and delays are only partially observable. For example, while a purchase is usually observable some time after the display, the decision of not buying is never explicitly sent to the system. In other words, the learner only observes delayed positive events. We formalize this problem as a novel stochastic delayed linear bandit and propose OTFLinUCB and OTFLinTS, two computationally efficient algorithms able to integrate new information as it becomes available and to deal with the permanently censored feedback. We prove optimal \u02dcO(d \u221a",
        "authors": [
            "Michael Brueckner "
        ],
        "created_time": "03 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Linear bandits with Stochastic Delayed Feedback.pdf"
    },
    {
        "internal_id": 219,
        "title": "Learning To Stop While Learning To Predict",
        "abstract": "There is a recent surge of interest in designing deep architectures based on the update steps in traditional algorithms, or learning neural networks to improve and replace traditional algorithms. While traditional algorithms have certain stop- ping criteria for outputting results at different iter- ations, many algorithm-inspired deep models are restricted to a \"fixed-depth\" for all inputs. Similar to algorithms, the optimal depth of a deep architec- ture may be different for different input instances, either to avoid \"over-thinking\", or because we want to compute less for operations converged al- ready. In this paper, we tackle this varying depth problem using a steerable architecture, where a feed-forward deep model and a variational stop- ping policy are learned together to sequentially determine the optimal number of layers for each input instance. Training such architecture is very challenging. We provide a variational Bayes per- spective and design a novel and effective training procedure which decomposes the task into an or- acle model learning stage and an imitation stage. Experimentally, we show that the learned deep model along with the stopping policy improves the performances on a diverse set of tasks, in- cluding learning sparse recovery, few-shot meta learning, and computer vision tasks.",
        "authors": [],
        "created_time": "01 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Learning To Stop While Learning To Predict.pdf"
    },
    {
        "internal_id": 220,
        "title": "The Many Shapley Values for Model Explanation",
        "abstract": "The Shapley value has become the basis for sev- eral methods that attribute the prediction of a machine-learning model on an input to its base features. The use of the Shapley value is justi- fied by citing the uniqueness result from (Shapley, 1953), which shows that it is the only method that satisfies certain good properties (axioms). There are, however, a multiplicity of ways in which the Shapley value is operationalized for model ex- planation. These differ in how they reference the model, the training data, and the explanation context. Hence they differ in output, rendering the uniqueness result inapplicable. Furthermore, the techniques that rely on they training data pro- duce non-intuitive attributions, for instance un- used features can still receive attribution. In this paper, we use the axiomatic approach to study the differences between some of the many op- erationalizations of the Shapley value for attribu- tion. We discuss a technique called Baseline Shap- ley (BShap), provide a proper uniqueness result for it, and contrast it with two other techniques from prior literature, Integrated Gradients (Sun- dararajan et al., 2017) and Conditional Expecta- tion Shapley (Lundberg & Lee, 2017).",
        "authors": [],
        "created_time": "12 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/The Many Shapley Values for Model Explanation.pdf"
    },
    {
        "internal_id": 222,
        "title": "Multi-step Greedy Reinforcement Learning Algorithms",
        "abstract": "Multi-step greedy policies have been extensively used in model-based reinforcement learning (RL), both when a model of the environment is available (e.g., in the game of Go) and when it is learned. In this paper, we explore their benefits in model-free RL, when employed using multi-step dynamic programming algorithms: \uf8ff-Policy Iteration (\uf8ff- PI) and \uf8ff-Value Iteration (\uf8ff-VI). These methods iteratively compute the next policy (\uf8ff-PI) and value function (\uf8ff-VI) by solving a surrogate deci- sion problem with a shaped reward and a smaller discount factor. We derive model-free RL algo- rithms based on \uf8ff-PI and \uf8ff-VI in which the sur- rogate problem can be solved by any discrete or continuous action RL method, such as DQN and TRPO. We identify the importance of a hyper- parameter that controls the extent to which the surrogate problem is solved and suggest a way to set this parameter. When evaluated on a range of Atari and MuJoCo benchmark tasks, our results in- dicate that for the right range of \uf8ff, our algorithms outperform DQN and TRPO. This shows that our multi-step greedy algorithms are general enough to be applied over any existing RL algorithm and can significantly improve its performance.",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Multi step Greedy Reinforcement Learning Algorithms.pdf"
    },
    {
        "internal_id": 223,
        "title": "Beyond Signal Propagation: Is Feature Diversity Necessary in Deep Neural Network Initialization?",
        "abstract": "Deep neural networks are typically initialized with random weights, with variances chosen to fa- cilitate signal propagation and stable gradients. It is also believed that diversity of features is an im- portant property of these initializations. We con- struct a deep convolutional network with identical features by initializing almost all the weights to 0. The architecture also enables perfect signal prop- agation and stable gradients, and achieves high accuracy on standard benchmarks. This indicates that random, diverse initializations are not neces- sary for training neural networks. An essential element in training this network is a mechanism of symmetry breaking; we study this phenomenon and find that standard GPU operations, which are non-deterministic, can serve as a sufficient source of symmetry breaking to enable training.",
        "authors": [],
        "created_time": "",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Beyond Signal Propagation  Is Feature Diversity Necessary in Deep Neural Network Initialization .pdf"
    },
    {
        "internal_id": 224,
        "title": "Causal Structure Discovery from Distributions Arising from Mixtures of DAGs",
        "abstract": "We consider distributions arising from a mixture of causal models, where each model is repre- sented by a directed acyclic graph (DAG). We provide a graphical representation of such mixture distributions and prove that this representation en- codes the conditional independence relations of the mixture distribution. We then consider the problem of structure learning based on samples from such distributions. Since the mixing variable is latent, we consider causal structure discovery algorithms such as FCI that can deal with latent variables. We show that such algorithms recover a \"union\" of the component DAGs and can identify variables whose conditional distribution across the component DAGs vary. We demonstrate our results on synthetic and real data showing that the inferred graph identifies nodes that vary between the different mixture components. As an imme- diate application, we demonstrate how retrieval of this causal information can be used to cluster samples according to each mixture component.",
        "authors": [],
        "created_time": "08 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Causal Structure Discovery from Distributions Arising from Mixtures of DAGs.pdf"
    },
    {
        "internal_id": 225,
        "title": "Non-convex Learning via Replica Exchange Stochastic Gradient MCMC",
        "abstract": "Replica exchange Monte Carlo (reMC), also known as parallel tempering, is an important tech- nique for accelerating the convergence of the con- ventional Markov Chain Monte Carlo (MCMC) algorithms. However, such a method requires the evaluation of the energy function based on the full dataset and is not scalable to big data. The na\u00a8\u0131ve implementation of reMC in mini-batch set- tings introduces large biases, which cannot be di- rectly extended to the stochastic gradient MCMC (SGMCMC), the standard sampling method for simulating from deep neural networks (DNNs). In this paper, we propose an adaptive replica ex- change SGMCMC (reSGMCMC) to automati- cally correct the bias and study the corresponding properties. The analysis implies an acceleration- accuracy trade-off in the numerical discretization of a Markov jump process in a stochastic environ- ment. Empirically, we test the algorithm through extensive experiments on various setups and ob- tain the state-of-the-art results on CIFAR10, CI- FAR100, and SVHN in both supervised learning and semi-supervised learning tasks.",
        "authors": [],
        "created_time": "11 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Non convex Learning via Replica Exchange Stochastic Gradient MCMC.pdf"
    },
    {
        "internal_id": 227,
        "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
        "abstract": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever- larger networks to cover more facts.",
        "authors": [],
        "created_time": "30 June 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Retrieval Augmented Language Model Pre Training.pdf"
    },
    {
        "internal_id": 228,
        "title": "One Size Fits All: Can We Train One Denoiser for All Noise Levels?",
        "abstract": "When training an estimator such as a neural net- work for tasks like image denoising, it is often preferred to train one estimator and apply it to all noise levels. The de facto training protocol to achieve this goal is to train the estimator with noisy samples whose noise levels are uniformly distributed across the range of interest. However, why should we allocate the samples uniformly? Can we have more training samples that are less noisy, and fewer samples that are more noisy? What is the optimal distribution? How do we ob- tain such a distribution? The goal of this paper is to address this training sample distribution prob- lem from a minimax risk optimization perspective. We derive a dual ascent algorithm to determine the optimal sampling distribution of which the convergence is guaranteed as long as the set of admissible estimators is closed and convex. For estimators with non-convex admissible sets such as deep neural networks, our dual formulation converges to a solution of the convex relaxation. We discuss how the algorithm can be implemented in practice. We evaluate the algorithm on linear estimators and deep networks.",
        "authors": [],
        "created_time": "03 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/One Size Fits All  Can We Train One Denoiser for All Noise Levels .pdf"
    },
    {
        "internal_id": 229,
        "title": "Inertial Block Proximal Methods For Non-Convex Non-Smooth Optimization",
        "abstract": "We propose inertial versions of block coordinate descent methods for solving non-convex non- smooth composite optimization problems. Our methods possess three main advantages compared to current state-of-the-art accelerated first-order methods: (1) they allow using two different ex- trapolation points to evaluate the gradients and to add the inertial force (we will empirically show that it is more efficient than using a single extrap- olation point), (2) they allow to randomly pick- ing the block of variables to update, and (3) they do not require a restarting step. We prove the subsequential convergence of the generated se- quence under mild assumptions, prove the global convergence under some additional assumptions, and provide convergence rates. We deploy the proposed methods to solve non-negative matrix factorization (NMF) and show that they compete favourably with the state-of-the-art NMF algo- rithms. Additional experiments on non-negative approximate canonical polyadic decomposition, also known as non-negative tensor factorization, are also provided.",
        "authors": [],
        "created_time": "22 June 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Inertial Block Proximal Methods for Non Convex Non Smooth Optimization.pdf"
    },
    {
        "internal_id": 230,
        "title": "A Generative Model for Molecular Distance Geometry",
        "abstract": "Great computational effort is invested in generat- ing equilibrium states for molecular systems us- ing, for example, Markov chain Monte Carlo. We present a probabilistic model that generates statis- tically independent samples for molecules from their graph representations. Our model learns a low-dimensional manifold that preserves the ge- ometry of local atomic neighborhoods through a principled learning representation that is based on Euclidean distance geometry. In a new benchmark for molecular conformation generation, we show experimentally that our generative model achieves state-of-the-art accuracy. Finally, we show how to use our model as a proposal distribution in an im- portance sampling scheme to compute molecular properties.",
        "authors": [],
        "created_time": "13 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/A Generative Model for Molecular Distance Geometry.pdf"
    },
    {
        "internal_id": 231,
        "title": "Dynamics of Deep Neural Networks and Neural Tangent Hierarchy",
        "abstract": "The evolution of a deep neural network trained by the gradient descent in the overparametrization regime can be described by its neural tangent ker- nel (NTK) (Jacot et al., 2018; Du et al., 2018b;a; Arora et al., 2019b). It was observed (Arora et al., 2019a) that there is a performance gap between the kernel regression using the limiting NTK and the deep neural networks. We study the dynamic of neural networks of finite width and derive an infinite hierarchy of differential equations, the neural tangent hierarchy (NTH). We prove that the NTH hierarchy truncated at the level p \u2a7e 2 approximates the dynamic of the NTK up to ar- bitrary precision under certain conditions on the neural network width and the data set dimension. The assumptions needed for these approximations become weaker as p increases. Finally, NTH can be viewed as higher order extensions of NTK. In particular, the NTH truncated at p = 2 recovers the NTK dynamics.",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Dynamics of Deep Neural Networks and Neural Tangent Hierarchy.pdf"
    },
    {
        "internal_id": 232,
        "title": "Hallucinative Topological Memory for Zero-Shot Visual Planning",
        "abstract": "In visual planning (VP), an agent learns to plan goal-directed behavior from observations of a dynamical system obtained offline, e.g., images obtained from self-supervised robot interaction. Most previous works on VP approached the prob- lem by planning in a learned latent space, re- sulting in low-quality visual plans, and difficult training algorithms. Here, instead, we propose a simple VP method that plans directly in image space and displays competitive performance. We build on the semi-parametric topological memory (SPTM) method: image samples are treated as nodes in a graph, the graph connectivity is learned from image sequence data, and planning can be performed using conventional graph search meth- ods. We propose two modifications on SPTM. First, we train an energy-based graph connectivity function using contrastive predictive coding that admits stable training. Second, to allow zero-shot planning in new domains, we learn a conditional VAE model that generates images given a context describing the domain, and use these hallucinated samples for building the connectivity graph and planning. We show that this simple approach sig- nificantly outperform the SOTA VP methods, in terms of both plan interpretability and success rate when using the plan to guide a trajectory- following controller. Interestingly, our method can pick up non-trivial visual properties of ob- jects, such as their geometry, and account for it in the plans.",
        "authors": [],
        "created_time": "02 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Hallucinative Topological Memory for Zero Shot Visual Planning.pdf"
    },
    {
        "internal_id": 233,
        "title": "Estimating the Error of Randomized Newton Methods: A Bootstrap Approach",
        "abstract": "Randomized Newton methods have recently be- come the focus of intense research activity in large-scale and distributed optimization. In gen- eral, these methods are based on a \"computation- accuracy trade-off\", which allows the user to gain scalability in exchange for error in the solution. However, the user does not know how much er- ror is created by the randomized approximation, which can be detrimental in two ways: On one hand, the user may try to assess the unknown error with theoretical worst-case error bounds, but this approach is impractical when the bounds involve unknown constants, and it often leads to exces- sive computation. On the other hand, the user may select the \"sketch size\" and stopping criteria in a heuristic manner, but this can lead to unreli- able results. Motivated by these difficulties, we show how bootstrapping can be used to directly estimate the unknown error, which prevents ex- cessive computation, and offers more confidence about the quality of a randomized solution.",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Estimating the Error of Randomized Newton Methods  A Bootstrap Approach.pdf"
    },
    {
        "internal_id": 234,
        "title": "The Neural Tangent Kernel in High Dimensions: Triple Descent and a Multi-Scale Theory of Generalization",
        "abstract": "Modern deep learning models employ consider- ably more parameters than required to fit the train- ing data. Whereas conventional statistical wisdom suggests such models should drastically overfit, in practice these models generalize remarkably well. An emerging paradigm for describing this unexpected behavior is in terms of a double de- scent curve, in which increasing a model's ca- pacity causes its test error to first decrease, then increase to a maximum near the interpolation threshold, and then decrease again in the over- parameterized regime. Recent efforts to explain this phenomenon theoretically have focused on simple settings, such as linear regression or ker- nel regression with unstructured random features, which we argue are too coarse to reveal important nuances of actual neural networks. We provide a precise high-dimensional asymptotic analysis of generalization under kernel regression with the Neural Tangent Kernel, which characterizes the behavior of wide neural networks optimized with gradient descent. Our results reveal that the test error has non-monotonic behavior deep in the overparameterized regime and can even exhibit additional peaks and descents when the number of parameters scales quadratically with the dataset size.",
        "authors": [],
        "created_time": "",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/The Neural Tangent Kernel in High Dimensions  Triple Descent and a Multi Scale Theory of Generalization.pdf"
    },
    {
        "internal_id": 235,
        "title": "Safe Screening Rules for \u21130-Regression from Perspective Relaxations",
        "abstract": "We give safe screening rules to eliminate variables from regression with \u21130 regularization or cardinal- ity constraint. These rules are based on guarantees that a feature may or may not be selected in an optimal solution. The screening rules can be com- puted from a convex relaxation solution in linear time, without solving the \u21130 optimization prob- lem. Thus, they can be used in a preprocessing step to safely remove variables from considera- tion apriori. Numerical experiments on real and synthetic data indicate that a significant number of the variables can be removed quickly, hence reducing the computational burden for optimiza- tion substantially. Therefore, the proposed fast and effective screening rules extend the scope of algorithms for \u21130-regression to larger data sets.",
        "authors": [],
        "created_time": "10 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Safe screening rules for L0 regression from Perspective Relaxations.pdf"
    },
    {
        "internal_id": 236,
        "title": "Online Pricing with Offline Data: Phase Transition and Inverse Square Law",
        "abstract": "This paper investigates the impact of pre-existing offline data on online learning, in the context of dynamic pricing. We study a single-product dy- namic pricing problem over a selling horizon of T periods. The demand in each period is deter- mined by the price of the product according to a linear demand model with unknown parame- ters. We assume that the seller already has some pre-existing offline data before the start of the sell- ing horizon. The seller wants to utilize both the pre-existing offline data and the sequential online data to minimize the regret of the online learning process. We characterize the joint effect of the size, location and dispersion of the offline data on the optimal regret of the online learning process. Our results reveal surprising transformations of the optimal regret rate with respect to the size of the offline data, which we refer to as phase tran- sitions. In addition, our results demonstrate that the location and dispersion of the offline data also have an intrinsic effect on the optimal regret, and we quantify this effect via the inverse-square law.",
        "authors": [],
        "created_time": "12 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Online Pricing with Offline Data  Phase Transition and Inverse Square Law.pdf"
    },
    {
        "internal_id": 238,
        "title": "Preselection Bandits",
        "abstract": "In this paper, we introduce the Preselection Bandit  problem, in which the learner preselects a subset  of arms (choice alternatives) for a user, which  then chooses the final arm from this subset. The  learner is not aware of the user's preferences, but  can learn them from observed choices. In our con\u00ad crete setting, we allow these choices to be stochas\u00ad tic and model the user's actions by means of the  Plackett-Luce model. The learner's main task is  to preselect subsets that eventually lead to highly  preferred choices. To formalize this goal, we in\u00ad troduce a reasonable notion of regret and derive  lower bounds on the expected regret. Moreover,  we propose algorithms for which the upper bound  on expected regret matches the lower bound up to  a logarithmic term of the time horizon.",
        "authors": [],
        "created_time": "12 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Preselection Bandits.pdf"
    },
    {
        "internal_id": 239,
        "title": "Enhanced POET: Open-ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions",
        "abstract": "Creating open-ended algorithms, which generate their own never-ending stream of novel and appro- priately challenging learning opportunities, could help to automate and accelerate progress in ma- chine learning. A recent step in this direction is the Paired Open-Ended Trailblazer (POET), an algorithm that generates and solves its own chal- lenges, and allows solutions to goal-switch be- tween challenges to avoid local optima. However, the original POET was unable to demonstrate its full creative potential because of limitations of the algorithm itself and because of external issues including a limited problem space and lack of a universal progress measure. Importantly, both lim- itations pose impediments not only for POET, but for the pursuit of open-endedness in general. Here we introduce and empirically validate two new in- novations to the original algorithm, as well as two external innovations designed to help elucidate its full potential. Together, these four advances en- able the most open-ended algorithmic demonstra- tion to date. The algorithmic innovations are (1) a domain-general measure of how meaningfully novel new challenges are, enabling the system to potentially create and solve interesting chal- lenges endlessly, and (2) an efficient heuristic for determining when agents should goal-switch from one problem to another (helping open-ended search better scale). Outside the algorithm itself, to enable a more definitive demonstration of open- endedness, we introduce (3) a novel, more flexible way to encode environmental challenges, and (4) a generic measure of the extent to which a sys- tem continues to exhibit open-ended innovation. Enhanced POET produces a diverse range of so-",
        "authors": [],
        "created_time": "12 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Enhanced POET  Open ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions.pdf"
    },
    {
        "internal_id": 240,
        "title": "Towards Accurate Post-training Network Quantization via Bit-Split and Stitching",
        "abstract": "Network quantization is essential for deploying deep models to IoT devices due to its high effi- ciency. Most existing quantization approaches rely on the full training datasets and the time- consuming fine-tuning to retain accuracy. Post- training quantization does not have these prob- lems, however, it has mainly been shown effec- tive for 8-bit quantization due to the simple op- timization strategy. In this paper, we propose a Bit-Split and Stitching framework (Bit-split) for lower-bit post-training quantization with minimal accuracy degradation. The proposed framework is validated on a variety of computer vision tasks, including image classification, object detection, instance segmentation, with various network ar- chitectures. Specifically, Bit-split can achieve near-original model performance even when quan- tizing FP32 models to INT3 without fine-tuning.",
        "authors": [],
        "created_time": "15 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Towards Accurate Post training Network Quantization via Bit Split and Stitching.pdf"
    },
    {
        "internal_id": 241,
        "title": "A Nearly-Linear Time Algorithm for Exact Community Recovery in Stochastic Block Model",
        "abstract": "Learning community structures in graphs that are randomly generated by stochastic block models (SBMs) has received much attention lately. In this paper, we focus on the problem of exactly re- covering the communities in a binary symmetric SBM, where a graph of n vertices is partitioned into two equal-sized communities and the vertices are connected with probability p = \u03b1 log(n)/n within communities and q = \u03b2 log(n)/n across communities for some \u03b1 > \u03b2 > 0. We propose a two-stage iterative algorithm for solving this problem, which employs the power method with a random starting point in the first stage and turns to a generalized power method that can identify the communities in a finite number of iterations in the second stage. It is shown that for any fixed \u03b1 and \u03b2 such that \u221a\u03b1\u2212\u221a\u03b2 > \u221a",
        "authors": [],
        "created_time": "06 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/A Nearly Linear Time Algorithm for Exact Community Recovery in Stochastic Block Model.pdf"
    },
    {
        "internal_id": 242,
        "title": "Low Bias Low Variance Gradient Estimates for Boolean Stochastic Networks",
        "abstract": "Stochastic neural networks with discrete random variables are an important class of models for their expressiveness and interpretability. Since direct differentiation and backpropagation is not possible, Monte Carlo gradient estimation tech- niques are a popular alternative. Efficient stochas- tic gradient estimators, such Straight-Through and Gumbel-Softmax, work well for shallow stochas- tic models. Their performance, however, suffers with hierarchical, more complex models. We fo- cus on stochastic networks with Boolean latent variables. To analyze such networks, we introduce the framework of harmonic analysis for Boolean functions to derive an analytic formulation for the bias and variance in the Straight-Through estima- tor. Exploiting these formulations, we propose FouST, a low-bias and low-variance gradient es- timation algorithm that is just as efficient. Ex- tensive experiments show that FouST performs favorably compared to state-of-the-art biased esti- mators and is much faster than unbiased ones.",
        "authors": [],
        "created_time": "11 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Low Bias Low Variance Gradient Estimates for Boolean Stochastic Networks.pdf"
    },
    {
        "internal_id": 243,
        "title": "Implicit Generative Modeling for Efficient Exploration",
        "abstract": "Efficient exploration remains a challenging prob- lem in reinforcement learning, especially for those tasks where rewards from environments are sparse. In this work, we introduce an explo- ration approach based on a novel implicit gener- ative modeling algorithm to estimate a Bayesian uncertainty of the agent's belief of the environ- ment dynamics. Each random draw from our generative model is a neural network that in- stantiates the dynamic function, hence multiple draws would approximate the posterior, and the variance in the predictions based on this poste- rior is used as an intrinsic reward for exploration. We design a training algorithm for our generative model based on the amortized Stein Variational Gradient Descent. In experiments, we demon- strate the effectiveness of this exploration algo- rithm in both pure exploration tasks and a down- stream task, comparing with state-of-the-art in- trinsic reward-based exploration approaches, in- cluding two recent approaches based on an en- semble of dynamic models. In challenging ex- ploration tasks, our implicit generative model consistently outperforms competing approaches regarding data efficiency in exploration.",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Implicit Generative Modeling for Efficient Exploration.pdf"
    },
    {
        "internal_id": 244,
        "title": "Provable Representation Learning for Imitation Learning",
        "abstract": "A common strategy in modern learning systems is to learn a representation that is useful for many tasks, a.k.a. representation learning. We study this strategy in the imitation learning setting for Markov decision processes (MDPs) where multi- ple experts' trajectories are available. We formu- late representation learning as a bi-level optimiza- tion problem where the \"outer\" optimization tries to learn the joint representation and the \"inner\" optimization encodes the imitation learning setup and tries to learn task-specific parameters. We instantiate this framework for the imitation learn- ing settings of behavior cloning and observation- alone. Theoretically, we show using our frame- work that representation learning can provide sam- ple complexity benefits for imitation learning in both settings. We also provide proof-of-concept experiments to verify our theory.",
        "authors": [],
        "created_time": "15 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Provable Representation Learning for Imitation Learning via Bi level Optimization.pdf"
    },
    {
        "internal_id": 245,
        "title": "Improved Optimistic Algorithms for Logistic Bandits",
        "abstract": "The generalized linear bandit framework has at- tracted a lot of attention in recent years by ex- tending the well-understood linear setting and allowing to model richer reward structures. It notably covers the logistic model, widely used when rewards are binary. For logistic bandits, the frequentist regret guarantees of existing al- gorithms are \u02dcO(\u03ba \u221a",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Improved Optimistic Algorithms for Logistic Bandits.pdf"
    },
    {
        "internal_id": 247,
        "title": "Obtaining Adjustable Regularization for Free via Iterate Averaging",
        "abstract": "Regularization for optimization is a crucial tech- nique to avoid overfitting in machine learning. In order to obtain the best performance, we usually train a model by tuning the regularization param- eters. It becomes costly, however, when a single round of training takes significant amount of time. Very recently, Neu & Rosasco (2018) show that if we run stochastic gradient descent (SGD) on linear regression problems, then by averaging the SGD iterates properly, we obtain a regularized so- lution. It left open whether the same phenomenon can be achieved for other optimization problems and algorithms. In this paper, we establish an aver- aging scheme that provably converts the iterates of SGD on an arbitrary strongly convex and smooth objective function to its regularized counterpart with an adjustable regularization parameter. Our approaches can be used for accelerated and pre- conditioned optimization methods as well. We further show that the same methods work empir- ically on more general optimization objectives including neural networks. In sum, we obtain ad- justable regularization for free for a large class of optimization problems and resolve an open ques- tion raised by Neu & Rosasco (2018).",
        "authors": [],
        "created_time": "15 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Obtaining Adjustable Regularization for Free via Iterate Averaging.pdf"
    },
    {
        "internal_id": 248,
        "title": "Online Learning for Active Cache Synchronization",
        "abstract": "Existing multi-armed bandit (MAB) models make two implicit assumptions: an arm generates a pay- off only when it is played, and the agent observes every payoff that is generated. This paper intro- duces SYNCHRONIZATION BANDITS, a MAB vari- ant where all arms generate costs at all times, but the agent observes an arm's instantaneous cost only when the arm is played. SYNCHRONIZATION MABs are inspired by online caching scenarios such as Web crawling, where an arm corresponds to a cached item and playing the arm means down- loading its fresh copy from a server. We present MIRRORSYNC, an online learning algorithm for",
        "authors": [],
        "created_time": "15 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Online Learning for Active Cache Synchronization.pdf"
    },
    {
        "internal_id": 249,
        "title": "Sub-linear Memory Sketches for Near Neighbor Search on Streaming Data",
        "abstract": "We present the first sublinear memory sketch that can be queried to find the nearest neighbors in a dataset. Our online sketching algorithm com- presses an N element dataset to a sketch of size O(N b log3 N) in O(N (b+1) log3 N) time, where b < 1. This sketch can correctly report the near- est neighbors of any query that satisfies a sta- bility condition parameterized by b. We achieve sublinear memory performance on stable queries by combining recent advances in locality sensi- tive hash (LSH)-based estimators, online kernel density estimation, and compressed sensing. Our theoretical results shed new light on the memory- accuracy tradeoff for nearest neighbor search, and our sketch, which consists entirely of short integer arrays, has a variety of attractive features in practice. We evaluate the memory-recall trade- off of our method on a friend recommendation task in the Google Plus social media network. We obtain orders of magnitude better compres- sion than the random projection based alternative while retaining the ability to report the nearest neighbors of practical queries.",
        "authors": [],
        "created_time": "15 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Sub linear Memory Sketches for Near Neighbor Search on Streaming Data.pdf"
    },
    {
        "internal_id": 250,
        "title": "Confidence Sets and Hypothesis Testing in a Likelihood-Free Inference Setting",
        "abstract": "Parameter estimation, statistical tests and confi- dence sets are the cornerstones of classical statis- tics that allow scientists to make inferences about the underlying process that generated the ob- served data. A key question is whether one can still construct hypothesis tests and confidence sets with proper coverage and high power in a so- called likelihood-free inference (LFI) setting; that is, a setting where the likelihood is not explic- itly known but one can forward-simulate observ- able data according to a stochastic model. In this paper, we present ACORE (Approximate Compu- tation via Odds Ratio Estimation), a frequentist approach to LFI that first formulates the classical likelihood ratio test (LRT) as a parametrized clas- sification problem, and then uses the equivalence of tests and confidence sets to build confidence regions for parameters of interest. We also present a goodness-of-fit procedure for checking whether the constructed tests and confidence regions are valid. ACORE is based on the key observation that the LRT statistic, the rejection probability of the test, and the coverage of the confidence set are conditional distribution functions which of- ten vary smoothly as a function of the parameters of interest. Hence, instead of relying solely on samples simulated at fixed parameter settings (as is the convention in standard Monte Carlo solu- tions), one can leverage machine learning tools and data simulated in the neighborhood of a pa- rameter to improve estimates of quantities of in- terest. We demonstrate the efficacy of ACORE with both theoretical and empirical results. Our implementation is available on Github.",
        "authors": [],
        "created_time": "13 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Confidence Sets and Hypothesis Testing in a Likelihood Free Inference Setting.pdf"
    },
    {
        "internal_id": 251,
        "title": "Fast and Consistent Learning of Hidden Markov Models by Incorporating Non-Consecutive Correlations",
        "abstract": "Can the parameters of a hidden Markov model (HMM) be estimated from a single sweep through the observations \u2013 and additionally, without be- ing trapped at a local optimum in the likelihood surface? That is the premise of recent method of moments algorithms devised for HMMs. In these, correlations between consecutive pair- or triplet- wise observations are empirically estimated and used to compute estimates of the HMM parame- ters. Albeit computationally very attractive, the main drawback is that by restricting to only low- order correlations in the data, information is being neglected which results in a loss of accuracy (com- pared to standard maximum likelihood schemes). In this paper, we propose extending these meth- ods (both pair- and triplet-based) by also includ- ing non-consecutive correlations in a way which does not significantly increase the computational cost (which scales linearly with the number of additional lags included). We prove strong con- sistency of the new methods, and demonstrate an improved performance in numerical experiments on both synthetic and real-world financial time- series datasets.",
        "authors": [],
        "created_time": "10 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Fast and Consistent Learning of Hidden Markov Models by Incorporating Non Consecutive Correlations.pdf"
    },
    {
        "internal_id": 252,
        "title": "Unlabelled Data Improves Bayesian Uncertainty Calibration under Covariate Shift",
        "abstract": "Modern neural networks have proven to be pow- erful function approximators, providing state-of- the-art performance in a multitude of applications. They however fall short in their ability to quan- tify confidence in their predictions \u2014 this is cru- cial in high-stakes applications that involve crit- ical decision-making. Bayesian neural networks (BNNs) aim at solving this problem by placing a prior distribution over the network's parameters, thereby inducing a posterior distribution that en- capsulates predictive uncertainty. While existing variants of BNNs based on Monte Carlo dropout produce reliable (albeit approximate) uncertainty estimates over in-distribution data, they tend to exhibit over-confidence in predictions made on target data whose feature distribution differs from the training data, i.e., the covariate shift setup. In this paper, we develop an approximate Bayesian inference scheme based on posterior regularisa- tion, wherein unlabelled target data are used as \"pseudo-labels\" of model confidence that are used to regularise the model's loss on labelled source data. We show that this approach significantly im- proves the accuracy of uncertainty quantification on covariate-shifted data sets, with minimal modi- fication to the underlying model architecture. We demonstrate the utility of our method in the con- text of transferring prognostic models of prostate cancer across globally diverse populations.",
        "authors": [],
        "created_time": "13 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Unlabelled Data Improves Bayesian Uncertainty Calibration under Covariate Shift.pdf"
    },
    {
        "internal_id": 253,
        "title": "Taylor Expansion Policy Optimization",
        "abstract": "In this work, we investigate the application of Taylor expansions in reinforcement learning. In particular, we propose Taylor expansion policy op- timization, a policy optimization formalism that generalizes prior work (e.g., TRPO) as a first- order special case. We also show that Taylor ex- pansions intimately relate to off-policy evaluation. Finally, we show that this new formulation entails modifications which improve the performance of several state-of-the-art distributed algorithms.",
        "authors": [],
        "created_time": "",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Taylor Expansion Policy Optimization.pdf"
    },
    {
        "internal_id": 254,
        "title": "Unsupervised Discovery of Interpretable Directions in the GAN Latent Space",
        "abstract": "The latent spaces of GAN models often have semantically meaningful directions. Mov- ing in these directions corresponds to human- interpretable image transformations, such as zooming or recoloring, enabling a more control- lable generation process. However, the discov- ery of such directions is currently performed in a supervised manner, requiring human labels, pre- trained models, or some form of self-supervision. These requirements severely restrict a range of di- rections existing approaches can discover. In this paper, we introduce an unsupervised method to identify interpretable directions in the latent space of a pretrained GAN model. By a simple model- agnostic procedure, we find directions correspond- ing to sensible semantic manipulations without any form of (self-)supervision. Furthermore, we reveal several non-trivial findings, which would be difficult to obtain by existing methods, e.g., a direction corresponding to background removal. As an immediate practical benefit of our work, we show how to exploit this finding to achieve competitive performance for weakly-supervised saliency detection. The implementation of our method is available online1.",
        "authors": [],
        "created_time": "13 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Unsupervised Discovery of Interpretable Directions in the GAN Latent Space.pdf"
    },
    {
        "internal_id": 255,
        "title": "Planning to Explore via Self-Supervised World Models",
        "abstract": "Reinforcement learning allows solving complex tasks, however, the learning tends to be task- specific and the sample efficiency remains a challenge. We present Plan2Explore, a self- supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task- specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code: https://ramanans1.github.io/ plan2explore/",
        "authors": [],
        "created_time": "15 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Planning to Explore via Self Supervised World Models.pdf"
    },
    {
        "internal_id": 256,
        "title": "Graph-based, Self-Supervised Program Repair from Diagnostic Feedback",
        "abstract": "We consider the problem of learning to repair pro- grams from diagnostic feedback (e.g., compiler error messages). Program repair is challenging for two reasons: First, it requires reasoning and tracking symbols across source code and diag- nostic feedback. Second, labeled datasets avail- able for program repair are relatively small. In this work, we propose novel solutions to these two challenges. First, we introduce a program- feedback graph, which connects symbols relevant to program repair in source code and diagnostic feedback, and then apply a graph neural network on top to model the reasoning process. Second, we present a self-supervised learning paradigm for program repair that leverages unlabeled pro- grams available online to create a large amount of extra program repair examples, which we use to pre-train our models. We evaluate our pro- posed approach on two applications: correcting introductory programming assignments (DeepFix dataset) and correcting the outputs of program synthesis (SPoC dataset). Our final system, DrRe- pair, significantly outperforms prior work, achiev- ing 68.2% full repair rate on DeepFix (+22.9% over the prior best), and 48.4% synthesis success rate on SPoC (+3.7% over the prior best).",
        "authors": [
            "Michihiro Yasunaga ",
            "Percy Liang "
        ],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Graph based  Self Supervised Program Repair from Diagnostic Feedback.pdf"
    },
    {
        "internal_id": 257,
        "title": "ACFlow: Flow Models for Arbitrary Conditional Likelihoods",
        "abstract": "Understanding the dependencies among features of a dataset is at the core of most unsupervised learning tasks. However, a majority of gener- ative modeling approaches are focused solely on the joint distribution p(x) and utilize models where it is intractable to obtain the conditional distribution of some arbitrary subset of features xu given the rest of the observed covariates xo: p(xu | xo). Traditional conditional approaches provide a model for a fixed set of covariates condi- tioned on another fixed set of observed covariates. Instead, in this work we develop a model that is capable of yielding all conditional distributions p(xu | xo) (for arbitrary xu) via tractable con- ditional likelihoods. We propose a novel exten- sion of (change of variables based) flow genera- tive models, arbitrary conditioning flow models (ACFlow). ACFlow can be conditioned on arbi- trary subsets of observed covariates, which was previously infeasible. We further extend ACFlow to model the joint distributions p(x) and arbi- trary marginal distributions p(xu). We also apply ACFlow to the imputation of features, and develop a unified platform for both multiple and single im- putation by introducing an auxiliary objective that provides a principled single \"best guess\" for flow models. Extensive empirical evaluations show that our model achieves state-of-the-art perfor- mance in modeling arbitrary conditional distri- butions in addition to both single and multiple imputation in synthetic and real-world datasets.",
        "authors": [],
        "created_time": "08 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/ACFlow  Flow Models for Arbitrary Conditional Likelihoods.pdf"
    },
    {
        "internal_id": 258,
        "title": "Optimal Estimator for Unlabeled Linear Regression",
        "abstract": "Unlabeled linear regression, or \"linear regression with an unknown permutation\", has attracted increasing attentions due to its applications in (e.g.,) linkage record and de-anonymization. However, the computation of unlabeled linear re- gression proves to be cumbersome and existing algorithms typically require considerable time, especially in the high dimensional regime. In this paper, we propose a one-step estimator which is optimal from both the computational and the statistical aspects. From the computational per- spective, our estimator exhibits the same order of computational complexity as that of the ora- cle case (which means the regression coefficients are known in advance and only the permutation needs recovery). From the statistical perspective, when comparing with the necessary conditions for permutation recovery, our requirement on the signal-to-noise ratio (SNR) agrees up to merely \u03a9 (log log n) difference when the stable rank of the regression coefficients B\u266e is much less than log n/ log log n. Numerical experiments are also provided to corroborate the theoretical claims.",
        "authors": [
            "Hang Zhang",
            "Ping Li",
            "WA "
        ],
        "created_time": "15 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Optimal Estimator for Unlabeled Linear Regression.pdf"
    },
    {
        "internal_id": 259,
        "title": "MetaFun: Meta-Learning with Iterative Functional Updates",
        "abstract": "We develop a functional encoder-decoder ap- proach to supervised meta-learning, where la- beled data is encoded into an infinite-dimensional functional representation rather than a finite- dimensional one. Furthermore, rather than di- rectly producing the representation, we learn a neural update rule resembling functional gradient descent which iteratively improves the representa- tion. The final representation is used to condition the decoder to make predictions on unlabeled data. Our approach is the first to demonstrates the suc- cess of encoder-decoder style meta-learning meth- ods like conditional neural processes on large- scale few-shot classification benchmarks such as miniImageNet and tieredImageNet, where it achieves state-of-the-art performance.",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/MetaFun  Meta Learning with Iterative Functional Updates.pdf"
    },
    {
        "internal_id": 260,
        "title": "Online Learning with Imperfect Hints",
        "abstract": "We consider a variant of the classical online lin- ear optimization problem in which at every step, the online player receives a \"hint\" vector before choosing the action for that round. Rather sur- prisingly, it was shown that if the hint vector is guaranteed to have a positive correlation with the cost vector, then the online player can achieve a regret of O(log T), thus significantly improv- ing over the O( \u221a",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Online Learning with Imperfect Hints.pdf"
    },
    {
        "internal_id": 261,
        "title": "Improving the Gating Mechanism of Recurrent Neural Networks",
        "abstract": "Gating mechanisms are widely used in neural network models, where they allow gradients to backpropagate more easily through depth or time. However, their saturation property introduces problems of its own. For example, in recurrent models these gates need to have outputs near 1 to propagate information over long time-delays, which requires them to operate in their saturation regime and hinders gradient-based learning of the gate mechanism. We address this problem by deriving two synergistic modifications to the stan- dard gating mechanism that are easy to implement, introduce no additional hyperparameters, and improve learnability of the gates when they are close to saturation. We show how these changes are related to and improve on alternative recently proposed gating mechanisms such as chrono initialization and Ordered Neurons. Empirically, our simple gating mechanisms robustly improve the performance of recurrent models on a range of applications, including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning, particularly when long-term dependencies are involved.",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Improving the Gating Mechanism of Recurrent Neural Networks.pdf"
    },
    {
        "internal_id": 262,
        "title": "Gradient-free Online Learning in Games with Delayed Rewards",
        "abstract": "Motivated by applications to online advertising and recommender systems, we consider a game- theoretic model with delayed rewards and asyn- chronous, payoff-based feedback. In contrast to previous work on delayed multi-armed bandits, we focus on multi-player games with continuous action spaces, and we examine the long-run be- havior of strategic agents that follow a no-regret learning policy (but are otherwise oblivious to the game being played, the objectives of their oppo- nents, etc.). To account for the lack of a consistent stream of information (for instance, rewards can arrive out of order, with an a priori unbounded delay, etc.), we introduce a gradient-free learning policy where payoff information is placed in a priority queue as it arrives. In this general con- text, we derive new bounds for the agents' regret; furthermore, under a standard diagonal concavity assumption, we show that the induced sequence of play converges to Nash equilibrium (NE) with probability 1, even if the delay between choosing an action and receiving the corresponding reward is unbounded.",
        "authors": [],
        "created_time": "15 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Gradient free Online Learning in Continuous Games with Delayed Rewards.pdf"
    },
    {
        "internal_id": 263,
        "title": "Accelerated Stochastic Gradient-free and Projection-free Methods",
        "abstract": "In the paper, we propose a class of accelerated stochastic gradient-free and projection-free (a.k.a., zeroth-order Frank-Wolfe) methods to solve the constrained stochastic and finite-sum nonconvex optimization. Specifically, we propose an accel- erated stochastic zeroth-order Frank-Wolfe (Acc- SZOFW) method based on the variance reduced technique of SPIDER/SpiderBoost and a novel momentum accelerated technique. Moreover, un- der some mild conditions, we prove that the Acc- SZOFW has the function query complexity of O(d\u221an\u03f5\u22122) for finding an \u03f5-stationary point in the finite-sum problem, which improves the ex- iting best result by a factor of O(\u221an\u03f5\u22122), and has the function query complexity of O(d\u03f5\u22123) in the stochastic problem, which improves the ex- iting best result by a factor of O(\u03f5\u22121). To relax the large batches required in the Acc-SZOFW, we further propose a novel accelerated stochastic zeroth-order Frank-Wolfe (Acc-SZOFW*) based on a new variance reduced technique of STORM, which still reaches the function query complex- ity of O(d\u03f5\u22123) in the stochastic problem without relying on any large batches. In particular, we present an accelerated framework of the Frank- Wolfe methods based on the proposed momentum accelerated technique. The extensive experimen- tal results on black-box adversarial attack and robust black-box classification demonstrate the efficiency of our algorithms.",
        "authors": [],
        "created_time": "11 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Accelerated Stochastic Gradient free and Projection free Methods.pdf"
    },
    {
        "internal_id": 264,
        "title": "A Natural Lottery Ticket Winner: Reinforcement Learning with Ordinary Neural Circuits",
        "abstract": "We propose a neural information processing sys- tem obtained by re-purposing the function of a biological neural circuit model to govern simu- lated and real-world control tasks. Inspired by the structure of the nervous system of the soil- worm, C. elegans, we introduce ordinary neural circuits (ONCs), defined as the model of biologi- cal neural circuits reparameterized for the control of alternative tasks. We first demonstrate that ONCs realize networks with higher maximum flow compared to arbitrary wired networks. We then learn instances of ONCs to control a series of robotic tasks, including the autonomous parking of a real-world rover robot. For reconfiguration of the purpose of the neural circuit, we adopt a search-based optimization algorithm. Ordinary neural circuits perform on par and, in some cases, significantly surpass the performance of contem- porary deep learning models. ONC networks are compact, 77% sparser than their counterpart neu- ral controllers, and their neural dynamics are fully interpretable at the cell-level.",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/A Natural Lottery Ticket Winner  Reinforcement Learning with Ordinary Neural Circuits.pdf"
    },
    {
        "internal_id": 265,
        "title": "Handling the Positive-Definite Constraint in the Bayesian Learning Rule",
        "abstract": "The Bayesian learning rule is a natural-gradient variational inference method, which not only con- tains many existing learning algorithms as special cases but also enables the design of new algo- rithms. Unfortunately, when variational parame- ters lie in an open constraint set, the rule may not satisfy the constraint and requires line-searches which could slow down the algorithm. In this work, we address this issue for positive-definite constraints by proposing an improved rule that naturally handles the constraints. Our modifica- tion is obtained by using Riemannian gradient methods, and is valid when the approximation at- tains a block-coordinate natural parameterization (e.g., Gaussian distributions and their mixtures). Our method outperforms existing methods with- out any significant increase in computation. Our work makes it easier to apply the rule in the pres- ence of positive-definite constraints in parameter spaces.",
        "authors": [],
        "created_time": "17 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Handling the Positive Definite Constraint in the Bayesian Learning Rule.pdf"
    },
    {
        "internal_id": 266,
        "title": "Dispersed Exponential Family Mixture VAEs for Interpretable Text Generation",
        "abstract": "Deep generative models are commonly used for generating images and text. Interpretability of these models is one important pursuit, other than the generation quality. Variational auto-encoder (VAE) with Gaussian distribution as prior has been successfully applied in text generation, but it is hard to interpret the meaning of the latent variable. To enhance the controllability and in- terpretability, one can replace the Gaussian prior with a mixture of Gaussian distributions (GM- VAE), whose mixture components could be re- lated to hidden semantic aspects of data. In this paper, we generalize the practice and introduce DEM-VAE, a class of models for text genera- tion using VAEs with a mixture distribution of exponential family. Unfortunately, a standard variational training algorithm fails due to the mode-collapse problem. We theoretically iden- tify the root cause of the problem and propose an effective algorithm to train DEM-VAE. Our method penalizes the training with an extra dis- persion term to induce a well-structured latent space. Experimental results show that our ap- proach does obtain a meaningful space, and it outperforms strong baselines in text generation benchmarks. The code is available at https: //github.com/wenxianxian/demvae.",
        "authors": [],
        "created_time": "15 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Dispersed Exponential Family Mixture VAEs for Interpretable Text Generation.pdf"
    },
    {
        "internal_id": 267,
        "title": "UNILMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training",
        "abstract": "We propose to pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked lan- guage model (PMLM). Given an input text with masked tokens, we rely on conventional masks to learn inter-relations between corrupted tokens and context via autoencoding, and pseudo masks to learn intra-relations between masked spans via partially autoregressive modeling. With well- designed position embeddings and self-attention masks, the context encodings are reused to avoid redundant computation. Moreover, conventional masks used for autoencoding provide global mask- ing information, so that all the position embed- dings are accessible in partially autoregressive language modeling. In addition, the two tasks pre-train a unified language model as a bidi- rectional encoder and a sequence-to-sequence decoder, respectively. Our experiments show that the unified language models pre-trained using PMLM achieve new state-of-the-art re- sults on a wide range of language understand- ing and generation tasks across several widely used benchmarks. The code and pre-trained mod- els are available at https://github.com/ microsoft/unilm.",
        "authors": [],
        "created_time": "15 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/UniLMv2  Pseudo Masked Language Models for Unified Language Model Pre Training.pdf"
    },
    {
        "internal_id": 268,
        "title": "Provably Efficient Exploration in Policy Optimization",
        "abstract": "While policy-based reinforcement learning (RL) achieves tremendous successes in practice, it is significantly less understood in theory, especially compared with value-based RL. In particular, it remains elusive how to design a provably efficient policy optimization algorithm that incorporates exploration. To bridge such a gap, this paper pro- poses an Optimistic variant of the Proximal Policy Optimization algorithm (OPPO), which follows an \"optimistic version\" of the policy gradient di- rection. This paper proves that, in the problem of episodic Markov decision process with linear function approximation, unknown transition, and adversarial reward with full-information feedback, OPPO achieves eO(",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Provably Efficient Exploration in Policy Optimization.pdf"
    },
    {
        "internal_id": 269,
        "title": "Goal-Aware Prediction: Learning to Model What Matters",
        "abstract": "Learned dynamics models combined with both planning and policy learning algorithms have shown promise in enabling artificial agents to learn to perform many diverse tasks with limited supervision. However, one of the fundamental challenges in using a learned forward dynamics model is the mismatch between the objective of the learned model (future state reconstruction), and that of the downstream planner or policy (completing a specified task). This issue is ex- acerbated by vision-based control tasks in diverse real-world environments, where the complexity of the real world dwarfs model capacity. In this paper, we propose to direct prediction towards task relevant information, enabling the model to be aware of the current task and encouraging it to only model relevant quantities of the state space, resulting in a learning objective that more closely matches the downstream task. Further, we do so in an entirely self-supervised manner, without the need for a reward function or image labels. We find that our method more effectively models the relevant parts of the scene conditioned on the goal, and as a result outperforms standard task-agnostic dynamics models and model-free reinforcement learning.",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Goal Aware Prediction  Learning to Model What Matters.pdf"
    },
    {
        "internal_id": 270,
        "title": "A Markov Decision Process Model for Socio-Economic Systems Impacted by Climate Change",
        "abstract": "Coastal communities are at high risk of natural hazards due to unremitting global warming and sea level rise. Both the catastrophic impacts, e.g., tidal flooding and storm surges, and the long-term impacts, e.g., beach erosion, inundation of low ly- ing areas, and saltwater intrusion into aquifers, cause economic, social, and ecological losses. Creating policies through appropriate modeling of the responses of stakeholders, such as govern- ment, businesses, and residents, to climate change and sea level rise scenarios can help to reduce these losses. In this work, we propose a Markov decision process (MDP) formulation for an agent (government) which interacts with the environ- ment (nature and residents) to deal with the im- pacts of climate change, in particular sea level rise. Through theoretical analysis we show that a reasonable government's policy on infrastructure development ought to be proactive and based on detected sea levels in order to minimize the ex- pected total cost, as opposed to a straightforward government that reacts to observed costs from nature. We also provide a deep reinforcement learning-based scenario planning tool considering different government and resident types in terms of cooperation, and different sea level rise projec- tions by the National Oceanic and Atmospheric Administration (NOAA).",
        "authors": [],
        "created_time": "04 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/A Markov Decision Process Model for Socio Economic Systems Impacted by Climate Change.pdf"
    },
    {
        "internal_id": 271,
        "title": "Improving Generalization by Controlling",
        "abstract": "In the presence of noisy or incorrect labels, neural networks have the undesirable tendency to memo- rize information about the noise. Standard regular- ization techniques such as dropout, weight decay or data augmentation sometimes help, but do not prevent this behavior. If one considers neural net- work weights as random variables that depend on the data and stochasticity of training, the amount of memorized information can be quantified with the Shannon mutual information between weights and the vector of all training labels given inputs, I(w; y | x). We show that for any training al- gorithm, low values of this term correspond to reduction in memorization of label-noise and bet- ter generalization bounds. To obtain these low values, we propose training algorithms that em- ploy an auxiliary network that predicts gradients in the final layers of a classifier without access- ing labels. We illustrate the effectiveness of our approach on versions of MNIST, CIFAR-10, and CIFAR-100 corrupted with various noise models, and on a large-scale dataset Clothing1M that has noisy labels.",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Improving generalization by controlling label noise information in neural network weights.pdf"
    },
    {
        "internal_id": 272,
        "title": "Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE",
        "abstract": "The empirical performance of neural ordinary dif- ferential equations (NODEs) is significantly infe- rior to discrete-layer models on benchmark tasks (e.g. image classification). We demonstrate an ex- planation is the inaccuracy of existing gradient es- timation methods: the adjoint method has numer- ical errors in reverse-mode integration; the naive method suffers from a redundantly deep computa- tion graph. We propose the Adaptive Checkpoint Adjoint (ACA) method: ACA applies a trajectory checkpoint strategy which records the forward- mode trajectory as the reverse-mode trajectory to guarantee accuracy; ACA deletes redundant components for shallow computation graphs; and ACA supports adaptive solvers. On image classifi- cation tasks, compared with the adjoint and naive method, ACA achieves half the error rate in half the training time; NODE trained with ACA out- performs ResNet in both accuracy and test-retest reliability. On time-series modeling, ACA outper- forms competing methods. Furthermore, NODE with ACA can incorporate physical knowledge to achieve better accuracy.",
        "authors": [],
        "created_time": "01 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE.pdf"
    },
    {
        "internal_id": 273,
        "title": "Extragradient with player sampling for faster Nash equilibrium finding",
        "abstract": "Data-driven modeling increasingly requires to find a Nash equilibrium in multi-player games, e.g. when training GANs. In this paper, we analyse a new extra-gradient method for Nash equilibrium finding, that performs gradient extrapolations and updates on a random subset of players at each iteration. This approach provably exhibits a better rate of convergence than full extra-gradient for non-smooth convex games with noisy gradient oracle. We propose an additional variance reduc- tion mechanism to obtain speed-ups in smooth convex games. Our approach makes extrapolation amenable to massive multiplayer settings, and brings empirical speed-ups, in particular when using a heuristic cyclic sampling scheme. Most importantly, it allows to train faster and better GANs and mixtures of GANs.",
        "authors": [],
        "created_time": "",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Extra gradient with player sampling for faster convergence in n player games.pdf"
    },
    {
        "internal_id": 274,
        "title": "DINO: Distributed Newton-Type Optimization Method",
        "abstract": "We present a novel communication-efficient Newton-type algorithm for finite-sum optimiza- tion over a distributed computing environment. Our method, named DINO, overcomes both theo- retical and practical shortcomings of similar ex- isting methods. Under minimal assumptions, we guarantee global sub-linear convergence of DINO to a first-order stationary point for general non- convex functions and arbitrary data distribution over the network. Furthermore, for functions sat- isfying Polyak-Lojasiewicz (PL) inequality, we show that DINO enjoys a linear convergence rate. Our proposed algorithm is practically parameter free, in that it will converge regardless of the se- lected hyper-parameters, which are easy to tune. Additionally, its sub-problems are simple linear least-squares, for which efficient solvers exist, and numerical simulations demonstrate the efficiency of DINO as compared with similar alternatives.",
        "authors": [],
        "created_time": "13 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/DINO  Distributed Newton Type Optimization Method.pdf"
    },
    {
        "internal_id": 275,
        "title": "The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent",
        "abstract": "This paper studies how neural network architec- ture affects the speed of training. We introduce a simple concept called gradient confusion to help formally analyze this. When gradient confusion is high, stochastic gradients produced by different data samples may be negatively correlated, slow- ing down convergence. But when gradient confu- sion is low, data samples interact harmoniously, and training proceeds quickly. Through theoreti- cal and experimental results, we demonstrate how the neural network architecture affects gradient confusion, and thus the efficiency of training. Our results show that, for popular initialization tech- niques, increasing the width of neural networks leads to lower gradient confusion, and thus faster model training. On the other hand, increasing the depth of neural networks has the opposite effect. Our results indicate that alternate initial- ization techniques or networks using both batch normalization and skip connections help reduce the training burden of very deep networks.",
        "authors": [],
        "created_time": "11 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent.pdf"
    },
    {
        "internal_id": 276,
        "title": "Fast Differentiable Sorting and Ranking",
        "abstract": "The sorting operation is one of the most com- monly used building blocks in computer program- ming. In machine learning, it is often used for robust statistics. However, seen as a function, it is piecewise linear and as a result includes many kinks where it is non-differentiable. More problematic is the related ranking operator, often used for order statistics and ranking metrics. It is a piecewise constant function, meaning that its derivatives are null or undefined. While nu- merous works have proposed differentiable prox- ies to sorting and ranking, they do not achieve the O(n log n) time complexity one would expect from sorting and ranking operations. In this pa- per, we propose the first differentiable sorting and ranking operators with O(n log n) time and O(n) space complexity. Our proposal in addition enjoys exact computation and differentiation. We achieve this feat by constructing differentiable operators as projections onto the permutahedron, the con- vex hull of permutations, and using a reduction to isotonic optimization. Empirically, we confirm that our approach is an order of magnitude faster than existing approaches and showcase two novel applications: differentiable Spearman's rank cor- relation coefficient and least trimmed squares.",
        "authors": [],
        "created_time": "30 June 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Fast Differentiable Sorting and Ranking.pdf"
    },
    {
        "internal_id": 277,
        "title": "On hyperparameter tuning in general clustering problems",
        "abstract": "Tuning hyperparameters for unsupervised learn- ing problems is difficult in general due to the lack of ground truth for validation. However, the suc- cess of most clustering methods depends heavily on the correct choice of the involved hyperparame- ters. Take for example the Lagrange multipliers of penalty terms in semidefinite programming (SDP) relaxations of community detection in networks, or the bandwidth parameter needed in the Gaus- sian kernel used to construct similarity matrices for spectral clustering. Despite the popularity of these clustering algorithms, there are not many provable methods for tuning these hyperparam- eters. In this paper, we provide an overarching framework with provable guarantees for tuning hyperparameters in the above class of problems under two different models. Our framework can be augmented with a cross validation procedure to do model selection as well. In a variety of simula- tion and real data experiments, we show that our framework outperforms other widely used tuning procedures in a broad range of parameter settings.",
        "authors": [],
        "created_time": "",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/On hyperparameter tuning in general clustering problemsm.pdf"
    },
    {
        "internal_id": 278,
        "title": "Bayesian Learning from Sequential Data using Gaussian Processes with Signature Covariances",
        "abstract": "We develop a Bayesian approach to learning from sequential data by using Gaussian processes (GPs) with so-called signature kernels as covariance functions. This allows to make sequences of dif- ferent length comparable and to rely on strong theoretical results from stochastic analysis. Sig- natures capture sequential structure with tensors that can scale unfavourably in sequence length and state space dimension. To deal with this, we introduce a sparse variational approach with in- ducing tensors. We then combine the resulting GP with LSTMs and GRUs to build larger models that leverage the strengths of each of these approaches and benchmark the resulting GPs on multivariate time series (TS) classification datasets.",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Bayesian Learning from Sequential Data using Gaussian Processes with Signature Covariances.pdf"
    },
    {
        "internal_id": 279,
        "title": "Automatic Shortcut Removal for Self-Supervised Representation Learning",
        "abstract": "In self-supervised visual representation learning, a feature extractor is trained on a \"pretext task\" for which labels can be generated cheaply, with- out human annotation. A central challenge in this approach is that the feature extractor quickly learns to exploit low-level visual features such as color aberrations or watermarks and then fails to learn useful semantic representations. Much work has gone into identifying such \"shortcut\" features and hand-designing schemes to reduce their effect. Here, we propose a general frame- work for mitigating the effect shortcut features. Our key assumption is that those features which are the first to be exploited for solving the pre- text task may also be the most vulnerable to an adversary trained to make the task harder. We show that this assumption holds across common pretext tasks and datasets by training a \"lens\" net- work to make small image changes that maximally reduce performance in the pretext task. Repre- sentations learned with the modified images out- perform those learned without in all tested cases. Additionally, the modifications made by the lens reveal how the choice of pretext task and dataset affects the features learned by self-supervision.",
        "authors": [],
        "created_time": "30 June 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Automatic Shortcut Removal for Self Supervised Representation Learning.pdf"
    },
    {
        "internal_id": 280,
        "title": "Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation",
        "abstract": "Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned from a labeled source dataset to solve similar tasks in a new un- labeled domain. Prior UDA methods typically require to access the source data when learning to adapt the model, making them risky and inef- ficient for decentralized private data. This work tackles a practical setting where only a trained source model is available and investigates how we can effectively utilize such a model without source data to solve UDA problems. We pro- pose a simple yet generic representation learn- ing framework, named Source HypOthesis Trans- fer (SHOT). SHOT freezes the classifier module (hypothesis) of the source model and learns the target-specific feature extraction module by ex- ploiting both information maximization and self- supervised pseudo-labeling to implicitly align rep- resentations from the target domains to the source hypothesis. To verify its versatility, we evaluate SHOT in a variety of adaptation cases including closed-set, partial-set, and open-set domain adap- tation. Experiments indicate that SHOT yields state-of-the-art results among multiple domain adaptation benchmarks.",
        "authors": [],
        "created_time": "15 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Do We Really Need to Access the Source Data  Source Hypothesis Transfer for Unsupervised Domain Adaptation.pdf"
    },
    {
        "internal_id": 281,
        "title": "Kernel Methods for Cooperative Multi-Agent Contextual Bandits",
        "abstract": "Cooperative multi-agent decision making involves a group of agents cooperatively solving learning problems while communicating over a network with delays. In this paper, we consider the ker- nelised contextual bandit problem, where the re- ward obtained by an agent is an arbitrary linear function of the contexts' images in the related reproducing kernel Hilbert space (RKHS), and a group of agents must cooperate to collectively solve their unique decision problems. For this problem, we propose COOP-KERNELUCB, an al- gorithm that provides near-optimal bounds on the per-agent regret, and is both computationally and communicatively efficient. For special cases of the cooperative problem, we also provide variants of COOP-KERNELUCB that provides optimal per- agent regret. In addition, our algorithm general- izes several existing results in the multi-agent ban- dit setting. Finally, on a series of both synthetic and real-world multi-agent network benchmarks, we demonstrate that our algorithm significantly outperforms existing benchmarks.",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Kernel Methods for Cooperative Multi Agent Contextual Bandits.pdf"
    },
    {
        "internal_id": 282,
        "title": "Guided Learning of Nonconvex Models through Successive Functional Gradient Optimization",
        "abstract": "This paper presents a framework of successive functional gradient optimization for training non- convex models such as neural networks, where training is driven by mirror descent in a function space. We provide a theoretical analysis and em- pirical study of the training method derived from this framework. It is shown that the method leads to better performance than that of standard train- ing techniques.",
        "authors": [],
        "created_time": "07 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Guided Learning of Nonconvex Models through Successive Functional Gradient Optimization.pdf"
    },
    {
        "internal_id": 283,
        "title": "Understanding the Curse of Horizon in Off-Policy Evaluation via Conditional Importance Sampling",
        "abstract": "Off-policy policy estimators that use importance sampling (IS) can suffer from high variance in long-horizon domains, and there has been partic- ular excitement over new IS methods that lever- age the structure of Markov decision processes. We analyze the variance of the most popular ap- proaches through the viewpoint of conditional Monte Carlo. Surprisingly, we find that in fi- nite horizon MDPs there is no strict variance reduction of per-decision importance sampling or marginalized importance sampling, compar- ing with vanilla importance sampling. We then provide sufficient conditions under which the per- decision or marginalized estimators will provably reduce the variance over importance sampling with finite horizons. For the asymptotic (in terms of horizon T) case, we develop upper and lower bounds on the variance of those estimators which yields sufficient conditions under which there ex- ists an exponential v.s. polynomial gap between the variance of importance sampling and that of the per-decision or stationary/marginalized esti- mators. These results help advance our under- standing of if and when new types of IS estima- tors will improve the accuracy of off-policy esti- mation.",
        "authors": [],
        "created_time": "14 August 2020",
        "conference": "ICML",
        "filepath": "/Users/adit/papers/ICML/Understanding the Curse of Horizon in Off Policy Evaluation via Conditional Importance Sampling.pdf"
    },
    {
        "internal_id": 284,
        "title": "Enriching ImageNet with Human Similarity Judgments and Psychological Embeddings",
        "abstract": "Advances in supervised learning approaches to object recognition flourished in part because of the availability of high-quality datasets and associated benchmarks. How- ever, these benchmarks\u2014such as ILSVRC\u2014are relatively task-specific, focusing predominately on predicting class labels. We introduce a publicly-available dataset that em- bodies the task-general capabilities of human perception and reasoning. The Human Similarity Judgments exten- sion to ImageNet (ImageNet-HSJ) is composed of a large set of human similarity judgments that supplements the ex- isting ILSVRC validation set. The new dataset supports a range of task and performance metrics, including evalua- tion of unsupervised algorithms. We demonstrate two meth- ods of assessment: using the similarity judgments directly and using a psychological embedding trained on the simi- larity judgments. This embedding space contains an order of magnitude more points (i.e., images) than previous ef- forts based on human judgments. We were able to scale to the full 50,000 image ILSVRC validation set through a se- lective sampling process that used variational Bayesian in- ference and model ensembles to sample aspects of the em- bedding space that were most uncertain. To demonstrate the utility of ImageNet-HSJ, we used the similarity ratings and the embedding space to evaluate how well several pop- ular models conform to human similarity judgments. One finding is that the more complex models that perform better on task-specific benchmarks do not better conform to hu- man semantic judgments. In addition to the human similar- ity judgments, pre-trained psychological embeddings and code for inferring variational embeddings are made pub- licly available. ImageNet-HSJ supports the appraisal of in- ternal representations and the development of more human- like models.",
        "authors": [],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Roads_Enriching_ImageNet_With_Human_Similarity_Judgments_and_Psychological_Embeddings_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 285,
        "title": "What Can Style Transfer and Paintings Do For Model Robustness?",
        "abstract": "A common strategy for improving model robustness is through data augmentations. Data augmentations encour- age models to learn desired invariances, such as invariance to horizontal flipping or small changes in color. Recent work has shown that arbitrary style transfer can be used as a form of data augmentation to encourage invariance to textures by creating painting-like images from photographs. However, a stylized photograph is not quite the same as an artist-created painting. Artists depict perceptually meaningful cues in paintings so that humans can recognize salient components in scenes, an emphasis which is not enforced in style transfer. Therefore, we study how style transfer and paintings differ in their impact on model robustness. First, we investigate the role of paintings as style images for stylization-based data augmentation. We find that style transfer functions well even without paintings as style images. Second, we show that learning from paintings as a form of perceptual data augmentation can improve model robustness. Finally, we investigate the invariances learned from stylization and from paintings, and show that models learn different invariances from these differing forms of data. Our results provide in- sights into how stylization improves model robustness, and provide evidence that artist-created paintings can be a valu- able source of data for model robustness. Code and data are available at: https://github.com/hubertsgithub/ style_painting_robustness",
        "authors": [
            "Hubert Lin",
            "Mitchell van Zuijlen",
            "Sylvia C Pont",
            "Kavita Bala"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Lin_What_Can_Style_Transfer_and_Paintings_Do_for_Model_Robustness_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 286,
        "title": "Curriculum Graph Co-Teaching for Multi-Target Domain Adaptation",
        "abstract": "In this paper we address multi-target domain adaptation (MTDA), where given one labeled source dataset and mul- tiple unlabeled target datasets that differ in data distribu- tions, the task is to learn a robust predictor for all the tar- get domains. We identify two key aspects that can help to alleviate multiple domain-shifts in the MTDA: feature ag- gregation and curriculum learning. To this end, we pro- pose Curriculum Graph Co-Teaching (CGCT) that uses a dual classifier head, with one of them being a graph con- volutional network (GCN) which aggregates features from similar samples across the domains. To prevent the clas- sifiers from over-fitting on its own noisy pseudo-labels we develop a co-teaching strategy with the dual classifier head that is assisted by curriculum learning to obtain more re- liable pseudo-labels. Furthermore, when the domain la- bels are available, we propose Domain-aware Curriculum Learning (DCL), a sequential adaptation strategy that first adapts on the easier target domains, followed by the harder ones. We experimentally demonstrate the effectiveness of our proposed frameworks on several benchmarks and ad- vance the state-of-the-art in the MTDA by large margins (e.g. +5.6% on the DomainNet).",
        "authors": [
            "Subhankar Roy",
            "Evgeny Krivosheev",
            "Zhun Zhong",
            "Nicu Sebe",
            "Elisa Ricci"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Roy_Curriculum_Graph_Co-Teaching_for_Multi-Target_Domain_Adaptation_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 287,
        "title": "BoxInst: High-Performance Instance Segmentation with Box Annotations",
        "abstract": "We present a high-performance method that can achieve mask-level instance segmentation with only bounding-box annotations for training. While this setting has been studied in the literature, here we show significantly stronger perfor- mance with a simple design (e.g., dramatically improving previous best reported mask AP of 21.1% [13] to 31.6% on the COCO dataset). Our core idea is to redesign the loss of learning masks in instance segmentation, with no mod- ification to the segmentation network itself. The new loss functions can supervise the mask training without relying on mask annotations. This is made possible with two loss terms, namely, 1) a surrogate term that minimizes the dis- crepancy between the projections of the ground-truth box and the predicted mask; 2) a pairwise loss that can exploit the prior that proximal pixels with similar colors are very likely to have the same category label. Experiments demonstrate that the redesigned mask loss can yield surprisingly high-quality instance masks with only box annotations. For example, without using any mask an- notations, with a ResNet-101 backbone and 3\u00d7 training",
        "authors": [
            "Zhi Tian",
            "Chunhua Shen",
            "Xinlong Wang",
            "Hao Chen",
            "bpc ",
            "bpc ",
            "bpc ",
            "bpc ",
            "bpc ",
            "bpc "
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Tian_BoxInst_High-Performance_Instance_Segmentation_With_Box_Annotations_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 288,
        "title": "Pushing it out of the Way: Interactive Visual Navigation",
        "abstract": "We have observed significant progress in visual naviga- tion for embodied agents. A common assumption in study- ing visual navigation is that the environments are static; this is a limiting assumption. Intelligent navigation may involve interacting with the environment beyond just moving for- ward/backward and turning left/right. Sometimes, the best way to navigate is to push something out of the way. In this paper, we study the problem of interactive navigation where agents learn to change the environment to navigate more ef- ficiently to their goals. To this end, we introduce the Neural Interaction Engine (NIE) to explicitly predict the change in the environment caused by the agent's actions. By model- ing the changes while planning, we find that agents exhibit significant improvements in their navigational capabilities. More specifically, we consider two downstream tasks in the physics-enabled, visually rich, AI2-THOR environment: (1) reaching a target while the path to the target is blocked (2) moving an object to a target location by pushing it. For both tasks, agents equipped with an NIE significantly outperform agents without the understanding of the effect of the ac- tions indicating the benefits of our approach. The code and dataset are available at github.com/KuoHaoZeng/",
        "authors": [
            "Kuo Hao Zeng",
            "Luca Weihs",
            "Ali Farhadi",
            "Roozbeh Mottaghi",
            "bpc ",
            "bpc ",
            "bpc ",
            "bpc ",
            "bpc ",
            "bpc "
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Zeng_Pushing_It_Out_of_the_Way_Interactive_Visual_Navigation_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 289,
        "title": "Towards Compact CNNs via Collaborative Compression",
        "abstract": "Channel pruning and tensor decomposition have re- ceived extensive attention in convolutional neural network compression. However, these two techniques are tradition- ally deployed in an isolated manner, leading to significant accuracy drop when pursuing high compression rates. In this paper, we propose a Collaborative Compression (CC) scheme, which joints channel pruning and tensor decom- position to compress CNN models by simultaneously learn- ing the model sparsity and low-rankness. Specifically, we first investigate the compression sensitivity of each layer in the network, and then propose a Global Compression Rate Optimization that transforms the decision problem of compression rate into an optimization problem. After that, we propose multi-step heuristic compression to remove re- dundant compression units step-by-step, which fully con- siders the effect of the remaining compression space (i.e., unremoved compression units). Our method demonstrates superior performance gains over previous ones on vari- ous datasets and backbone architectures. For example, we achieve 52.9% FLOPs reduction by removing 48.4% pa- rameters on ResNet-50 with only a Top-1 accuracy drop of 0.56% on ImageNet 2012.",
        "authors": [
            "Yuchao Li",
            "Shaohui Lin",
            "Jianzhuang Liu",
            "Qixiang Ye",
            "Mengdi Wang",
            "Fei Chao",
            "Fan Yang",
            "Qi Tian",
            "Rongrong Ji",
            "Alibaba Group"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Li_Towards_Compact_CNNs_via_Collaborative_Compression_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 290,
        "title": "Wide-Depth-Range 6D Object Pose Estimation in Space",
        "abstract": "6D pose estimation in space poses unique challenges that are not commonly encountered in the terrestrial set- ting. One of the most striking differences is the lack of at- mospheric scattering, allowing objects to be visible from a great distance while complicating illumination conditions. Currently available benchmark datasets do not place a suf- ficient emphasis on this aspect and mostly depict the target in close proximity. Prior work tackling pose estimation under large scale variations relies on a two-stage approach to first estimate scale, followed by pose estimation on a resized image patch. We instead propose a single-stage hierarchical end-to-end trainable network that is more robust to scale variations. We demonstrate that it outperforms existing approaches not only on images synthesized to resemble images taken in space but also on standard benchmarks.",
        "authors": [
            "Yinlin Hu ",
            "Wenzel Jakob ",
            "Pascal Fua ",
            "Mathieu Salzmann ",
            " ClearSpace SA"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Hu_Wide-Depth-Range_6D_Object_Pose_Estimation_in_Space_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 291,
        "title": "SliceNet: deep dense depth estimation from a single indoor panorama using a slice-based representation",
        "abstract": "We introduce a novel deep neural network to estimate a depth map from a single monocular indoor panorama. The network directly works on the equirectangular projection, exploiting the properties of indoor 360\u25e6 images. Starting from the fact that gravity plays an important role in the design and construction of man-made indoor scenes, we propose a compact representation of the scene into vertical slices of the sphere, and we exploit long- and short-term relationships among slices to recover the equirectangular depth map. Our design makes it possible to maintain high- resolution information in the extracted features even with a deep network. The experimental results demonstrate that our method outperforms current state-of-the-art solutions in prediction accuracy, particularly for real-world data.",
        "authors": [
            "Giovanni Pintore",
            "Visual Computing",
            "Marco Agus",
            "Eva Almansa",
            "Visual Computing",
            "Jens Schneider",
            "Enrico Gobbetti",
            "Visual Computing"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Pintore_SliceNet_Deep_Dense_Depth_Estimation_From_a_Single_Indoor_Panorama_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 292,
        "title": "Model-Aware Gesture-to-Gesture Translation",
        "abstract": "Hand gesture-to-gesture translation is a significant and interesting problem, which serves as a key role in many ap- plications, such as sign language production. This task in- volves fine-grained structure understanding of the mapping between the source and target gestures. Current works fol- low a data-driven paradigm based on sparse 2D joint rep- resentation. However, given the insufficient representation capability of 2D joints, this paradigm easily leads to blurry generation results with incorrect structure. In this paper, we propose a novel model-aware gesture-to-gesture translation framework, which introduces hand prior with hand meshes as the intermediate representation. To take full advantage of the structured hand model, we first build a dense topology map aligning the image plane with the encoded embedding of the visible hand mesh. Then, a transformation flow is calculated based on the correspondence of the source and target topology map. During the generation stage, we inject the topology information into generation streams by modu- lating the activations in a spatially-adaptive manner. Fur- ther, we incorporate the source local characteristic to en- hance the translated gesture image according to the trans- formation flow. Extensive experiments on two benchmark datasets have demonstrated that our method achieves new state-of-the-art performance.",
        "authors": [
            "Hezhen Hu ",
            "Weilun Wang ",
            "Wengang Zhou ",
            "Weichao Zhao ",
            "Houqiang Li "
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Hu_Model-Aware_Gesture-to-Gesture_Translation_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 293,
        "title": "Learning Graph Embeddings for Compositional Zero-shot Learning",
        "abstract": "In compositional zero-shot learning, the goal is to recog- nize unseen compositions (e.g. old dog) of observed visual primitives states (e.g. old, cute) and objects (e.g. car, dog) in the training set. This is challenging because the same state can for example alter the visual appearance of a dog drastically differently from a car. As a solution, we propose a novel graph formulation called Compositional Graph Em- bedding (CGE) that learns image features, compositional classifiers and latent representations of visual primitives in an end-to-end manner. The key to our approach is exploit- ing the dependency between states, objects and their com- positions within a graph structure to enforce the relevant knowledge transfer from seen to unseen compositions. By learning a joint compatibility that encodes semantics be- tween concepts, our model allows for generalization to un- seen compositions without relying on an external knowledge base like WordNet. We show that in the challenging gen- eralized compositional zero-shot setting our CGE signifi- cantly outperforms the state of the art on MIT-States and UT-Zappos. We also propose a new benchmark for this task based on the recent GQA dataset. Code is available at: https://github.com/ExplainableML/czsl",
        "authors": [
            "Muhammad Ferjad Naeem",
            "Yongqin Xian",
            "Federico Tombari",
            "Zeynep Akata",
            "MPI for Informatics"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Naeem_Learning_Graph_Embeddings_for_Compositional_Zero-Shot_Learning_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 294,
        "title": "On Learning the Geodesic Path for Incremental Learning",
        "abstract": "Neural networks notoriously suffer from the problem of catastrophic forgetting, the phenomenon of forgetting the past knowledge when acquiring new knowledge. Overcom- ing catastrophic forgetting is of significant importance to em- ulate the process of \"incremental learning\", where the model is capable of learning from sequential experience in an effi- cient and robust way. State-of-the-art techniques for incre- mental learning make use of knowledge distillation towards preventing catastrophic forgetting. Therein, one updates the network while ensuring that the network's responses to previ- ously seen concepts remain stable throughout updates. This in practice is done by minimizing the dissimilarity between current and previous responses of the network one way or another. Our work contributes a novel method to the arsenal of distillation techniques. In contrast to the previous state of the art, we propose to firstly construct low-dimensional manifolds for previous and current responses and minimize the dissimilarity between the responses along the geodesic connecting the manifolds. This induces a more formidable knowledge distillation with smooth properties which pre- serves the past knowledge more efficiently as observed by our comprehensive empirical study. 1",
        "authors": [
            "Christian Simon",
            "Piotr Koniusz",
            "Data CSIRO"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Simon_On_Learning_the_Geodesic_Path_for_Incremental_Learning_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 295,
        "title": "Robust Representation Learning with Feedback for Single Image Deraining",
        "abstract": "A deraining network can be interpreted as a conditional generator that aims at removing rain streaks from image. Most existing image deraining methods ignore model er- rors caused by uncertainty that reduces embedding qual- ity. Unlike existing image deraining methods that embed low-quality features into the model directly, we replace low- quality features by latent high-quality features. The spirit of closed-loop feedback in the automatic control field is bor- rowed to obtain latent high-quality features. A new method for error detection and feature compensation is proposed to address model errors. Extensive experiments on bench- mark datasets as well as specific real datasets demonstrate that the proposed method outperforms recent state-of-the- art methods. Code is available at: https://github.com/LI-Hao-SJTU/DerainRLNet 1. Introduction",
        "authors": [],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Chen_Robust_Representation_Learning_With_Feedback_for_Single_Image_Deraining_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 296,
        "title": "Automatic Vertebra Localization and Identification in CT by Spine Rectification and Anatomically-constrained Optimization",
        "abstract": "Accurate vertebra localization and identification are re- quired in many clinical applications of spine disorder di- agnosis and surgery planning. However, significant chal- lenges are posed in this task by highly varying patholo- gies (such as vertebral compression fracture, scoliosis, and vertebral fixation) and imaging conditions (such as limited field of view and metal streak artifacts). This paper pro- poses a robust and accurate method that effectively exploits the anatomical knowledge of the spine to facilitate verte- bra localization and identification. A key point localization model is trained to produce activation maps of vertebra cen- ters. They are then re-sampled along the spine centerline to produce spine-rectified activation maps, which are fur- ther aggregated into 1-D activation signals. Following this, an anatomically-constrained optimization module is intro- duced to jointly search for the optimal vertebra centers un- der a soft constraint that regulates the distance between ver- tebrae and a hard constraint on the consecutive vertebra in- dices. When being evaluated on a major public benchmark of 302 highly pathological CT images, the proposed method reports the state of the art identification (id.) rate of 97.4%, and outperforms the best competing method of 94.7% id. rate by reducing the relative id. error rate by half.",
        "authors": [
            "Fakai Wang",
            "Kang Zheng",
            "Le Lu",
            "Jing Xiao",
            "Min Wu",
            "Shun Miao"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Wang_Automatic_Vertebra_Localization_and_Identification_in_CT_by_Spine_Rectification_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 297,
        "title": "CoCoNets: Continuous Contrastive 3D Scene Representations",
        "abstract": "This paper explores self-supervised learning of amodal 3D feature representations from RGB and RGB-D posed images and videos, agnostic to object and scene semantic content, and evaluates the resulting scene representations in the downstream tasks of visual correspondence, object tracking, and object detection. The model infers a latent 3D representation of the scene in the form of 3D feature points, where each continuous world 3D point is mapped to its corresponding feature vector. The model is trained for contrastive view prediction by rendering 3D feature clouds in queried viewpoints and matching against the 3D feature point cloud predicted from the query view. Notably, the rep- resentation can be queried for any 3D location, even if it is not visible from the input view. Our model brings together three powerful ideas of recent exciting research work: 3D feature grids as a neural bottleneck for view prediction, im- plicit functions for handling resolution limitations of 3D grids, and contrastive learning for unsupervised training of feature representations. We show the resulting 3D vi- sual feature representations effectively scale across objects and scenes, imagine information occluded or missing from the input viewpoints, track objects over time, align seman- tically related objects in 3D, and improve 3D object detec- tion. We outperform many existing state-of-the-art methods for 3D feature learning and view prediction, which are ei- ther limited by 3D grid spatial resolution, do not attempt to build amodal 3D representations, or do not handle com- binatorial scene variability due to their non-convolutional bottlenecks.",
        "authors": [
            "Shamit Lal",
            "Mihir Prabhudesai",
            "Ishita Mediratta",
            "Adam W Harley",
            "Katerina Fragkiadaki"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Lal_CoCoNets_Continuous_Contrastive_3D_Scene_Representations_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 298,
        "title": "Causal Attention for Vision-Language Tasks",
        "abstract": "We present a novel attention mechanism: Causal Atten- tion (CATT), to remove the ever-elusive confounding effect in existing attention-based vision-language models. This ef- fect causes harmful bias that misleads the attention mod- ule to focus on the spurious correlations in training data, damaging the model generalization. As the confounder is unobserved in general, we use the front-door adjustment to realize the causal intervention, which does not require any knowledge on the confounder. Specifically, CATT is implemented as a combination of 1) In-Sample Attention (IS-ATT) and 2) Cross-Sample Attention (CS-ATT), where the latter forcibly brings other samples into every IS-ATT, mimicking the causal intervention. CATT abides by the Q- K-V convention and hence can replace any attention mod- ule such as top-down attention and self-attention in Trans- formers. CATT improves various popular attention-based vision-language models by considerable margins. In partic- ular, we show that CATT has great potential in large-scale pre-training, e.g., it can promote the lighter LXMERT [57], which uses fewer data and less computational power, com- parable to the heavier UNITER [14]. Code is published in https://github.com/yangxuntu/lxmertcatt.",
        "authors": [
            "Xu Yang",
            "Hanwang Zhang",
            "Guojun Qi",
            "Jianfei Cai"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Yang_Causal_Attention_for_Vision-Language_Tasks_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 299,
        "title": "We are More than Our Joints: Predicting how 3D Bodies Move",
        "abstract": "A key step towards understanding human behavior is the prediction of 3D human motion. Successful solutions have many applications in human tracking, HCI, and graphics. Most previous work focuses on predicting a time series of future 3D joint locations given a sequence 3D joints from the past. This Euclidean formulation generally works bet- ter than predicting pose in terms of joint rotations. Body joint locations, however, do not fully constrain 3D human pose, leaving degrees of freedom (like rotation about a limb) undefined. Note that 3D joints can be viewed as a sparse point cloud. Thus the problem of human motion prediction can be seen as a problem of point cloud pre- diction. With this observation, we instead predict a sparse set of locations on the body surface that correspond to mo- tion capture markers. Given such markers, we fit a para- metric body model to recover the 3D body of the person. These sparse surface markers also carry detailed informa- tion about human movement that is not present in the joints, increasing the naturalness of the predicted motions. Us- ing the AMASS dataset, we train MOJO (More than Our JOints), which is a novel variational autoencoder with a la-",
        "authors": [
            "Yan Zhang",
            "Michael J Black",
            "Siyu Tang",
            "T ubingen",
            "bpc "
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Zhang_We_Are_More_Than_Our_Joints_Predicting_How_3D_Bodies_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 300,
        "title": "Distilling Object Detectors via Decoupled Features",
        "abstract": "Knowledge distillation is a widely used paradigm for in- heriting information from a complicated teacher network to a compact student network and maintaining the strong per- formance. Different from image classification, object detec- tors are much more sophisticated with multiple loss func- tions in which features that semantic information rely on are tangled. In this paper, we point out that the information of features derived from regions excluding objects are also essential for distilling the student detector, which is usu- ally ignored in existing approaches. In addition, we eluci- date that features from different regions should be assigned with different importance during distillation. To this end, we present a novel distillation algorithm via decoupled fea- tures (DeFeat) for learning a better student detector. Specif- ically, two levels of decoupled features will be processed for embedding useful information into the student, i.e., decou- pled features from neck and decoupled proposals from clas- sification head. Extensive experiments on various detectors with different backbones show that the proposed DeFeat is able to surpass the state-of-the-art distillation methods for object detection. For example, DeFeat improves ResNet50 based Faster R-CNN from 37.4% to 40.9% mAP, and im- proves ResNet50 based RetinaNet from 36.5% to 39.7% mAP on COCO benchmark. Code will be released1,2.",
        "authors": [
            "Jianyuan Guo",
            "Kai Han",
            "Yunhe Wang",
            "Han Wu",
            "Xinghao Chen",
            "Chunjing Xu",
            "Chang Xu"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Guo_Distilling_Object_Detectors_via_Decoupled_Features_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 301,
        "title": "Im2Vec: Synthesizing Vector Graphics without Vector Supervision",
        "abstract": "Vector graphics are widely used to represent fonts, lo- gos, digital artworks, and graphic designs. But, while a vast body of work has focused on generative algorithms for raster images, only a handful of options exists for vec- tor graphics. One can always rasterize the input graphic and resort to image-based generative approaches, but this negates the advantages of the vector representation. The current alternative is to use specialized models that require explicit supervision on the vector graphics representation at training time. This is not ideal because large-scale high- quality vector-graphics datasets are difficult to obtain. Fur- thermore, the vector representation for a given design is not unique, so models that supervise on the vector repre- sentation are unnecessarily constrained. Instead, we pro- pose a new neural network that can generate complex vec- tor graphics with varying topologies, and only requires in- direct supervision from readily-available raster training im- ages (i.e., with no vector counterparts). To enable this, we use a differentiable rasterization pipeline that renders the generated vector shapes and composites them together onto a raster canvas. We demonstrate our method on a range of datasets, and provide comparison with state-of-the-art SVG-VAE and DeepSVG, both of which require explicit vec- tor graphics supervision. Finally, we also demonstrate our approach on the MNIST dataset, for which no groundtruth vector representation is available. Source code, datasets and more results are available at http://geometry. cs.ucl.ac.uk/projects/2021/Im2Vec/.",
        "authors": [
            "Pradyumna Reddy",
            "Micha el Gharbi",
            "Niloy J Mitra"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Reddy_Im2Vec_Synthesizing_Vector_Graphics_Without_Vector_Supervision_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 302,
        "title": "Differentiable Multi-Granularity Human Representation Learning for Instance-Aware Human Semantic Parsing",
        "abstract": "To address the challenging task of instance-aware hu- man part parsing, a new bottom-up regime is proposed to learn category-level human semantic segmentation as well as multi-person pose estimation in a joint and end-to-end manner. It is a compact, efficient and powerful framework that exploits structural information over different human granularities and eases the difficulty of person partition- ing. Specifically, a dense-to-sparse projection field, which allows explicitly associating dense human semantics with sparse keypoints, is learnt and progressively improved over the network feature pyramid for robustness. Then, the dif- ficult pixel grouping problem is cast as an easier, multi- person joint assembling task. By formulating joint associa- tion as maximum-weight bipartite matching, a differentiable solution is developed to exploit projected gradient descent and Dykstra's cyclic projection algorithm. This makes our method end-to-end trainable and allows back-propagating the grouping error to directly supervise multi-granularity human representation learning. This is distinguished from current bottom-up human parsers or pose estimators which require sophisticated post-processing or heuristic greedy algorithms. Experiments on three instance-aware human parsing datasets show that our model outperforms other bottom-up alternatives with much more efficient inference.",
        "authors": [
            "Tianfei Zhou",
            "Wenguan Wang",
            "Si Liu",
            "Yi Yang",
            "Luc Van Gool"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Zhou_Differentiable_Multi-Granularity_Human_Representation_Learning_for_Instance-Aware_Human_Semantic_Parsing_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 303,
        "title": "RSN: Range Sparse Net for Efficient, Accurate LiDAR 3D Object Detection",
        "abstract": "The detection of 3D objects from LiDAR data is a criti- cal component in most autonomous driving systems. Safe, high speed driving needs larger detection ranges, which are enabled by new LiDARs. These larger detection ranges re- quire more efficient and accurate detection models. Towards this goal, we propose Range Sparse Net (RSN) \u2013 a sim- ple, efficient, and accurate 3D object detector \u2013 in order to tackle real time 3D object detection in this extended detection regime. RSN predicts foreground points from range images and applies sparse convolutions on the selected foreground points to detect objects. The lightweight 2D convolutions on dense range images results in significantly fewer selected foreground points, thus enabling the later sparse convolu- tions in RSN to efficiently operate. Combining features from the range image further enhance detection accuracy. RSN runs at more than 60 frames per second on a 150m \u00d7 150m detection region on Waymo Open Dataset (WOD) while be- ing more accurate than previously published detectors. As of 11/2020, RSN is ranked first in the WOD leaderboard based on the APH/LEVEL 1 metrics for LiDAR-based pedestrian and vehicle detection, while being several times faster than alternatives.",
        "authors": [
            "Pei Sun",
            "Weiyue Wang",
            "Yuning Chai",
            "Gamaleldin Elsayed",
            "Alex Bewley",
            "Xiao Zhang",
            "Dragomir Anguelov",
            "Waymo LLC"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Sun_RSN_Range_Sparse_Net_for_Efficient_Accurate_LiDAR_3D_Object_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 304,
        "title": "Memory-Efficient Network for Large-scale Video Compressive Sensing",
        "abstract": "Video snapshot compressive imaging (SCI) captures a se- quence of video frames in a single shot using a 2D detector. The underlying principle is that during one exposure time, different masks are imposed on the high-speed scene to form a compressed measurement. With the knowledge of masks, optimization algorithms or deep learning methods are employed to reconstruct the desired high-speed video frames from this snapshot measurement. Unfortunately, though these methods can achieve decent results, the long running time of optimization algorithms or huge training memory occupation of deep networks still pre- clude them in practical applications. In this paper, we develop a memory-efficient network for large-scale video SCI based on multi-group reversible 3D convolutional neural networks. In addition to the basic model for the grayscale SCI system, we take one step further to combine demosaicing and SCI reconstruction to directly recover color video from Bayer measurements. Extensive results on both simulation and real data captured by SCI cam- eras demonstrate that our proposed model outperforms previous state-of-the-art with less memory and thus can be used in large-scale problems. The code is at https: //github.com/BoChenGroup/RevSCI-net.",
        "authors": [
            "Ziheng Cheng",
            "Bo Chen",
            "Guanliang Liu",
            "Hao Zhang",
            "Xin Yuan"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Cheng_Memory-Efficient_Network_for_Large-Scale_Video_Compressive_Sensing_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 305,
        "title": "Learning a Facial Expression Embedding Disentangled from Identity",
        "abstract": "The facial expression analysis requires a compact and identity-ignored expression representation. In this paper, we model the expression as the deviation from the iden- tity by a subtraction operation, extracting a continuous and identity-invariant expression embedding. We propose a De- viation Learning Network (DLN) with a pseudo-siamese structure to extract the deviation feature vector. To reduce the optimization difficulty caused by additional fully con- nection layers, DLN directly provides high-order polyno- mial to nonlinearly project the high-dimensional feature to a low-dimensional manifold. Taking label noise into ac- count, we add a crowd layer to DLN for robust embedding extraction. Also, to achieve a more compact representa- tion, we use hierarchical annotation for data augmenta- tion. We evaluate our facial expression embedding on the FEC validation set. The quantitative results prove that we achieve the state-of-the-art, both in terms of fine-grained and identity-invariant property. We further conduct exten- sive experiments to show that our expression embedding is of high quality for expression recognition, image retrieval, and face manipulation.",
        "authors": [
            "Wei Zhang",
            "Xianpeng Ji",
            "Keyu Chen",
            "Yu Ding",
            "Changjie Fan",
            "Virtual Human Group"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Zhang_Learning_a_Facial_Expression_Embedding_Disentangled_From_Identity_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 306,
        "title": "MoViNets: Mobile Video Networks for Efficient Video Recognition",
        "abstract": "We present Mobile Video Networks (MoViNets), a fam- ily of computation and memory efficient video networks that can operate on streaming video for online inference. 3D convolutional neural networks (CNNs) are accurate at video recognition but require large computation and mem- ory budgets and do not support online inference, making them difficult to work on mobile devices. We propose a three-step approach to improve computational efficiency while substantially reducing the peak memory usage of 3D CNNs. First, we design a video network search space and employ neural architecture search to generate efficient and diverse 3D CNN architectures. Second, we introduce the Stream Buffer technique that decouples memory from video clip duration, allowing 3D CNNs to embed arbitrary-length streaming video sequences for both training and inference with a small constant memory footprint. Third, we pro- pose a simple ensembling technique to improve accuracy further without sacrificing efficiency. These three progres- sive techniques allow MoViNets to achieve state-of-the-art accuracy and efficiency on the Kinetics, Moments in Time, and Charades video action recognition datasets. For in- stance, MoViNet-A5-Stream achieves the same accuracy as X3D-XL on Kinetics 600 while requiring 80% fewer FLOPs and 65% less memory. Code is available at https: //github.com/google-research/movinet.",
        "authors": [
            "Liangzhe Yuan",
            "Li Zhang",
            "Mingxing Tan",
            "Matthew Brown",
            "Boqing Gong"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Kondratyuk_MoViNets_Mobile_Video_Networks_for_Efficient_Video_Recognition_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 307,
        "title": "Class-Aware Robust Adversarial Training for Object Detection",
        "abstract": "Object detection is an important computer vision task with plenty of real-world applications; therefore, how to enhance its robustness against adversarial attacks has emerged as a crucial issue. However, most of the pre- vious defense methods focused on the classification task and had few analysis in the context of the object detection task. In this work, to address the issue, we present a novel class-aware robust adversarial training paradigm for the object detection task. For a given image, the proposed ap- proach generates an universal adversarial perturbation to simultaneously attack all the occurred objects in the image through jointly maximizing the respective loss for each ob- ject. Meanwhile, instead of normalizing the total loss with the number of objects, the proposed approach decomposes the total loss into class-wise losses and normalizes each class loss using the number of objects for the class. The adversarial training based on the class weighted loss can not only balances the influence of each class but also ef- fectively and evenly improves the adversarial robustness of trained models for all the object classes as compared with the previous defense methods. Furthermore, with the re- cent development of fast adversarial training, we provide a fast version of the proposed algorithm which can be trained faster than the traditional adversarial training while keep- ing comparable performance. With extensive experiments on the challenging PASCAL-VOC and MS-COCO datasets, the evaluation results demonstrate that the proposed de- fense methods can effectively enhance the robustness of the object detection models.",
        "authors": [
            "Pin Chun Chen",
            "Bo Han Kung",
            "Jun Cheng Chen",
            "Academia Sinica"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Chen_Class-Aware_Robust_Adversarial_Training_for_Object_Detection_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 308,
        "title": "Behavior-Driven Synthesis of Human Dynamics",
        "abstract": "Generating and representing human behavior are of ma- jor importance for various computer vision applications. Commonly, human video synthesis represents behavior as sequences of postures while directly predicting their likely progressions or merely changing the appearance of the de- picted persons, thus not being able to exercise control over their actual behavior during the synthesis process. In con- trast, controlled behavior synthesis and transfer across indi- viduals requires a deep understanding of body dynamics and calls for a representation of behavior that is independent of appearance and also of specific postures. In this work, we present a model for human behavior synthesis which learns a dedicated representation of human dynamics inde- pendent of postures. Using this representation, we are able to change the behavior of a person depicted in an arbitrary posture, or to even directly transfer behavior observed in a given video sequence. To this end, we propose a condi- tional variational framework which explicitly disentangles posture from behavior. We demonstrate the effectiveness of our approach on this novel task, evaluating capturing, trans- ferring, and sampling fine-grained, diverse behavior, both quantitatively and qualitatively. Project page is available at https://cutt.ly/5l7rXEp",
        "authors": [
            "Timo Milbich",
            "Michael Dorkenwald",
            "Bj orn Ommer"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Blattmann_Behavior-Driven_Synthesis_of_Human_Dynamics_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 309,
        "title": "Towards Efficient Tensor Decomposition-Based DNN Model Compression with Optimization Framework",
        "abstract": "Advanced tensor decomposition, such as tensor train (TT) and tensor ring (TR), has been widely studied for deep neural network (DNN) model compression, especially for recurrent neural networks (RNNs). However, compressing convolutional neural networks (CNNs) using TT/TR always suffers significant accuracy loss. In this paper, we propose a systematic framework for tensor decomposition-based model compression using Alternating Direction Method of Multipliers (ADMM). By formulating TT decomposition- based model compression to an optimization problem with constraints on tensor ranks, we leverage ADMM technique to systemically solve this optimization problem in an itera- tive way. During this procedure, the entire DNN model is trained in the original structure instead of TT format, but gradually enjoys the desired low tensor rank characteris- tics. We then decompose this uncompressed model to TT format, and fine-tune it to finally obtain a high-accuracy TT- format DNN model. Our framework is very general, and it works for both CNNs and RNNs, and can be easily modified to fit other tensor decomposition approaches. We evaluate our proposed framework on different DNN models for image classification and video recognition tasks. Experimental re- sults show that our ADMM-based TT-format models demon- strate very high compression performance with high accu- racy. Notably, on CIFAR-100, with 2.3\u02c6 and 2.4\u02c6 com- pression ratios, our models have 1.96% and 2.21% higher top-1 accuracy than the original ResNet-20 and ResNet-32, respectively. For compressing ResNet-18 on ImageNet, our model achieves 2.47\u02c6 FLOPs reduction without accuracy loss.",
        "authors": [
            "Miao Yin",
            "Yang Sui"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Yin_Towards_Efficient_Tensor_Decomposition-Based_DNN_Model_Compression_With_Optimization_Framework_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 310,
        "title": "Attention-guided Image Compression by Deep Reconstruction of Compressive Sensed Saliency Skeleton",
        "abstract": "We propose a deep learning system for attention-guided dual-layer image compression (AGDL). In the AGDL com- pression system, an image is encoded into two layers, a base layer and an attention-guided refinement layer. Unlike the existing ROI image compression methods that spend an ex- tra bit budget equally on all pixels in ROI, AGDL employs a CNN module to predict those pixels on and near a saliency sketch within ROI that are critical to perceptual quality. Only the critical pixels are further sampled by compressive sensing (CS) to form a very compact refinement layer. An- other novel CNN method is developed to jointly decode the two compression layers for a much refined reconstruction, while strictly satisfying the transmitted CS constraints on perceptually critical pixels. Extensive experiments demon- strate that the proposed AGDL system advances the state of the art in perception-aware image compression.",
        "authors": [
            "Xi Zhang",
            "Xiaolin Wu",
            "McMaster Univeristy"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Zhang_Attention-Guided_Image_Compression_by_Deep_Reconstruction_of_Compressive_Sensed_Saliency_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 311,
        "title": "Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts",
        "abstract": "The rapid progress in 3D scene understanding has come with growing demand for data; however, collecting and an- notating 3D scenes (e.g. point clouds) are notoriously hard. For example, the number of scenes (e.g. indoor rooms) that can be accessed and scanned might be limited; even given sufficient data, acquiring 3D labels (e.g. instance masks) requires intensive human labor. In this paper, we explore data-efficient learning for 3D point cloud. As a first step towards this direction, we propose Contrastive Scene Contexts, a 3D pre-training method that makes use of both point-level correspondences and spatial contexts in a scene. Our method achieves state-of-the-art results on a suite of benchmarks where training data or labels are scarce. Our study reveals that exhaustive labelling of 3D point clouds might be unnecessary; and remarkably, on ScanNet, even using 0.1% of point labels, we still achieve 89% (instance segmentation) and 96% (semantic segmenta- tion) of the baseline performance that uses full annotations.",
        "authors": [
            "Ji Hou",
            "Benjamin Graham",
            "Matthias Nie ner",
            "Saining Xie",
            "Training Data",
            "with Limited Annotations",
            "Instance Segmentation",
            "bpc ",
            "bpc ",
            "bpc "
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Hou_Exploring_Data-Efficient_3D_Scene_Understanding_With_Contrastive_Scene_Contexts_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 312,
        "title": "Towards Improving the Consistency, Efficiency, and Flexibility of Differentiable Neural Architecture Search",
        "abstract": "Most differentiable neural architecture search methods construct a super-net for search and derive a target-net as its sub-graph for evaluation. There exists a significant gap between the architectures in search and evaluation. As a result, current methods suffer from an inconsistent, ineffi- cient, and inflexible search process. In this paper, we in- troduce EnTranNAS that is composed of Engine-cells and Transit-cells. The Engine-cell is differentiable for architec- ture search, while the Transit-cell only transits a sub-graph by architecture derivation. Consequently, the gap between the architectures in search and evaluation is significantly reduced. Our method also spares much memory and com- putation cost, which speeds up the search process. A feature sharing strategy is introduced for more balanced optimiza- tion and more efficient search. Furthermore, we develop an architecture derivation method to replace the traditional one that is based on a hand-crafted rule. Our method en- ables differentiable sparsification, and keeps the derived ar- chitecture equivalent to that of Engine-cell, which further improves the consistency between search and evaluation. More importantly, it supports the search for topology where a node can be connected to prior nodes with any number of connections, so that the searched architectures could be more flexible. Our search on CIFAR-10 has an error rate of 2.22% with only 0.07 GPU-day. We can also directly per- form the search on ImageNet with topology learnable and achieve a top-1 error rate of 23.8% in 2.1 GPU-day.",
        "authors": [
            "Yibo Yang",
            "Shan You",
            "Hongyang Li",
            "Fei Wang",
            "Chen Qian",
            "Zhouchen Lin"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Yang_Towards_Improving_the_Consistency_Efficiency_and_Flexibility_of_Differentiable_Neural_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 313,
        "title": "Graph Attention Tracking",
        "abstract": "Siamese network based trackers formulate the visual tracking task as a similarity matching problem. Almost all popular Siamese trackers realize the similarity learning via convolutional feature cross-correlation between a tar- get branch and a search branch. However, since the size of target feature region needs to be pre-fixed, these cross- correlation base methods suffer from either reserving much adverse background information or missing a great deal of foreground information. Moreover, the global matching be- tween the target and search region also largely neglects the target structure and part-level information. In this paper, to solve the above issues, we propose a simple target-aware Siamese graph attention network for general object tracking. We propose to establish part-to- part correspondence between the target and the search re- gion with a complete bipartite graph, and apply the graph attention mechanism to propagate target information from the template feature to the search feature. Further, instead of using the pre-fixed region cropping for template-feature- area selection, we investigate a target-aware area selection mechanism to fit the size and aspect ratio variations of dif- ferent objects. Experiments on challenging benchmarks in- cluding GOT-10k, UAV123, OTB-100 and LaSOT demon- strate that the proposed SiamGAT outperforms many state- of-the-art trackers and achieves leading performance. Code is available at: https: // git. io/ SiamGAT",
        "authors": [
            "Dongyan Guo",
            "Yanyan Shao",
            "Ying Cui",
            "Zhenhua Wang",
            "Liyan Zhang",
            "Chunhua Shen"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Guo_Graph_Attention_Tracking_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 314,
        "title": "Neural Cellular Automata Manifold",
        "abstract": "Very recently, the Neural Cellular Automata (NCA) has been proposed to simulate the morphogenesis process with deep networks. NCA learns to grow an image starting from a fixed single pixel. In this work, we show that the neural network (NN) architecture of the NCA can be encapsulated in a larger NN. This allows us to propose a new model that encodes a manifold of NCA, each of them capable of generating a distinct image. Therefore, we are effectively learning an embedding space of CA, which shows gener- alization capabilities. We accomplish this by introducing dynamic convolutions inside an Auto-Encoder architecture, for the first time used to join two different sources of infor- mation, the encoding and cell's environment information. In biological terms, our approach would play the role of the transcription factors, modulating the mapping of genes into specific proteins that drive cellular differentiation, which occurs right before the morphogenesis. We thoroughly evalu- ate our approach in a dataset of synthetic emojis and also in real images of CIFAR-10. Our model introduces a general- purpose network, which can be used in a broad range of problems beyond image generation.",
        "authors": [
            "Francesc Moreno Noguer",
            "CSIC UPC",
            "Barcelona Supercomputing Center"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Hernandez_Neural_Cellular_Automata_Manifold_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 315,
        "title": "pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis",
        "abstract": "We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches how- ever fall short in two ways: first, they may lack an under- lying 3D representation or rely on view-inconsistent ren- dering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Ad- versarial Networks (\u03c0-GAN or pi-GAN), for high-quality 3D-aware image synthesis. \u03c0-GAN leverages neural repre- sentations with periodic activation functions and volumetric rendering to represent scenes as view-consistent radiance fields. The proposed approach obtains state-of-the-art re- sults for 3D-aware image synthesis with multiple real and synthetic datasets.",
        "authors": [
            "Jiajun Wu",
            "Gordon Wetzstein"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Chan_Pi-GAN_Periodic_Implicit_Generative_Adversarial_Networks_for_3D-Aware_Image_Synthesis_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 316,
        "title": "Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition",
        "abstract": "Linguistic knowledge is of great benefit to scene text recognition. However, how to effectively model linguistic rules in end-to-end deep networks remains a research chal- lenge. In this paper, we argue that the limited capacity of language models comes from: 1) implicitly language modeling; 2) unidirectional feature representation; and 3) language model with noise input. Correspondingly, we pro- pose an autonomous, bidirectional and iterative ABINet for scene text recognition. Firstly, the autonomous suggests to block gradient flow between vision and language models to enforce explicitly language modeling. Secondly, a novel bidirectional cloze network (BCN) as the language model is proposed based on bidirectional feature representation. Thirdly, we propose an execution manner of iterative correc- tion for language model which can effectively alleviate the impact of noise input. Additionally, based on the ensemble of iterative predictions, we propose a self-training method which can learn from unlabeled images effectively. Extensive experiments indicate that ABINet has superiority on low- quality images and achieves state-of-the-art results on sev- eral mainstream benchmarks. Besides, the ABINet trained with ensemble self-training shows promising improvement in realizing human-level recognition. Code is available at https://github.com/FangShancheng/ABINet.",
        "authors": [
            "Shancheng Fang",
            "Hongtao Xie",
            "Yuxin Wang",
            "Zhendong Mao",
            "Yongdong Zhang"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Fang_Read_Like_Humans_Autonomous_Bidirectional_and_Iterative_Language_Modeling_for_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 317,
        "title": "Joint Negative and Positive Learning for Noisy Labels",
        "abstract": "Training of Convolutional Neural Networks (CNNs) with data with noisy labels is known to be a challenge. Based on the fact that directly providing the label to the data (Positive Learning; PL) has a risk of allowing CNNs to memorize the contaminated labels for the case of noisy data, the indirect learning approach that uses complementary labels (Nega- tive Learning for Noisy Labels; NLNL) has proven to be highly effective in preventing overfitting to noisy data as it reduces the risk of providing faulty target. NLNL further employs a three-stage pipeline to improve convergence. As a result, filtering noisy data through the NLNL pipeline is cumbersome, increasing the training cost. In this study, we propose a novel improvement of NLNL, named Joint Neg- ative and Positive Learning (JNPL), that unifies the filter- ing pipeline into a single stage. JNPL trains CNN via two losses, NL+ and PL+, which are improved upon NL and PL loss functions, respectively. We analyze the fundamental issue of NL loss function and develop new NL+ loss func- tion producing gradient that enhances the convergence of noisy data. Furthermore, PL+ loss function is designed to enable faster convergence to expected-to-be-clean data. We show that the NL+ and PL+ train CNN simultaneously, significantly simplifying the pipeline, allowing greater ease of practical use compared to NLNL. With a simple semi- supervised training technique, our method achieves state- of-the-art accuracy for noisy data classification based on the superior filtering ability.",
        "authors": [
            "Youngdong Kim",
            "Juseung Yun",
            "Junmo Kim"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Kim_Joint_Negative_and_Positive_Learning_for_Noisy_Labels_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 318,
        "title": "Knowledge Evolution in Neural Networks",
        "abstract": "Deep learning relies on the availability of a large cor- pus of data (labeled or unlabeled). Thus, one challenging unsettled question is: how to train a deep network on a rela- tively small dataset? To tackle this question, we propose an evolution-inspired training approach to boost performance on relatively small datasets. The knowledge evolution (KE) approach splits a deep network into two hypotheses: the fit- hypothesis and the reset-hypothesis. We iteratively evolve the knowledge inside the fit-hypothesis by perturbing the reset-hypothesis for multiple generations. This approach not only boosts performance, but also learns a slim network with a smaller inference cost. KE integrates seamlessly with both vanilla and residual convolutional networks. KE re- duces both overfitting and the burden for data collection. We evaluate KE on various network architectures and loss functions. We evaluate KE using relatively small datasets (e.g., CUB-200) and randomly initialized deep net- works. KE achieves an absolute 21% improvement margin on a state-of-the-art baseline. This performance improve- ment is accompanied by a relative 73% reduction in infer- ence cost. KE achieves state-of-the-art results on classifi- cation and metric learning benchmarks. Code available at http://bit.ly/3uLgwYb",
        "authors": [
            "Ahmed Taha",
            "Abhinav Shrivastava",
            "Larry Davis"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Taha_Knowledge_Evolution_in_Neural_Networks_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 319,
        "title": "Gradient-based Algorithms for Machine Teaching",
        "abstract": "The problem of machine teaching is considered. A new formulation is proposed under the assumption of an optimal student, where optimality is defined in the usual machine learning sense of empirical risk minimization. This is a sensi- ble assumption for machine learning students and for human students in crowdsourcing platforms, who tend to perform at least as well as machine learning systems. It is shown that, if allowed unbounded effort, the optimal student always learns the optimal predictor for a classification task. Hence, the role of the optimal teacher is to select the teaching set that minimizes student effort. This is formulated as a problem of functional optimization where, at each teaching iteration, the teacher seeks to align the steepest descent directions of the risk of (1) the teaching set and (2) entire example population. The optimal teacher, denoted MaxGrad, is then shown to maximize the gradient of the risk on the set of new examples selected per iteration. MaxGrad teaching algorithms are finally provided for both binary and multiclass tasks, and shown to have some similarities with boosting algorithms. Experimental evaluations demonstrate the effectiveness of MaxGrad, which outperforms previous algorithms on the classification task, for both machine learning and human students from MTurk, by a substantial margin.",
        "authors": [
            "Pei Wang",
            "Kabir Nagrecha",
            "Nuno Vasconcelos"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Wang_Gradient-Based_Algorithms_for_Machine_Teaching_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 320,
        "title": "LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from Video using Pose and Lighting Normalization",
        "abstract": "In this paper, we present a video-based learning frame- work for animating personalized 3D talking faces from au- dio. We introduce two training-time data normalizations that significantly improve data sample efficiency. First, we isolate and represent faces in a normalized space that de- couples 3D geometry, head pose, and texture. This decom- poses the prediction problem into regressions over the 3D face shape and the corresponding 2D texture atlas. Second, we leverage facial symmetry and approximate albedo con- stancy of skin to isolate and remove spatio-temporal light- ing variations. Together, these normalizations allow sim- ple networks to generate high fidelity lip-sync videos under novel ambient illumination while training with just a single speaker-specific video. Further, to stabilize temporal dy- namics, we introduce an auto-regressive approach that con- ditions the model on its previous visual state. Human rat- ings and objective metrics demonstrate that our method out- performs contemporary state-of-the-art audio-driven video reenactment benchmarks in terms of realism, lip-sync and visual quality scores. We illustrate several applications en- abled by our framework.",
        "authors": [
            "Avisek Lahiri",
            "Vivek Kwatra",
            "Christian Frueh",
            "John Lewis",
            "Chris Bregler"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Lahiri_LipSync3D_Data-Efficient_Learning_of_Personalized_3D_Talking_Faces_From_Video_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 321,
        "title": "M3DSSD: Monocular 3D Single Stage Object Detector",
        "abstract": "In this paper, we propose a Monocular 3D Single Stage object Detector (M3DSSD) with feature alignment and asymmetric non-local attention. Current anchor-based monocular 3D object detection methods suffer from fea- ture mismatching. To overcome this, we propose a two- step feature alignment approach. In the first step, the shape alignment is performed to enable the receptive field of the feature map to focus on the pre-defined anchors with high confidence scores. In the second step, the center align- ment is used to align the features at 2D/3D centers. Fur- ther, it is often difficult to learn global information and capture long-range relationships, which are important for the depth prediction of objects. Therefore, we propose a novel asymmetric non-local attention block with multi- scale sampling to extract depth-wise features. The pro- posed M3DSSD achieves significantly better performance than the monocular 3D object detection methods on the KITTI dataset, in both 3D object detection and bird's eye view tasks. The code is released at https://github. com/mumianyuxin/M3DSSD.",
        "authors": [
            " Yong Ding",
            "Abu Dhabi",
            "Abu Dhabi"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Luo_M3DSSD_Monocular_3D_Single_Stage_Object_Detector_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 322,
        "title": "Dual Contradistinctive Generative Autoencoder",
        "abstract": "We present a new generative autoencoder model with dual contradistinctive losses to improve generative autoencoder that performs simultaneous inference (reconstruction) and synthesis (sampling). Our model, named dual contradis- tinctive generative autoencoder (DC-VAE), integrates an instance-level discriminative loss (maintaining the instance- level fidelity for the reconstruction/synthesis) with a set-level adversarial loss (encouraging the set-level fidelity for the reconstruction/synthesis), both being contradistinctive. Ex- tensive experimental results by DC-VAE across different res- olutions including 32\u00d732, 64\u00d764, 128\u00d7128, and 512\u00d7512 are reported. The two contradistinctive losses in VAE work harmoniously in DC-VAE leading to a significant qualitative and quantitative performance enhancement over the base- line VAEs without architectural changes. State-of-the-art or competitive results among generative autoencoders for image reconstruction, image synthesis, image interpolation, and representation learning are observed. DC-VAE is a general-purpose VAE model, applicable to a wide variety of downstream tasks in computer vision and machine learning.",
        "authors": [
            "Gaurav Parmar ",
            "Dacheng Li ",
            "Kwonjoon Lee ",
            "Zhuowen Tu",
            "bpc ",
            "bpc ",
            "bpc ",
            "bpc "
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Parmar_Dual_Contradistinctive_Generative_Autoencoder_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 323,
        "title": "Prototype Completion with Primitive Knowledge for Few-Shot Learning",
        "abstract": "Few-shot learning is a challenging task, which aims to learn a classifier for novel classes with few examples. Pre- training based meta-learning methods effectively tackle the problem by pre-training a feature extractor and then fine- tuning it through the nearest centroid based meta-learning. However, results show that the fine-tuning step makes very marginal improvements. In this paper, 1) we figure out the key reason, i.e., in the pre-trained feature space, the base classes already form compact clusters while novel classes spread as groups with large variances, which im- plies that fine-tuning the feature extractor is less meaning- ful; 2) instead of fine-tuning the feature extractor, we focus on estimating more representative prototypes during meta- learning. Consequently, we propose a novel prototype com- pletion based meta-learning framework. This framework first introduces primitive knowledge (i.e., class-level part or attribute annotations) and extracts representative attribute features as priors. Then, we design a prototype completion network to learn to complete prototypes with these priors. To avoid the prototype completion error caused by primitive knowledge noises or class differences, we further develop a Gaussian based prototype fusion strategy that combines the mean-based and completed prototypes by exploiting the unlabeled samples. Extensive experiments show that our method: (i) can obtain more accurate prototypes; (ii) out- performs state-of-the-art techniques by 2% \u223c 9% in terms of classification accuracy. Our code is available online 1.",
        "authors": [
            "Baoquan Zhang",
            "Xutao Li",
            "Yunming Ye",
            "Zhichao Huang",
            "Lisai Zhang"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Zhang_Prototype_Completion_With_Primitive_Knowledge_for_Few-Shot_Learning_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 324,
        "title": "BASAR:Black-box Attack on Skeletal Action Recognition",
        "abstract": "Skeletal motion plays a vital role in human activity recognition as either an independent data source or a com- plement [33]. The robustness of skeleton-based activity recognizers has been questioned recently [29, 50], which shows that they are vulnerable to adversarial attacks when the full-knowledge of the recognizer is accessible to the at- tacker. However, this white-box requirement is overly re- strictive in most scenarios and the attack is not truly threat- ening. In this paper, we show that such threats do exist under black-box settings too. To this end, we propose the first black-box adversarial attack method BASAR. Through BASAR, we show that adversarial attack is not only truly a threat but also can be extremely deceitful, because on- manifold adversarial samples are rather common in skele- tal motions, in contrast to the common belief that adver- sarial samples only exist off-manifold [18]. Through ex- haustive evaluation and comparison, we show that BASAR can deliver successful attacks across models, data, and at- tack modes. Through harsh perceptual studies, we show that it achieves effective yet imperceptible attacks. By an- alyzing the attack on different activity recognizers, BASAR helps identify the potential causes of their vulnerability and provides insights on what classifiers are likely to be more robust against attack.",
        "authors": [
            "Yunfeng Diao",
            "Tianjia Shao",
            "Yong Liang Yang",
            "Kun Zhou",
            "He Wang"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Diao_BASARBlack-Box_Attack_on_Skeletal_Action_Recognition_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 326,
        "title": "Roses are Red, Violets are Blue... But Should VQA expect Them To?",
        "abstract": "Models for Visual Question Answering (VQA) are notori- ous for their tendency to rely on dataset biases, as the large and unbalanced diversity of questions and concepts involved and tends to prevent models from learning to \"rea- son\", leading them to perform \"educated guesses\" instead. In this paper, we claim that the standard evaluation met- ric, which consists in measuring the overall in-domain ac- curacy, is misleading. Since questions and concepts are unbalanced, this tends to favor models which exploit sub- tle training set statistics. Alternatively, naively introducing artificial distribution shifts between train and test splits is also not completely satisfying. First, the shifts do not re- flect real-world tendencies, resulting in unsuitable models; second, since the shifts are handcrafted, trained models are specifically designed for this particular setting, and do not generalize to other configurations. We propose the GQA- OOD benchmark designed to overcome these concerns: we measure and compare accuracy over both rare and frequent question-answer pairs, and argue that the former is better suited to the evaluation of reasoning abilities, which we ex- perimentally validate with models trained to more or less exploit biases. In a large-scale study involving 7 VQA mod- els and 3 bias reduction techniques, we also experimentally demonstrate that these models fail to address questions in- volving infrequent concepts and provide recommendations for future directions of research.",
        "authors": [
            "Corentin Kervadec",
            "Grigory Antipov",
            "Moez Baccouche",
            "Christian Wolf",
            "INSA Lyon",
            "UMR CNRS "
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Kervadec_Roses_Are_Red_Violets_Are_Blue..._but_Should_VQA_Expect_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 327,
        "title": "TrafficSim: Learning to Simulate Realistic Multi-Agent Behaviors",
        "abstract": "Simulation has the potential to massively scale evalua- tion of self-driving systems, enabling rapid development as well as safe deployment. Bridging the gap between simu- lation and the real world requires realistic multi-agent be- haviors. Existing simulation environments rely on heuristic- based models that directly encode traffic rules, which cannot capture irregular maneuvers (e.g., nudging, U-turns) and complex interactions (e.g., yielding, merging). In contrast, we leverage real-world data to learn directly from human demonstration, and thus capture more naturalistic driving behaviors. To this end, we propose TRAFFICSIM, a multi- agent behavior model for realistic traffic simulation. In particular, we parameterize the policy with an implicit la- tent variable model that generates socially-consistent plans for all actors in the scene jointly. To learn a robust policy amenable for long horizon simulation, we unroll the policy in training and optimize through the fully differentiable simu- lation across time. Our learning objective incorporates both human demonstrations as well as common sense. We show TRAFFICSIM generates significantly more realistic traffic scenarios as compared to a diverse set of baselines. Notably, we can exploit trajectories generated by TRAFFICSIM as ef- fective data augmentation for training better motion planner.",
        "authors": [
            "Simon Suo",
            "Sebastian Regalado",
            "Sergio Casas",
            "Raquel Urtasun",
            "Uber ATG"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Suo_TrafficSim_Learning_To_Simulate_Realistic_Multi-Agent_Behaviors_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 328,
        "title": "You See What I Want You to See: Exploring Targeted Black-Box Transferability Attack for Hash-based Image Retrieval Systems",
        "abstract": "With the large multimedia content online, deep hashing has become a popular method for efficient image retrieval and storage. However, by inheriting the algorithmic back- end from softmax classification, these techniques are vul- nerable to the well-known adversarial examples as well. The massive collection of online images into the database also opens up new attack vectors. Attackers can embed ad- versarial images into the database and target specific cat- egories to be retrieved by user queries. In this paper, we start from an adversarial standpoint to explore and enhance the capacity of targeted black-box transferability attack for deep hashing. We motivate this work by a series of empiri- cal studies to see the unique challenges in image retrieval. We study the relations between adversarial subspace and black-box transferability via utilizing random noise as a proxy. Then we develop a new attack that is simultane- ously adversarial and robust to noise to enhance transfer- ability. Our experimental results demonstrate about 1.2-3\u00d7 improvements of black-box transferability compared with the state-of-the-art mechanisms. The code is available at: https://github.com/SugarRuy/CVPR21 Transferred Hash.",
        "authors": [],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Xiao_You_See_What_I_Want_You_To_See_Exploring_Targeted_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 329,
        "title": "Stable View Synthesis",
        "abstract": "We present Stable View Synthesis (SVS). Given a set of source images depicting a scene from freely distributed viewpoints, SVS synthesizes new views of the scene. The method operates on a geometric scaffold computed via structure-from-motion and multi-view stereo. Each point on this 3D scaffold is associated with view rays and cor- responding feature vectors that encode the appearance of this point in the input images. The core of SVS is view- dependent on-surface feature aggregation, in which direc- tional feature vectors at each 3D point are processed to produce a new feature vector for a ray that maps this point into the new target view. The target view is then rendered by a convolutional network from a tensor of features syn-",
        "authors": [
            "Gernot Riegler",
            "Vladlen Koltun",
            "bpc "
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Riegler_Stable_View_Synthesis_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 330,
        "title": "FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space",
        "abstract": "Federated learning allows distributed medical institu- tions to collaboratively learn a shared prediction model with privacy protection. While at clinical deployment, the models trained in federated learning can still suffer from performance drop when applied to completely unseen hos- pitals outside the federation. In this paper, we point out and solve a novel problem setting of federated domain gener- alization (FedDG), which aims to learn a federated model from multiple distributed source domains such that it can directly generalize to unseen target domains. We present a novel approach, named as Episodic Learning in Contin- uous Frequency Space (ELCFS), for this problem by en- abling each client to exploit multi-source data distributions under the challenging constraint of data decentralization. Our approach transmits the distribution information across clients in a privacy-protecting way through an effective con- tinuous frequency space interpolation mechanism. With the transferred multi-source distributions, we further carefully design a boundary-oriented episodic learning paradigm to expose the local learning to domain distribution shifts and particularly meet the challenges of model generalization in medical image segmentation scenario. The effectiveness of our method is demonstrated with superior performance over state-of-the-arts and in-depth ablation experiments on two medical image segmentation tasks. The code is avail- able at https://github.com/liuquande/FedDG-ELCFS.",
        "authors": [
            "Cheng Chen",
            "Jing Qin",
            "Qi Dou",
            "Pheng Ann Heng"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Liu_FedDG_Federated_Domain_Generalization_on_Medical_Image_Segmentation_via_Episodic_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 332,
        "title": "Body Meshes as Points",
        "abstract": "We consider the challenging multi-person 3D body mesh estimation task in this work. Existing methods are mostly two-stage based\u2014one stage for person localization and the other stage for individual body mesh estimation, lead- ing to redundant pipelines with high computation cost and degraded performance for complex scenes (e.g., occluded person instances). In this work, we present a single- stage model, Body Meshes as Points (BMP), to simplify the pipeline and lift both efficiency and performance. In par- ticular, BMP adopts a new method that represents multiple person instances as points in the spatial-depth space where each point is associated with one body mesh. Hinging on such representations, BMP can directly predict body meshes for multiple persons in a single stage by concurrently local- izing person instance points and estimating the correspond- ing body meshes. To better reason about depth ordering of all the persons within the same scene, BMP designs a simple yet effective inter-instance ordinal depth loss to ob- tain depth-coherent body mesh estimation. BMP also in- troduces a novel keypoint-aware augmentation to enhance model robustness to occluded person instances. Compre- hensive experiments on benchmarks Panoptic, MuPoTS- 3D and 3DPW clearly demonstrate the state-of-the-art ef- ficiency of BMP for multi-person body mesh estimation, to- gether with outstanding accuracy. Code can be found at: https://github.com/jfzhang95/BMP.",
        "authors": [
            "Jianfeng Zhang",
            "Dongdong Yu",
            "Jun Hao Liew",
            "Xuecheng Nie",
            "Jiashi Feng"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Zhang_Body_Meshes_as_Points_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 333,
        "title": "Privacy-Preserving Image Features via Adversarial Affine Subspace Embeddings",
        "abstract": "Many computer vision systems require users to upload im- age features to the cloud for processing and storage. These features can be exploited to recover sensitive information about the scene or subjects, e.g., by reconstructing the ap- pearance of the original image. To address this privacy concern, we propose a new privacy-preserving feature repre- sentation. The core idea of our work is to drop constraints from each feature descriptor by embedding it within an affine subspace containing the original feature as well as adver- sarial feature samples. Feature matching on the privacy- preserving representation is enabled based on the notion of subspace-to-subspace distance. We experimentally demon- strate the effectiveness of our method and its high practical relevance for the applications of visual localization and mapping as well as face authentication. Compared to the original features, our approach makes it significantly more difficult for an adversary to recover private information.",
        "authors": [
            "Mihai Dusmanu",
            "Sudipta N Sinha",
            "Marc Pollefeys",
            "ETH Z rich",
            " Microsoft"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Dusmanu_Privacy-Preserving_Image_Features_via_Adversarial_Affine_Subspace_Embeddings_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 334,
        "title": "Understanding the Behaviour of Contrastive Loss",
        "abstract": "Unsupervised contrastive learning has achieved out- standing success, while the mechanism of contrastive loss has been less studied. In this paper, we concentrate on the understanding of the behaviours of unsupervised con- trastive loss. We will show that the contrastive loss is a hardness-aware loss function, and the temperature \u03c4 con- trols the strength of penalties on hard negative samples. The previous study has shown that uniformity is a key property of contrastive learning. We build relations between the uni- formity and the temperature \u03c4. We will show that uniformity helps the contrastive learning to learn separable features, however excessive pursuit to the uniformity makes the con- trastive loss not tolerant to semantically similar samples, which may break the underlying semantic structure and be harmful to the formation of features useful for downstream tasks. This is caused by the inherent defect of the instance discrimination objective. Specifically, instance discrimina- tion objective tries to push all different instances apart, ig- noring the underlying relations between samples. Pushing semantically consistent samples apart has no positive effect for acquiring a prior informative to general downstream tasks. A well-designed contrastive loss should have some extents of tolerance to the closeness of semantically sim- ilar samples. Therefore, we find that the contrastive loss meets a uniformity-tolerance dilemma, and a good choice of temperature can compromise these two properties prop- erly to both learn separable features and tolerant to seman- tically similar samples, improving the feature qualities and the downstream performances.",
        "authors": [
            "Feng Wang",
            "Huaping Liu"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Wang_Understanding_the_Behaviour_of_Contrastive_Loss_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 335,
        "title": "Rotation Coordinate Descent for Fast Globally Optimal Rotation Averaging",
        "abstract": "Under mild conditions on the noise level of the measure- ments, rotation averaging satisfies strong duality, which en- ables global solutions to be obtained via semidefinite pro- gramming (SDP) relaxation. However, generic solvers for SDP are rather slow in practice, even on rotation averag- ing instances of moderate size, thus developing specialised algorithms is vital. In this paper, we present a fast algo- rithm that achieves global optimality called rotation co- ordinate descent (RCD). Unlike block coordinate descent (BCD) which solves SDP by updating the semidefinite ma- trix in a row-by-row fashion, RCD directly maintains and updates all valid rotations throughout the iterations. This obviates the need to store a large dense semidefinite matrix. We mathematically prove the convergence of our algorithm and empirically show its superior efficiency over state-of- the-art global methods on a variety of problem configura- tions. Maintaining valid rotations also facilitates incor- porating local optimisation routines for further speed-ups. Moreover, our algorithm is simple to implement 1.",
        "authors": [
            "Alvaro Parra",
            "Shin Fang Chng",
            "Tat Jun Chin",
            "Ian Reid"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Parra_Rotation_Coordinate_Descent_for_Fast_Globally_Optimal_Rotation_Averaging_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 336,
        "title": "Learnable Companding Quantization for Accurate Low-bit Neural Networks",
        "abstract": "Quantizing deep neural networks is an effective method for reducing memory consumption and improving inference speed, and is thus useful for implementation in resource- constrained devices. However, it is still hard for extremely low-bit models to achieve accuracy comparable with that of full-precision models. To address this issue, we propose learnable companding quantization (LCQ) as a novel non- uniform quantization method for 2-, 3-, and 4-bit models. LCQ jointly optimizes model weights and learnable com- panding functions that can flexibly and non-uniformly con- trol the quantization levels of weights and activations. We also present a new weight normalization technique that al- lows more stable training for quantization. Experimental results show that LCQ outperforms conventional state-of- the-art methods and narrows the gap between quantized and full-precision models for image classification and ob- ject detection tasks. Notably, the 2-bit ResNet-50 model on ImageNet achieves top-1 accuracy of 75.1% and reduces the gap to 1.7%, allowing LCQ to further exploit the poten- tial of non-uniform quantization.",
        "authors": [
            "Kohei Yamamoto"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Yamamoto_Learnable_Companding_Quantization_for_Accurate_Low-Bit_Neural_Networks_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 337,
        "title": "Visually Informed Binaural Audio Generation without Binaural Audios",
        "abstract": "Stereophonic audio, especially binaural audio, plays an essential role in immersive viewing environments. Recent research has explored generating visually guided stereo- phonic audios supervised by multi-channel audio collections. However, due to the requirement of professional recording devices, existing datasets are limited in scale and variety, which impedes the generalization of supervised methods in real-world scenarios. In this work, we propose Pseu- doBinaural, an effective pipeline that is free of binaural recordings. The key insight is to carefully build pseudo visual-stereo pairs with mono data for training. Specifi- cally, we leverage spherical harmonic decomposition and head-related impulse response (HRIR) to identify the rela- tionship between spatial locations and received binaural audios. Then in the visual modality, corresponding visual cues of the mono data are manually placed at sound source positions to form the pairs. Compared to fully-supervised paradigms, our binaural-recording-free pipeline shows great stability in cross-dataset evaluation and achieves compara- ble performance under subjective preference. Moreover, combined with binaural recordings, our method is able to further boost the performance of binaural audio generation under supervised settings1.",
        "authors": [
            "Xudong Xu",
            "Hang Zhou",
            "Ziwei Liu",
            "Bo Dai",
            "Xiaogang Wang",
            "Dahua Lin"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Xu_Visually_Informed_Binaural_Audio_Generation_without_Binaural_Audios_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 338,
        "title": "Towards Fast and Accurate Real-World Depth Super-Resolution: Benchmark Dataset and Baseline",
        "abstract": "Depth maps obtained by commercial depth sensors are always in low-resolution, making it difficult to be used in various computer vision tasks. Thus, depth map super- resolution (SR) is a practical and valuable task, which up- scales the depth map into high-resolution (HR) space. How- ever, limited by the lack of real-world paired low-resolution (LR) and HR depth maps, most existing methods use down- sampling to obtain paired training samples. To this end, we first construct a large-scale dataset named \"RGB-D-D\", which can greatly promote the study of depth map SR and even more depth-related real-world tasks. The \"D-D\" in our dataset represents the paired LR and HR depth maps captured from mobile phone and Lucid Helios respectively ranging from indoor scenes to challenging outdoor scenes. Besides, we provide a fast depth map super-resolution (FDSR) baseline, in which the high-frequency component adaptively decomposed from RGB image to guide the depth map SR. Extensive experiments on existing public datasets demonstrate the effectiveness and efficiency of our network compared with the state-of-the-art methods. Moreover, for the real-world LR depth maps, our algorithm can produce more accurate HR depth maps with clearer boundaries and to some extent correct the depth value errors.",
        "authors": [
            "Lingzhi He",
            "Hongguang Zhu",
            "Feng Li",
            "Huihui Bai",
            "Runmin Cong",
            "Chunjie Zhang",
            "Chunyu Lin",
            "Meiqin Liu",
            "Yao Zhao"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/He_Towards_Fast_and_Accurate_Real-World_Depth_Super-Resolution_Benchmark_Dataset_and_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 339,
        "title": "Monocular 3D Object Detection: An Extrinsic Parameter Free Approach",
        "abstract": "Monocular 3D object detection is an important task in autonomous driving. It can be easily intractable where there exists ego-car pose change w.r.t. ground plane. This is common due to the slight fluctuation of road smoothness and slope. Due to the lack of insight in industrial appli- cation, existing methods on open datasets neglect the cam- era pose information, which inevitably results in the detec- tor being susceptible to camera extrinsic parameters. The perturbation of objects is very popular in most autonomous driving cases for industrial products. To this end, we pro- pose a novel method to capture camera pose to formulate the detector free from extrinsic perturbation. Specifically, the proposed framework predicts camera extrinsic param- eters by detecting vanishing point and horizon change. A converter is designed to rectify perturbative features in the latent space. By doing so, our 3D detector works indepen- dent of the extrinsic parameter variations and produces ac- curate results in realistic cases, e.g., potholed and uneven roads, where almost all existing monocular detectors fail to handle. Experiments demonstrate our method yields the best performance compared with the other state-of-the-arts by a large margin on both KITTI 3D and nuScenes datasets.",
        "authors": [
            "Hongzi Zhu "
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Zhou_Monocular_3D_Object_Detection_An_Extrinsic_Parameter_Free_Approach_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 340,
        "title": "Relation-aware Instance Refinement for Weakly Supervised Visual Grounding",
        "abstract": "Visual grounding, which aims to build a correspon- dence between visual objects and their language entities, plays a key role in cross-modal scene understanding. One promising and scalable strategy for learning visual ground- ing is to utilize weak supervision from only image-caption pairs. Previous methods typically rely on matching query phrases directly to a precomputed, fixed object candidate pool, which leads to inaccurate localization and ambigu- ous matching due to lack of semantic relation constraints. In our paper, we propose a novel context-aware weakly- supervised learning method that incorporates coarse-to- fine object refinement and entity relation modeling into a two-stage deep network, capable of producing more ac- curate object representation and matching. To effectively train our network, we introduce a self-taught regression loss for the proposal locations and a classification loss based on parsed entity relations. Extensive experiments on two public benchmarks Flickr30K Entities and Refer- ItGame demonstrate the efficacy of our weakly grounding framework. The results show that we outperform the previ- ous methods by a considerable margin, achieving 59.27% top-1 accuracy in Flickr30K Entities and 37.68% in the ReferItGame dataset respectively1.",
        "authors": [
            "Yongfei Liu",
            "Bo Wan",
            "Lin Ma",
            "Xuming He",
            "KU Leuven"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Liu_Relation-aware_Instance_Refinement_for_Weakly_Supervised_Visual_Grounding_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 341,
        "title": "Discrimination-Aware Mechanism for Fine-grained Representation Learning",
        "abstract": "Recently, with the emergence of retrieval requirements for certain individual in the same superclass, e.g., birds, persons, cars, fine-grained recognition task has attracted a significant amount of attention from academia and industry. In fine-grained recognition scenario, the inter-class differ- ences are quite diverse and subtle, which makes it challeng- ing to extract all the discriminative cues. Traditional train- ing mechanism optimizes the overall discriminativeness of the whole feature. It may stop early when some feature ele- ments has been trained to distinguish training samples well, leaving other elements insufficiently trained for a feature. This would result in a less generalizable feature extractor that only captures major discriminative cues and ignores subtle ones. Therefore, there is a need for a training mech- anism that enforces the discriminativeness of all the ele- ments in the feature to capture more the subtle visual cues. In this paper, we propose a Discrimination-Aware Mecha- nism (DAM) that iteratively identifies insufficiently trained elements and improves them. DAM is able to increase the number of well learned elements, which captures more vi- sual cues by the feature extractor. In this way, a more infor- mative representation is learned, which brings better gen- eralization performance. We show that DAM can be easily applied to both proxy-based and pair-based loss functions, and thus can be used in most existing fine-grained recog- nition paradigms. Comprehensive experiments on CUB- 200-2011, Cars196, Market-1501, and MSMT17 datasets demonstrate the advantages of our DAM based loss over the related state-of-the-art approaches.",
        "authors": [
            "Furong Xu",
            "Meng Wang",
            "Wei Zhang",
            "Yuan Cheng"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Xu_Discrimination-Aware_Mechanism_for_Fine-Grained_Representation_Learning_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 342,
        "title": "PISE: Person Image Synthesis and Editing with Decoupled GAN",
        "abstract": "Person image synthesis, e.g., pose transfer, is a challeng- ing problem due to large variation and occlusion. Existing methods have difficulties predicting reasonable invisible re- gions and fail to decouple the shape and style of clothing, which limits their applications on person image editing. In this paper, we propose PISE, a novel two-stage generative model for Person Image Synthesis and Editing, which is able to generate realistic person images with desired poses, textures, or semantic layouts. For human pose transfer, we first synthesize a human parsing map aligned with the target pose to represent the shape of clothing by a parsing gener- ator, and then generate the final image by an image genera- tor. To decouple the shape and style of clothing, we propose joint global and local per-region encoding and normaliza- tion to predict the reasonable style of clothing for invisi- ble regions. We also propose spatial-aware normalization to retain the spatial context relationship in the source im- age. The results of qualitative and quantitative experiments demonstrate the superiority of our model on human pose transfer. Besides, the results of texture transfer and region editing show that our model can be applied to person im- age editing. The code is available for research purposes at https://github.com/Zhangjinso/PISE.",
        "authors": [
            "Jinsong Zhang",
            "Kun Li",
            "Yu Kun Lai",
            "Jingyu Yang"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Zhang_PISE_Person_Image_Synthesis_and_Editing_With_Decoupled_GAN_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 343,
        "title": "CapsuleRRT: Relationships-aware Regression Tracking via Capsules",
        "abstract": "Regression tracking has gained more and more attention thanks to its easy-to-implement characteristics, while exist- ing regression trackers rarely consider the relationships be- tween the object parts and the complete object. This would ultimately result in drift from the target object when missing some parts of the target object. Recently, Capsule Network (CapsNet) has shown promising results for image classifica- tion benefits from its part-object relationships mechanism, while CapsNet is known for its high computational demand even when carrying out simple tasks. Therefore, a primitive adaptation of CapsNet to regression tracking does not make sense, since this will seriously affect speed of a tracker. To solve these problems, we first explore the spatial-temporal relationships endowed by the CapsNet for regression track- ing. The entire regression framework, dubbed CapsuleRRT, consists of three parts. One is S-Caps, which captures the spatial relationships between the parts and the object. Meanwhile, a T-Caps module is designed to exploit the tem- poral relationships within the target. The response of the target is obtained by STCaps Learning. Further, a prior- guided capsule routing algorithm is proposed to generate more accurate capsule assignments for subsequent frames. Apart from this, the heavy computation burden in CapsNet is addressed with a knowledge distillation pose matrix com- pression strategy that exploits more tight and discriminative representation with few samples. Extensive experimental results show that CapsuleRRT performs favorably against state-of-the-art methods in terms of accuracy and speed.",
        "authors": [
            "Ding Ma",
            "Xiangqian Wu"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Ma_CapsuleRRT_Relationships-Aware_Regression_Tracking_via_Capsules_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 344,
        "title": "Spk2ImgNet: Learning to Reconstruct Dynamic Scene from Continuous Spike Stream",
        "abstract": "The recently invented retina-inspired spike camera has shown great potential for capturing dynamic scenes. Differ- ent from the conventional digital cameras that compact the photoelectric information within the exposure interval into a single snapshot, the spike camera produces a continuous spike stream to record the dynamic light intensity variation process. For spike cameras, image reconstruction remains an important and challenging issue. To this end, this paper develops a spike-to-image neural network (Spk2ImgNet) to reconstruct the dynamic scene from the continuous spike stream. In particular, to handle the challenges brought by both noise and high-speed motion, we propose a hier- archical architecture to exploit the temporal correlation of the spike stream progressively. Firstly, a spatially adaptive light inference subnet is proposed to exploit the local tem- poral correlation, producing basic light intensity estimates of different moments. Then, a pyramid deformable align- ment is utilized to align the intermediate features such that the feature fusion module can exploit the long-term tem- poral correlation, while avoiding undesired motion blur. In addition, to train the network, we simulate the work- ing mechanism of spike camera to generate a large-scale spike dataset composed of spike streams and correspond- ing ground truth images. Experimental results demonstrate that the proposed network evidently outperforms the state- of-the-art spike camera reconstruction methods.",
        "authors": [
            "Jing Zhao",
            "Ruiqin Xiong",
            "Hangfan Liu",
            "Jian Zhang",
            "Tiejun Huang"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Zhao_Spk2ImgNet_Learning_To_Reconstruct_Dynamic_Scene_From_Continuous_Spike_Stream_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 345,
        "title": "3D Object Detection with Pointformer",
        "abstract": "Feature learning for 3D object detection from point clouds is very challenging due to the irregularity of 3D point cloud data. In this paper, we propose Pointformer, a Trans- former backbone designed for 3D point clouds to learn fea- tures effectively. Specifically, a Local Transformer module is employed to model interactions among points in a local region, which learns context-dependent region features at an object level. A Global Transformer is designed to learn context-aware representations at the scene level. To fur- ther capture the dependencies among multi-scale represen- tations, we propose Local-Global Transformer to integrate local features with global features from higher resolution. In addition, we introduce an efficient coordinate refinement module to shift down-sampled points closer to object cen- troids, which improves object proposal generation. We use Pointformer as the backbone for state-of-the-art object de- tection models and demonstrate significant improvements over original models on both indoor and outdoor datasets.",
        "authors": [
            "Alexa AI"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Pan_3D_Object_Detection_With_Pointformer_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 346,
        "title": "Spatiotemporal Registration for Event-based Visual Odometry",
        "abstract": "A useful application of event sensing is visual odometry, especially in settings that require high-temporal resolution. The state-of-the-art method of contrast maximisation recov- ers the motion from a batch of events by maximising the contrast of the image of warped events. However, the cost scales with image resolution and the temporal resolution can be limited by the need for large batch sizes to yield suf- ficient structure in the contrast image1. In this work, we pro- pose spatiotemporal registration as a compelling technique for event-based rotational motion estimation. We theoreti- cally justify the approach and establish its fundamental and practical advantages over contrast maximisation. In par- ticular, spatiotemporal registration also produces feature tracks as a by-product, which directly supports an efficient visual odometry pipeline with graph-based optimisation for motion averaging. The simplicity of our visual odometry pipeline allows it to process more than 1 M events/second. We also contribute a new event dataset for visual odometry, where motion sequences with large velocity variations were acquired using a high-precision robot arm2.",
        "authors": [
            "Daqi Liu",
            "Alvaro Parra",
            "Tat Jun Chin"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Liu_Spatiotemporal_Registration_for_Event-Based_Visual_Odometry_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 347,
        "title": "Graph Stacked Hourglass Networks for 3D Human Pose Estimation",
        "abstract": "In this paper, we propose a novel graph convolutional network architecture, Graph Stacked Hourglass Networks, for 2D-to-3D human pose estimation tasks. The proposed architecture consists of repeated encoder-decoder, in which graph-structured features are processed across three differ- ent scales of human skeletal representations. This multi- scale architecture enables the model to learn both local and global feature representations, which are critical for 3D human pose estimation. We also introduce a multi-level feature learning approach using different-depth intermedi- ate features and show the performance improvements that result from exploiting multi-scale, multi-level feature repre- sentations. Extensive experiments are conducted to validate our approach, and the results show that our model outper- forms the state-of-the-art.",
        "authors": [
            "Tianhan Xu",
            "Wataru Takano"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Xu_Graph_Stacked_Hourglass_Networks_for_3D_Human_Pose_Estimation_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 348,
        "title": "Bilevel Online Adaptation for Out-of-Domain Human Mesh Reconstruction",
        "abstract": "This paper considers a new problem of adapting a pre- trained model of human mesh reconstruction to out-of- domain streaming videos. However, most previous methods based on the parametric SMPL model [36] underperform in new domains with unexpected, domain-specific attributes, such as camera parameters, lengths of bones, backgrounds, and occlusions. Our general idea is to dynamically fine- tune the source model on test video streams with additional temporal constraints, such that it can mitigate the domain gaps without over-fitting the 2D information of individual test frames. A subsequent challenge is how to avoid con- flicts between the 2D and temporal constraints. We propose to tackle this problem using a new training algorithm named Bilevel Online Adaptation (BOA), which divides the opti- mization process of overall multi-objective into two steps of weight probe and weight update in a training iteration. We demonstrate that BOA leads to state-of-the-art results on two human mesh reconstruction benchmarks1.",
        "authors": [
            "Shanyan Guan",
            "Jingwei Xu",
            "Yunbo Wang",
            "Bingbing Ni",
            "Xiaokang Yang",
            "Shanghai "
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Guan_Bilevel_Online_Adaptation_for_Out-of-Domain_Human_Mesh_Reconstruction_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 349,
        "title": "Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling",
        "abstract": "The canonical approach to video-and-language learning (e.g., video question answering) dictates a neural model to learn from offline-extracted dense video features from vi- sion models and text features from language models. These feature extractors are trained independently and usually on tasks different from the target domains, rendering these fixed features sub-optimal for downstream tasks. Moreover, due to the high computational overload of dense video fea- tures, it is often difficult (or infeasible) to plug feature ex- tractors directly into existing approaches for easy finetun- ing. To provide a remedy to this dilemma, we propose a generic framework CLIPBERT that enables affordable end- to-end learning for video-and-language tasks, by employ- ing sparse sampling, where only a single or a few sparsely sampled short clips from a video are used at each train- ing step. Experiments on text-to-video retrieval and video question answering on six datasets demonstrate that CLIP- BERT outperforms (or is on par with) existing methods that exploit full-length videos, suggesting that end-to-end learn- ing with just a few sparsely sampled clips is often more accurate than using densely extracted offline features from full-length videos, proving the proverbial less-is-more prin- ciple. Videos in the datasets are from considerably differ- ent domains and lengths, ranging from 3-second generic- domain GIF videos to 180-second YouTube human activity videos, showing the generalization ability of our approach. Comprehensive ablation studies and thorough analyses are provided to dissect what factors lead to this success. Our code is publicly available.1",
        "authors": [
            "Jie Lei ",
            "Linjie Li ",
            "Luowei Zhou",
            "Zhe Gan",
            "Tamara L Berg",
            "Mohit Bansal",
            "Jingjing Liu",
            "UNC Chapel Hill"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Lei_Less_Is_More_ClipBERT_for_Video-and-Language_Learning_via_Sparse_Sampling_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 350,
        "title": "Learning by Aligning Videos in Time",
        "abstract": "We present a self-supervised approach for learning video representations using temporal video alignment as a pre- text task, while exploiting both frame-level and video-level information. We leverage a novel combination of tempo- ral alignment loss and temporal regularization terms, which can be used as supervision signals for training an encoder network. Specifically, the temporal alignment loss (i.e., Soft-DTW) aims for the minimum cost for temporally align- ing videos in the embedding space. However, optimizing solely for this term leads to trivial solutions, particularly, one where all frames get mapped to a small cluster in the embedding space. To overcome this problem, we pro- pose a temporal regularization term (i.e., Contrastive-IDM) which encourages different frames to be mapped to differ- ent points in the embedding space. Extensive evaluations on various tasks, including action phase classification, ac- tion phase progression, and fine-grained frame retrieval, on three datasets, namely Pouring, Penn Action, and IKEA ASM, show superior performance of our approach over state-of-the-art methods for self-supervised representation learning from videos. In addition, our method provides sig-",
        "authors": [
            "Sanjay Haresh",
            "Sateesh Kumar",
            "Huseyin Coskun",
            "Shahram N Syed",
            "M Zeeshan Zia",
            "Quoc Huy Tran",
            "Temporal Alignment Loss",
            "Temporal Regularization",
            "Embedding Space",
            "Temporal Video Alignment"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Haresh_Learning_by_Aligning_Videos_in_Time_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 351,
        "title": "Variational Prototype Learning for Deep Face Recognition",
        "abstract": "Deep face recognition has achieved remarkable im- provements due to the introduction of margin-based soft- max loss, in which the prototype stored in the last linear layer represents the center of each class. In these methods, training samples are enforced to be close to positive pro- totypes and far apart from negative prototypes by a clear margin. However, we argue that prototype learning only employs sample-to-prototype comparisons without consid- ering sample-to-sample comparisons during training and the low loss value gives us an illusion of perfect feature embedding, impeding the further exploration of SGD. To this end, we propose Variational Prototype Learning (VPL), which represents every class as a distribution instead of a point in the latent space. By identifying the slow feature drift phenomenon, we directly inject memorized features into prototypes to approximate variational prototype sam- pling. The proposed VPL can simulate sample-to-sample comparisons within the classification framework, encour- aging the SGD solver to be more exploratory, while boost- ing performance. Moreover, VPL is conceptually simple, easy to implement, computationally efficient and memory saving. We present extensive experimental results on pop- ular benchmarks, which demonstrate the superiority of the proposed VPL method over the state-of-the-art competitors.",
        "authors": [
            "Jiankang Deng ",
            "Jia Guo ",
            "Jing Yang",
            "Stefanos Zafeiriou"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Deng_Variational_Prototype_Learning_for_Deep_Face_Recognition_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 352,
        "title": "Railroad is not a Train: Saliency as Pseudo-pixel Supervision for Weakly Supervised Semantic Segmentation",
        "abstract": "Existing studies in weakly-supervised semantic segmen- tation (WSSS) using image-level weak supervision have sev- eral limitations: sparse object coverage, inaccurate ob- ject boundaries, and co-occurring pixels from non-target objects. To overcome these challenges, we propose a novel framework, namely Explicit Pseudo-pixel Supervision (EPS), which learns from pixel-level feedback by combin- ing two weak supervisions; the image-level label provides the object identity via the localization map and the saliency map from the off-the-shelf saliency detection model offers rich boundaries. We devise a joint training strategy to fully utilize the complementary relationship between both infor- mation. Our method can obtain accurate object boundaries and discard co-occurring pixels, thereby significantly im- proving the quality of pseudo-masks. Experimental results show that the proposed method remarkably outperforms ex- isting methods by resolving key challenges of WSSS and achieves the new state-of-the-art performance on both PAS- CAL VOC 2012 and MS COCO 2014 datasets. The code is available at https://github.com/halbielee/EPS.",
        "authors": [
            "Seungho Lee",
            "Minhyun Lee",
            "Hyunjung Shim"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Lee_Railroad_Is_Not_a_Train_Saliency_As_Pseudo-Pixel_Supervision_for_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 353,
        "title": "Unsupervised Multi-source Domain Adaptation Without Access to Source Data",
        "abstract": "Unsupervised Domain Adaptation (UDA) aims to learn a predictor model for an unlabeled domain by transferring knowledge from a separate labeled source domain. However, most of these conventional UDA approaches make the strong assumption of having access to the source data during train- ing, which may not be very practical due to privacy, security and storage concerns. A recent line of work addressed this problem and proposed an algorithm that transfers knowl- edge to the unlabeled target domain from a single source model without requiring access to the source data. How- ever, for adaptation purposes, if there are multiple trained source models available to choose from, this method has to go through adapting each and every model individually, to check for the best source. Thus, we ask the question: can we find the optimal combination of source models, with no source data and without target labels, whose performance is no worse than the single best source? To answer this, we propose a novel and efficient algorithm which automatically combines the source models with suitable weights in such a way that it performs at least as good as the best source model. We provide intuitive theoretical insights to justify our claim. Furthermore, extensive experiments are conducted on several benchmark datasets to show the effectiveness of our algorithm, where in most cases, our method not only reaches best source accuracy but also outperforms it.",
        "authors": [
            "Sk Miraj Ahmed",
            "Dripta S Raychaudhuri",
            "Sujoy Paul",
            "Samet Oymak"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Ahmed_Unsupervised_Multi-Source_Domain_Adaptation_Without_Access_to_Source_Data_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 354,
        "title": "Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction",
        "abstract": "A video prediction model that generalizes to diverse scenes would enable intelligent agents such as robots to perform a variety of tasks via planning with the model. However, while existing video prediction models have produced promising results on small datasets, they suf- fer from severe underfitting when trained on large and diverse datasets. To address this underfitting challenge, we first observe that the ability to train larger video prediction models is often bottlenecked by the memory constraints of GPUs or TPUs. In parallel, deep hierar- chical latent variable models can produce higher quality predictions by capturing the multi-level stochasticity of future observations, but end-to-end optimization of such models is notably difficult. Our key insight is that greedy and modular optimization of hierarchical autoencoders can simultaneously address both the memory constraints and the optimization challenges of large-scale video pre- diction. We introduce Greedy Hierarchical Variational Autoencoders (GHVAEs), a method that learns high- fidelity video predictions by greedily training each level of a hierarchical autoencoder. In comparison to state- of-the-art models, GHVAEs provide 17-55% gains in prediction performance on four video datasets, a 35- 40% higher success rate on real robot tasks, and can improve performance monotonically by simply adding more modules. Visualization and more details are at https: // sites. google. com/ view/ ghvae .",
        "authors": [
            "Bohan Wu",
            "Suraj Nair",
            "Li Fei Fei",
            "Chelsea Finn",
            "CA "
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Wu_Greedy_Hierarchical_Variational_Autoencoders_for_Large-Scale_Video_Prediction_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 355,
        "title": "Adaptive Class Suppression Loss for Long-Tail Object Detection",
        "abstract": "To address the problem of long-tail distribution for the large vocabulary object detection task, existing methods usually divide the whole categories into several groups and treat each group with different strategies. These methods bring the following two problems. One is the training in- consistency between adjacent categories of similar sizes, and the other is that the learned model is lack of discrim- ination for tail categories which are semantically similar to some of the head categories. In this paper, we devise a novel Adaptive Class Suppression Loss (ACSL) to effec- tively tackle the above problems and improve the detection performance of tail categories. Specifically, we introduce a statistic-free perspective to analyze the long-tail distribu- tion, breaking the limitation of manual grouping. According to this perspective, our ACSL adjusts the suppression gra- dients for each sample of each class adaptively, ensuring the training consistency and boosting the discrimination for rare categories. Extensive experiments on long-tail datasets LVIS and Open Images show that the our ACSL achieves 5.18% and 5.2% improvements with ResNet50-FPN, and sets a new state of the art. Code and models are available at https://github.com/CASIA-IVA-Lab/ACSL.",
        "authors": [
            "Tong Wang",
            "Yousong Zhu",
            "Chaoyang Zhao",
            "Wei Zeng",
            "Jinqiao Wang",
            "Ming Tang",
            " NEXWISE Co"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Wang_Adaptive_Class_Suppression_Loss_for_Long-Tail_Object_Detection_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 356,
        "title": "pixelNeRF: Neural Radiance Fields from One or Few Images",
        "abstract": "We propose pixelNeRF, a learning framework that pre- dicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields [27] involves optimiz- ing the representation to every scene independently, requir- ing many calibrated views and significant compute time. We take a step towards resolving these shortcomings by in- troducing an architecture that conditions a NeRF on im- age inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no ex- plicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view syn- thesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixel- NeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pix- elNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website: https://alexyu.net/pixelnerf.",
        "authors": [
            "Alex Yu",
            "Vickie Ye",
            "Matthew Tancik",
            "Angjoo Kanazawa",
            "bpc ",
            "bpc ",
            "bpc ",
            "bpc ",
            "bpc ",
            "bpc "
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Yu_pixelNeRF_Neural_Radiance_Fields_From_One_or_Few_Images_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 357,
        "title": "DyGLIP: A Dynamic Graph Model with Link Prediction for Accurate Multi-Camera Multiple Object Tracking",
        "abstract": "Multi-Camera Multiple Object Tracking (MC-MOT) is a significant computer vision problem due to its emerging applicability in several real-world applications. Despite a large number of existing works, solving the data association problem in any MC-MOT pipeline is arguably one of the most challenging tasks. Developing a robust MC-MOT sys- tem, however, is still highly challenging due to many practi- cal issues such as inconsistent lighting conditions, varying object movement patterns, or the trajectory occlusions of the objects between the cameras. To address these prob- lems, this work, therefore, proposes a new Dynamic Graph Model with Link Prediction (DyGLIP) approach 1 to solve the data association task. Compared to existing methods, our new model offers several advantages, including bet- ter feature representations and the ability to recover from lost tracks during camera transitions. Moreover, our model works gracefully regardless of the overlapping ratios be- tween the cameras. Experimental results show that we out- perform existing MC-MOT algorithms by a large margin on several practical datasets. Notably, our model works favor- ably on online settings but can be extended to an incremen- tal approach for large-scale datasets.",
        "authors": [
            "Kha Gia Quach",
            "Pha Nguyen",
            "Huu Le",
            "Thanh Dat Truong",
            "Chi Nhan Duong",
            "Minh Triet Tran",
            "Khoa Luu",
            "VNU HCM"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Quach_DyGLIP_A_Dynamic_Graph_Model_With_Link_Prediction_for_Accurate_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 358,
        "title": "Prioritized Architecture Sampling with Monto-Carlo Tree Search",
        "abstract": "One-shot neural architecture search (NAS) methods sig- nificantly reduce the search cost by considering the whole search space as one network, which only needs to be trained once. However, current methods select each operation in- dependently without considering previous layers. Besides, the historical information obtained with huge computation costs is usually used only once and then discarded. In this paper, we introduce a sampling strategy based on Monte Carlo tree search (MCTS) with the search space modeled as a Monte Carlo tree (MCT), which captures the depen- dency among layers. Furthermore, intermediate results are stored in the MCT for future decisions and a better exploration-exploitation balance. Concretely, MCT is up- dated using the training loss as a reward to the architec- ture performance; for accurately evaluating the numerous nodes, we propose node communication and hierarchical node selection methods in the training and search stages, respectively, making better uses of the operation rewards and hierarchical information. Moreover, for a fair com- parison of different NAS methods, we construct an open- source NAS benchmark of a macro search space evaluated on CIFAR-10, namely NAS-Bench-Macro. Extensive ex- periments on NAS-Bench-Macro and ImageNet demonstrate that our method significantly improves search efficiency and performance. For example, by only searching 20 architec- tures, our obtained architecture achieves 78.0% top-1 ac- curacy with 442M FLOPs on ImageNet. Code (Benchmark) is available at: https://github.com/xiusu/NAS- Bench-Macro.",
        "authors": [
            "Xiu Su",
            "Tao Huang",
            "Yanxi Li",
            "Shan You",
            "Fei Wang",
            "Chen Qian",
            "Changshui Zhang",
            "Chang Xu"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Su_Prioritized_Architecture_Sampling_With_Monto-Carlo_Tree_Search_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 359,
        "title": "Information Bottleneck Disentanglement for Identity Swapping",
        "abstract": "Improving the performance of face forgery detectors often requires more identity-swapped images of higher- quality. One core objective of identity swapping is to gener- ate identity-discriminative faces that are distinct from the target while identical to the source. To this end, prop- erly disentangling identity and identity-irrelevant informa- tion is critical and remains a challenging endeavor. In this work, we propose a novel information disentangling and swapping network, called InfoSwap, to extract the most expressive information for identity representation from a pre-trained face recognition model. The key insight of our method is to formulate the learning of disentangled repre- sentations as optimizing an information bottleneck trade- off, in terms of finding an optimal compression of the pre- trained latent features. Moreover, a novel identity con- trastive loss is proposed for further disentanglement by re- quiring a proper distance between the generated identity and the target. While the most prior works have focused on using various loss functions to implicitly guide the learn- ing of representations, we demonstrate that our model can provide explicit supervision for learning disentangled rep- resentations, achieving impressive performance in generat- ing more identity-discriminative swapped faces.",
        "authors": [
            "Gege Gao",
            "Huaibo Huang",
            "Chaoyou Fu",
            "Zhaoyang Li",
            "Ran He"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Gao_Information_Bottleneck_Disentanglement_for_Identity_Swapping_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 360,
        "title": "Invertible Image Signal Processing",
        "abstract": "Unprocessed RAW data is a highly valuable image for- mat for image editing and computer vision. However, since the file size of RAW data is huge, most users can only get ac- cess to processed and compressed sRGB images. To bridge this gap, we design an Invertible Image Signal Processing (InvISP) pipeline, which not only enables rendering visually appealing sRGB images but also allows recovering nearly perfect RAW data. Due to our framework's inherent re- versibility, we can reconstruct realistic RAW data instead of synthesizing RAW data from sRGB images without any memory overhead. We also integrate a differentiable JPEG compression simulator that empowers our framework to re- construct RAW data from JPEG images. Extensive quan- titative and qualitative experiments on two DSLR demon- strate that our method obtains much higher quality in both rendered sRGB images and reconstructed RAW data than alternative methods.",
        "authors": [
            "Yazhou Xing",
            "Zian Qian",
            "Qifeng Chen"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Xing_Invertible_Image_Signal_Processing_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 361,
        "title": "You Only Look One-level Feature",
        "abstract": "This paper revisits feature pyramids networks (FPN) for one-stage detectors and points out that the success of FPN is due to its divide-and-conquer solution to the optimiza- tion problem in object detection rather than multi-scale fea- ture fusion. From the perspective of optimization, we in- troduce an alternative way to address the problem instead of adopting the complex feature pyramids - utilizing only one-level feature for detection. Based on the simple and ef- ficient solution, we present You Only Look One-level Fea- ture (YOLOF). In our method, two key components, Di- lated Encoder and Uniform Matching, are proposed and bring considerable improvements. Extensive experiments on the COCO benchmark prove the effectiveness of the pro- posed model. Our YOLOF achieves comparable results with its feature pyramids counterpart RetinaNet while be- ing 2.5\u00d7 faster. Without transformer layers, YOLOF can match the performance of DETR in a single-level feature manner with 7\u00d7 less training epochs. Code is available at https://github.com/megvii-model/YOLOF.",
        "authors": [
            "Qiang Chen",
            "Yingming Wang",
            "Tong Yang",
            "Xiangyu Zhang",
            "Jian Cheng",
            "Jian Sun"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Chen_You_Only_Look_One-Level_Feature_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 362,
        "title": "Exploiting Semantic Embedding and Visual Feature for Facial Action Unit Detection",
        "abstract": "Recent study on detecting facial action units (AU) has utilized auxiliary information (i.e., facial landmarks, rela- tionship among AUs and expressions, web facial images, etc.), in order to improve the AU detection performance. As of now, no semantic information of AUs has yet been ex- plored for such a task. As a matter of fact, AU semantic de- scriptions provide much more information than the binary AU labels alone, thus we propose to exploit the Semantic Embedding and Visual feature (SEV-Net) for AU detec- tion. More specifically, AU semantic embeddings are ob- tained through both Intra-AU and Inter-AU attention mod- ules, where the Intra-AU attention module captures the rela- tion among words within each sentence that describes indi- vidual AU, and the Inter-AU attention module focuses on the relation among those sentences. The learned AU semantic embeddings are then used as guidance for the generation of attention maps through a cross-modality attention network. The generated cross-modality attention maps are further used as weights for the aggregated feature. Our proposed method is unique in that the semantic features are exploited as the first of this kind. The approach has been evaluated on three public AU-coded facial expression databases, and has achieved a superior performance than the state-of-the-art peer methods.",
        "authors": [
            "Huiyuan Yang",
            "Lijun Yin",
            "Yi Zhou",
            "Jiuxiang Gu",
            " IBM"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Yang_Exploiting_Semantic_Embedding_and_Visual_Feature_for_Facial_Action_Unit_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 363,
        "title": "Improving OCR-based Image Captioning by Incorporating Geometrical Relationship",
        "abstract": "OCR-based image captioning aims to automatically de- scribe images based on all the visual entities (both visual objects and scene text) in images. Compared with conven- tional image captioning, the reasoning of scene text is re- quired for OCR-based image captioning since the gener- ated descriptions often contain multiple OCR tokens. Ex- isting methods attempt to achieve this goal via encoding the OCR tokens with rich visual and semantic representa- tions. However, strong correlations between OCR tokens may not be established with such limited representations. In this paper, we propose to enhance the connections between OCR tokens from the viewpoint of exploiting the geometri- cal relationship. We comprehensively consider the height, width, distance, IoU and orientation relations between the OCR tokens for constructing the geometrical relationship. To integrate the learned relation as well as the visual and semantic representations into a unified framework, a Long Short-Term Memory plus Relation-aware pointer network (LSTM-R) architecture is presented in this paper. Under the guidance of the geometrical relationship between OCR to- kens, our LSTM-R capitalizes on a newly-devised relation- aware pointer network to select OCR tokens from the scene text for OCR-based image captioning. Extensive experi- ments demonstrate the effectiveness of our LSTM-R. More remarkably, LSTM-R achieves state-of-the-art performance on TextCaps, with the CIDEr-D score being increased from 98.0% to 109.3%.",
        "authors": [
            "Jing Wang",
            "Jinhui Tang",
            "Mingkun Yang",
            "Xiang Bai"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Wang_Improving_OCR-Based_Image_Captioning_by_Incorporating_Geometrical_Relationship_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 364,
        "title": "Anomaly Detection in Video via Self-Supervised and Multi-Task Learning",
        "abstract": "Anomaly detection in video is a challenging computer vi- sion problem. Due to the lack of anomalous events at train- ing time, anomaly detection requires the design of learn- ing methods without full supervision. In this paper, we approach anomalous event detection in video through self- supervised and multi-task learning at the object level. We first utilize a pre-trained detector to detect objects. Then, we train a 3D convolutional neural network to produce dis- criminative anomaly-specific information by jointly learn- ing multiple proxy tasks: three self-supervised and one based on knowledge distillation. The self-supervised tasks are: (i) discrimination of forward/backward moving ob- jects (arrow of time), (ii) discrimination of objects in con- secutive/intermittent frames (motion irregularity) and (iii) reconstruction of object-specific appearance information. The knowledge distillation task takes into account both classification and detection information, generating large prediction discrepancies between teacher and student mod- els when anomalies occur. To the best of our knowledge, we are the first to approach anomalous event detection in video as a multi-task learning problem, integrating multi- ple self-supervised and knowledge distillation proxy tasks in a single architecture. Our lightweight architecture out- performs the state-of-the-art methods on three benchmarks: Avenue, ShanghaiTech and UCSD Ped2. Additionally, we perform an ablation study demonstrating the importance of integrating self-supervised learning and normality-specific distillation in a multi-task learning setting.",
        "authors": [
            "Mariana Iuliana Georgescu",
            "Radu Tudor Ionescu",
            "Fahad Shahbaz Khan",
            "Marius Popescu",
            "Mubarak Shah",
            "Abu Dhabi"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Georgescu_Anomaly_Detection_in_Video_via_Self-Supervised_and_Multi-Task_Learning_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 365,
        "title": "Magic Layouts: Structural Prior for Component Detection in User Interface Designs",
        "abstract": "We present Magic Layouts; a method for parsing screen- shots or hand-drawn sketches of user interface (UI) layouts. Our core contribution is to extend existing detectors to ex- ploit a learned structural prior for UI designs, enabling ro- bust detection of UI components; buttons, text boxes and similar. Specifically we learn a prior over mobile UI lay- outs, encoding common spatial co-occurrence relationships between different UI components. Conditioning region pro- posals using this prior leads to performance gains on UI layout parsing for both hand-drawn UIs and app screen- shots, which we demonstrate within the context an interac- tive application for rapidly acquiring digital prototypes of user experience (UX) designs.",
        "authors": [
            "Hailin Jin",
            "John Collomosse"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Manandhar_Magic_Layouts_Structural_Prior_for_Component_Detection_in_User_Interface_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 367,
        "title": "TediGAN: Text-Guided Diverse Face Image Generation and Manipulation",
        "abstract": "In this work, we propose TediGAN, a novel framework for multi-modal image generation and manipulation with textual descriptions. The proposed method consists of three components: StyleGAN inversion module, visual-linguistic similarity learning, and instance-level optimization. The inversion module maps real images to the latent space of a well-trained StyleGAN. The visual-linguistic similar- ity learns the text-image matching by mapping the image and text into a common embedding space. The instance- level optimization is for identity preservation in manipu- lation. Our model can produce diverse and high-quality images with an unprecedented resolution at 10242. Us- ing a control mechanism based on style-mixing, our Tedi- GAN inherently supports image synthesis with multi-modal inputs, such as sketches or semantic labels, with or with- out instance guidance. To facilitate text-guided multi- modal synthesis, we propose the Multi-Modal CelebA-HQ, a large-scale dataset consisting of real face images and corresponding semantic segmentation map, sketch, and textual descriptions. Extensive experiments on the in- troduced dataset demonstrate the superior performance of our proposed method. Code and data are available at https://github.com/weihaox/TediGAN.",
        "authors": [
            "Weihao Xia",
            "Yujiu Yang",
            "Jing Hao Xue",
            "Baoyuan Wu"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Xia_TediGAN_Text-Guided_Diverse_Face_Image_Generation_and_Manipulation_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 368,
        "title": "Domain-Independent Dominance of Adaptive Methods",
        "abstract": "From a simplified analysis of adaptive methods, we de- rive AvaGrad, a new optimizer which outperforms SGD on vision tasks when its adaptability is properly tuned. We ob- serve that the power of our method is partially explained by a decoupling of learning rate and adaptability, greatly simplifying hyperparameter search. In light of this obser- vation, we demonstrate that, against conventional wisdom, Adam can also outperform SGD on vision tasks, as long as the coupling between its learning rate and adaptability is taken into account. In practice, AvaGrad matches the best results, as measured by generalization accuracy, de- livered by any existing optimizer (SGD or adaptive) across image classification (CIFAR, ImageNet) and character-level language modelling (Penn Treebank) tasks. When training GANs, AvaGrad improves upon existing optimizers.1",
        "authors": [
            "Pedro Savarese",
            "TTI Chicago",
            "David McAllester",
            "TTI Chicago",
            "Sudarshan Babu",
            "TTI Chicago",
            "Michael Maire"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Savarese_Domain-Independent_Dominance_of_Adaptive_Methods_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 369,
        "title": "Unbiased Mean Teacher for Cross-domain Object Detection",
        "abstract": "Cross-domain object detection is challenging, because object detection model is often vulnerable to data variance, especially to the considerable domain shift between two distinctive domains. In this paper, we propose a new Un- biased Mean Teacher (UMT) model for cross-domain ob- ject detection. We reveal that there often exists a consider- able model bias for the simple mean teacher (MT) model in cross-domain scenarios, and eliminate the model bias with several simple yet highly effective strategies. In par- ticular, for the teacher model, we propose a cross-domain distillation method for MT to maximally exploit the exper- tise of the teacher model. Moreover, for the student model, we alleviate its bias by augmenting training samples with pixel-level adaptation. Finally, for the teaching process, we employ an out-of-distribution estimation strategy to select samples that most fit the current model to further enhance the cross-domain distillation process. By tackling the model bias issue with these strategies, our UMT model achieves mAPs of 44.1%, 58.1%, 41.7%, and 43.1% on bench- mark datasets Clipart1k, Watercolor2k, Foggy Cityscapes, and Cityscapes, respectively, which outperforms the exist- ing state-of-the-art results in notable margins. Our imple- mentation is available at https://github.com/kinredon/umt.",
        "authors": [
            "Jinhong Deng",
            "Wen Li",
            "Yuhua Chen",
            "Lixin Duan"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Deng_Unbiased_Mean_Teacher_for_Cross-Domain_Object_Detection_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 370,
        "title": "COMPLETER: Incomplete Multi-view Clustering via Contrastive Prediction",
        "abstract": "In this paper, we study two challenging problems in in- complete multi-view clustering analysis, namely, i) how to learn an informative and consistent representation among different views without the help of labels and ii) how to re- cover the missing views from data. To this end, we propose a novel objective that incorporates representation learning and data recovery into a unified framework from the view of information theory. To be specific, the informative and consistent representation is learned by maximizing the mu- tual information across different views through contrastive learning, and the missing views are recovered by minimiz- ing the conditional entropy of different views through dual prediction. To the best of our knowledge, this could be the first work to provide a theoretical framework that unifies the consistent representation learning and cross-view data re- covery. Extensive experimental results show the proposed method remarkably outperforms 10 competitive multi-view clustering methods on four challenging datasets. The code is available at https://pengxi.me.",
        "authors": [
            "Yijie Lin",
            "Yuanbiao Gou",
            "Zitao Liu",
            "Boyun Li",
            "Jiancheng Lv",
            "Xi Peng"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Lin_COMPLETER_Incomplete_Multi-View_Clustering_via_Contrastive_Prediction_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 371,
        "title": "Generative PointNet: Deep Energy-Based Learning on Unordered Point Sets for 3D Generation, Reconstruction and Classification",
        "abstract": "We propose a generative model of unordered point sets, such as point clouds, in the forms of an energy-based model, where the energy function is parameterized by an input- permutation-invariant bottom-up neural network. The en- ergy function learns a coordinate encoding of each point and then aggregates all individual point features into an energy for the whole point cloud. We call our model the Generative PointNet because it can be derived from the dis- criminative PointNet. Our model can be trained by MCMC- based maximum likelihood learning (as well as its vari- ants), without the help of any assisting networks like those in GANs and VAEs. Unlike most point cloud generators that rely on hand-crafted distance metrics, our model does not require any hand-crafted distance metric for the point cloud generation, because it synthesizes point clouds by matching observed examples in terms of statistical properties defined by the energy function. Furthermore, we can learn a short- run MCMC toward the energy-based model as a flow-like generator for point cloud reconstruction and interpolation. The learned point cloud representation can be useful for point cloud classification. Experiments demonstrate the ad- vantages of the proposed generative model of point clouds.",
        "authors": [
            "Jianwen Xie ",
            "Yifei Xu ",
            "Zilong Zheng "
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Xie_Generative_PointNet_Deep_Energy-Based_Learning_on_Unordered_Point_Sets_for_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 372,
        "title": "Reconstructing 3D Human Pose by Watching Humans in the Mirror",
        "abstract": "In this paper, we introduce the new task of reconstruct- ing 3D human pose from a single image in which we can see the person and the person's image through a mirror. Compared to general scenarios of 3D pose estimation from a single view, the mirror reflection provides an additional view for resolving the depth ambiguity. We develop an optimization-based approach that exploits mirror symme- try constraints for accurate 3D pose reconstruction. We also provide a method to estimate the surface normal of the mirror from vanishing points in the single image. To validate the proposed approach, we collect a large-scale dataset named Mirrored-Human, which covers a large vari- ety of human subjects, poses and backgrounds. The experi- ments demonstrate that, when trained on Mirrored-Human with our reconstructed 3D poses as pseudo ground-truth, the accuracy and generalizability of existing single-view 3D pose estimators can be largely improved. The code and dataset are available at https://zju3dv.github. io/Mirrored-Human/.",
        "authors": [
            "Qi Fang",
            "Qing Shuai",
            "Junting Dong",
            "Hujun Bao",
            "Xiaowei Zhou"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Fang_Reconstructing_3D_Human_Pose_by_Watching_Humans_in_the_Mirror_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 373,
        "title": "Learning Cross-Modal Retrieval with Noisy Labels",
        "abstract": "Recently, cross-modal retrieval is emerging with the help of deep multimodal learning. However, even for unimodal data, collecting large-scale well-annotated data is expen- sive and time-consuming, and not to mention the additional challenges from multiple modalities. Although crowd- sourcing annotation, e.g., Amazon's Mechanical Turk, can be utilized to mitigate the labeling cost, but leading to the unavoidable noise in labels for the non-expert annotating. To tackle the challenge, this paper presents a general Multi- modal Robust Learning framework (MRL) for learning with multimodal noisy labels to mitigate noisy samples and cor- relate distinct modalities simultaneously. To be specific, we propose a Robust Clustering loss (RC) to make the deep networks focus on clean samples instead of noisy ones. Be- sides, a simple yet effective multimodal loss function, called Multimodal Contrastive loss (MC), is proposed to maxi- mize the mutual information between different modalities, thus alleviating the interference of noisy samples and cross- modal discrepancy. Extensive experiments are conducted on four widely-used multimodal datasets to demonstrate the effectiveness of the proposed approach by comparing to 14 state-of-the-art methods.",
        "authors": [
            "Peng Hu",
            "Xi Peng",
            "Hongyuan Zhu",
            "Liangli Zhen",
            "Jie Lin",
            "Chengdu "
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Hu_Learning_Cross-Modal_Retrieval_With_Noisy_Labels_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 374,
        "title": "Fully Convolutional Networks for Panoptic Segmentation",
        "abstract": "In this paper, we present a conceptually simple, strong, and efficient framework for panoptic segmentation, called Panoptic FCN. Our approach aims to represent and pre- dict foreground things and background stuff in a unified fully convolutional pipeline. In particular, Panoptic FCN encodes each object instance or stuff category into a spe- cific kernel weight with the proposed kernel generator and produces the prediction by convolving the high-resolution feature directly. With this approach, instance-aware and semantically consistent prosperties for things and stuff can be respectively satisfied in a simple generate-kernel-then- segment workflow. Without extra boxes for localization or instance separation, the proposed approach outperforms previous box-based and -free models with high efficiency on COCO, Cityscapes, and Mapillary Vistas datasets with single scale input. Our code is made publicly available at https://github.com/Jia-Research-Lab/PanopticFCN.1",
        "authors": [
            "Yanwei Li",
            "Hengshuang Zhao",
            "Xiaojuan Qi",
            "Liwei Wang",
            "Zeming Li",
            "Jian Sun",
            "Jiaya Jia"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Li_Fully_Convolutional_Networks_for_Panoptic_Segmentation_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 375,
        "title": "A functional approach to rotation equivariant non-linearities for Tensor Field Networks.",
        "abstract": "Learning pose invariant representation is a fundamental problem in shape analysis. Most existing deep learning al- gorithms for 3D shape analysis are not robust to rotations and are often trained on synthetic datasets consisting of pre-aligned shapes, yielding poor generalization to unseen poses. This observation motivates a growing interest in ro- tation invariant and equivariant methods. The field of rota- tion equivariant deep learning is developing in recent years thanks to a well established theory of Lie group representa- tions and convolutions. A fundamental problem in equivari- ant deep learning is to design activation functions which are both informative and preserve equivariance. The recently introduced Tensor Field Network (TFN) framework pro- vides a rotation equivariant network design for point cloud analysis. TFN features undergo a rotation in feature space given a rotation of the input pointcloud. TFN and similar designs consider nonlinearities which operate only over ro- tation invariant features such as the norm of equivariant features to preserve equivariance, making them unable to capture the directional information. In a recent work en- titled \"Gauge Equivariant Mesh CNNs: Anisotropic Con- volutions on Geometric Graphs\" Hann et al. interpret 2D rotation equivariant features as Fourier coefficients of func- tions on the circle. In this work we transpose the idea of Hann et al. to 3D by interpreting TFN features as spher- ical harmonics coefficients of functions on the sphere. We introduce a new equivariant nonlinearity and pooling for TFN. We show improvments over the original TFN design and other equivariant nonlinearities in classification and segmentation tasks. Furthermore our method is competi- tive with state of the art rotation invariant methods in some instances.",
        "authors": [
            "Adrien Poulenard",
            "Leonidas J Guibas"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Poulenard_A_Functional_Approach_to_Rotation_Equivariant_Non-Linearities_for_Tensor_Field_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 376,
        "title": "Deep Denoising of Flash and No-Flash Pairs for Photography in Low-Light Environments",
        "abstract": "We introduce a neural network-based method to denoise pairs of images taken in quick succession, with and with- out a flash, in low-light environments. Our goal is to pro- duce a high-quality rendering of the scene that preserves the color and mood from the ambient illumination of the noisy no-flash image, while recovering surface texture and detail revealed by the flash. Our network outputs a gain map and a field of kernels, the latter obtained by linearly mixing elements of a per-image low-rank kernel basis. We first apply the kernel field to the no-flash image, and then multiply the result with the gain map to create the final output. We show our network effectively learns to produce high-quality images by combining a smoothed out estimate of the scene's ambient appearance from the no-flash image, with high-frequency albedo details extracted from the flash input. Our experiments show significant improvements over alternative captures without a flash, and baseline denoisers that use flash no-flash pairs. In particular, our method pro- duces images that are both noise-free and contain accurate ambient colors without the sharp shadows or strong specu- lar highlights visible in the flash image.",
        "authors": [
            "Zhihao Xia",
            "Micha l Gharbi",
            "Federico Perazzi",
            "Kalyan Sunkavalli",
            "Ayan Chakrabarti"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Xia_Deep_Denoising_of_Flash_and_No-Flash_Pairs_for_Photography_in_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 377,
        "title": "Rich Context Aggregation with Reflection Prior for Glass Surface Detection",
        "abstract": "Glass surfaces appear everywhere. Their existence can however pose a serious problem to computer vision tasks. Recently, a method is proposed to detect glass surfaces by learning multi-scale contextual information. However, as it is only based on a general context integration operation and does not consider any specific glass surface properties, it gets confused when the images contain objects that are sim- ilar to glass surfaces and degenerates in challenging scenes with insufficient contexts. We observe that humans often rely on identifying reflections in order to sense the existence of glass and on locating the boundary in order to deter- mine the extent of the glass. Hence, we propose a model for glass surface detection, which consists of two novel mod- ules: (1) a rich context aggregation module (RCAM) to ex- tract multi-scale boundary features from rich context fea- tures for locating glass surface boundaries of different sizes and shapes, and (2) a reflection-based refinement module (RRM) to detect reflection and then incorporate it so as to differentiate glass regions from non-glass regions. In ad- dition, we also propose a challenging dataset consisting of 4,012 glass images with annotations for glass surface detec- tion. Our experiments demonstrate that the proposed model outperforms state-of-the-art methods from relevant fields.",
        "authors": [
            "Jiaying Lin",
            "Zebang He"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Lin_Rich_Context_Aggregation_With_Reflection_Prior_for_Glass_Surface_Detection_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 378,
        "title": "Skip-Convolutions for Efficient Video Processing",
        "abstract": "We propose Skip-Convolutions to leverage the large amount of redundancies in video streams and save compu- tations. Each video is represented as a series of changes across frames and network activations, denoted as residu- als. We reformulate standard convolution to be efficiently computed on residual frames: each layer is coupled with a binary gate deciding whether a residual is important to the model prediction, e.g. foreground regions, or it can be safely skipped, e.g. background regions. These gates can ei- ther be implemented as an efficient network trained jointly with convolution kernels, or can simply skip the residuals based on their magnitude. Gating functions can also in- corporate block-wise sparsity structures, as required for ef- ficient implementation on hardware platforms. By replac- ing all convolutions with Skip-Convolutions in two state-of- the-art architectures, namely EfficientDet and HRNet, we reduce their computational cost consistently by a factor of 3 \u223c 4\u00d7 for two different tasks, without any accuracy drop. Extensive comparisons with existing model compression, as well as image and video efficiency methods demonstrate that Skip-Convolutions set a new state-of-the-art by effec- tively exploiting the temporal redundancies in videos.",
        "authors": [
            "Amirhossein Habibian",
            "Davide Abati",
            "Taco S Cohen",
            "Babak Ehteshami Bejnordi"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Habibian_Skip-Convolutions_for_Efficient_Video_Processing_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 379,
        "title": "Scalability vs. Utility: Do We Have to Sacrifice One for the Other in Data Importance Quantification?",
        "abstract": "Quantifying the importance of each training point to a learning task is a fundamental problem in machine learning and the estimated importance scores have been leveraged to guide a range of data workflows such as data summa- rization and domain adaption. One simple idea is to use the leave-one-out error of each training point to indicate its importance. Recent work has also proposed to use the Shap- ley value, as it defines a unique value distribution scheme that satisfies a set of appealing properties. However, cal- culating Shapley values is often expensive, which limits its applicability in real-world applications at scale. Multiple heuristics to improve the scalability of calculating Shapley values have been proposed recently, with the potential risk of compromising their utility in real-world applications. How well do existing data quantification methods per- form on existing workflows? How do these methods compare with each other, empirically and theoretically? Must we sacrifice scalability for the utility in these workflows when using these methods? In this paper, we conduct a novel theo- retical analysis comparing the utility of different importance quantification methods, and report extensive experimental studies on existing and proposed workflows such as noisy la- bel detection, watermark removal, data summarization, data acquisition, and domain adaptation. We show that Shapley value approximation based on a KNN surrogate over pre- trained feature embeddings obtains comparable utility with existing algorithms while achieving significant scalability improvement, often by orders of magnitude. Our theoretical analysis also justifies its advantage over the leave-one-out error. The code is available at https://github.com/AI- secure/Shapley-Study.",
        "authors": [
            "Ruoxi Jia",
            "Fan Wu",
            "Xuehui Sun",
            "Jiacen Xu",
            "David Dao",
            "Bhavya Kailkhura",
            "Ce Zhang",
            "Bo Li",
            "Dawn Song",
            "Virginia Tech"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Jia_Scalability_vs._Utility_Do_We_Have_To_Sacrifice_One_for_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 380,
        "title": "t-vMF Similarity For Regularizing Intra-Class Feature Distribution",
        "abstract": "Deep convolutional neural networks (CNNs) leverage large-scale training dataset to produce remarkable perfor- mance on various image classification tasks. It, however, is difficult to effectively train the CNNs on some realis- tic learning situations such as regarding class imbalance, small-scale and label noises. Regularizing CNNs works well on learning with such deteriorated training datasets by mitigating overfitting issues. In this work, we propose a method to effectively impose regularization on feature rep- resentation learning. By focusing on the angle between a feature and a classifier which is embedded in cosine similar- ity at the classification layer, we formulate a novel similarity beyond the cosine based on von Mises-Fisher distribution of directional statistics. In contrast to the cosine similar- ity, our similarity is compact while having heavy tail, which contributes to regularizing intra-class feature distribution to improve generalization performance. Through the exper- iments on some realistic learning situations such as of im- balance, small-scale and noisy labels, we demonstrate the effectiveness of the proposed method for training CNNs, in comparison to the other regularization methods. Codes are available at https://github.com/tk1980/tvMF.",
        "authors": [
            "Takumi Kobayashi"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Kobayashi_T-vMF_Similarity_for_Regularizing_Intra-Class_Feature_Distribution_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 381,
        "title": "OBoW: Online Bag-of-Visual-Words Generation for Self-Supervised Learning",
        "abstract": "Learning image representations without human super- vision is an important and active research field. Several recent approaches have successfully leveraged the idea of making such a representation invariant under different types of perturbations, especially via contrastive-based instance discrimination training. Although effective visual represen- tations should indeed exhibit such invariances, there are other important characteristics, such as encoding contextual reasoning skills, for which alternative reconstruction-based approaches might be better suited. With this in mind, we propose a teacher-student scheme to learn representations by training a convolutional net to reconstruct a bag-of-visual-words (BoW) representation of an image, given as input a perturbed version of that same image. Our strategy performs an online training of both the teacher network (whose role is to generate the BoW targets) and the student network (whose role is to learn represen- tations), along with an online update of the visual-words vocabulary (used for the BoW targets). This idea effectively enables fully online BoW-guided unsupervised learning. Ex- tensive experiments demonstrate the interest of our BoW- based strategy, which surpasses previous state-of-the-art methods (including contrastive-based ones) in several ap- plications. For instance, in downstream tasks such Pascal object detection, Pascal classification and Places205 clas- sification, our method improves over all prior unsupervised approaches, thus establishing new state-of-the-art results that are also significantly better even than those of super- vised pre-training. We provide the implementation code at https://github.com/valeoai/obow.",
        "authors": [
            "Spyros Gidaris",
            "Gilles Puy",
            "Nikos Komodakis",
            "Matthieu Cord",
            "Patrick P rez",
            "valeo ai",
            "Sorbonne Universit"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Gidaris_OBoW_Online_Bag-of-Visual-Words_Generation_for_Self-Supervised_Learning_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 382,
        "title": "Unveiling the Potential of Structure Preserving for Weakly Supervised Object Localization",
        "abstract": "Weakly supervised object localization (WSOL) remains an open problem given the deficiency of finding object extent information using a classification network. Although prior works struggled to localize objects through various spatial regularization strategies, we argue that how to extract object structural information from the trained classification network is neglected. In this paper, we propose a two-stage approach, termed structure-preserving activation (SPA), to- ward fully leveraging the structure information incorporated in convolutional features for WSOL. First, a restricted acti- vation module (RAM) is designed to alleviate the structure- missing issue caused by the classification network on the ba- sis of the observation that the unbounded classification map and global average pooling layer drive the network to focus only on object parts. Second, we designed a post-process approach, termed self-correlation map generating (SCG) module to obtain structure-preserving localization maps on the basis of the activation maps acquired from the first stage. Specifically, we utilize the high-order self-correlation (HSC) to extract the inherent structural information retained in the learned model and then aggregate HSC of multiple points for precise object localization. Extensive experiments on two publicly available benchmarks including CUB-200- 2011 and ILSVRC show that the proposed SPA achieves substantial and consistent performance gains compared with baseline approaches. Code and models are available at github.com/Panxjia/SPA CVPR2021.",
        "authors": [
            "Xingjia Pan",
            "Yingguo Gao",
            "Weiming Dong",
            "Haolei Yuan",
            "Feiyue Huang",
            "Changsheng Xu"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Pan_Unveiling_the_Potential_of_Structure_Preserving_for_Weakly_Supervised_Object_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 383,
        "title": "Denoise and Contrast for Category Agnostic Shape Completion",
        "abstract": "In this paper, we present a deep learning model that ex- ploits the power of self-supervision to perform 3D point cloud completion, estimating the missing part and a context region around it. Local and global information are encoded in a combined embedding. A denoising pretext task provides the network with the needed local cues, decoupled from the high-level semantics and naturally shared over multi- ple classes. On the other hand, contrastive learning max- imizes the agreement between variants of the same shape with different missing portions, thus producing a represen- tation which captures the global appearance of the shape. The combined embedding inherits category-agnostic prop- erties from the chosen pretext tasks. Differently from exist- ing approaches, this allows to better generalize the comple- tion properties to new categories unseen at training time. Moreover, while decoding the obtained joint representa- tion, we better blend the reconstructed missing part with the partial shape by paying attention to its known surround- ing region and reconstructing this frame as auxiliary objec- tive. Our extensive experiments and detailed ablation on the ShapeNet dataset show the effectiveness of each part of the method with new state of the art results. Our quantitative and qualitative analysis confirms how our approach is able to work on novel categories without relying neither on clas- sification and shape symmetry priors, nor on adversarial training procedures.",
        "authors": [
            "Antonio Alliegro",
            "Diego Valsesia",
            "Giulia Fracastoro",
            "Enrico Magli",
            "Tatiana Tommasi",
            "Politecnico di Torino"
        ],
        "created_time": "",
        "conference": "CVPR",
        "filepath": "/Users/adit/papers/CVPR/Alliegro_Denoise_and_Contrast_for_Category_Agnostic_Shape_Completion_CVPR_2021_paper.pdf"
    },
    {
        "internal_id": 384,
        "title": "SnapMix: Semantically Proportional Mixing for Augmenting Fine-grained Data",
        "abstract": "Data mixing augmentation has proved effective in training deep models. Recent methods mix labels mainly based on the mixture proportion of image pixels. As the main discrimi- native information of a fine-grained image usually resides in subtle regions, methods along this line are prone to heavy la- bel noise in fine-grained recognition. We propose in this paper a novel scheme, termed as Semantically Proportional Mixing (SnapMix), which exploits class activation map (CAM) to lessen the label noise in augmenting fine-grained data. Snap- Mix generates the target label for a mixed image by estimat- ing its intrinsic semantic composition, and allows for asym- metric mixing operations and ensures semantic correspon- dence between synthetic images and target labels. Experi- ments show that our method consistently outperforms exist- ing mixed-based approaches on various datasets and under different network depths. Furthermore, by incorporating the mid-level features, the proposed SnapMix achieves top-level performance, demonstrating its potential to serve as a solid baseline for fine-grained recognition.",
        "authors": [
            "Shaoli Huang"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/SnapMixSemanticallyProportionalMixingforAugmentingFinegrainedData.pdf"
    },
    {
        "internal_id": 385,
        "title": "DeepWriteSYN: On-Line Handwriting Synthesis via Deep Short-Term Representations",
        "abstract": "This study proposes DeepWriteSYN, a novel on-line hand- writing synthesis approach via deep short-term representa- tions. It comprises two modules: i) an optional and inter- changeable temporal segmentation, which divides the hand- writing into short-time segments consisting of individual or multiple concatenated strokes; and ii) the on-line synthesis of those short-time handwriting segments, which is based on a sequence-to-sequence Variational Autoencoder (VAE). The main advantages of the proposed approach are that the syn- thesis is carried out in short-time segments (that can run from a character fraction to full characters) and that the VAE can be trained on a configurable handwriting dataset. These two properties give a lot of flexibility to our synthesiser, e.g., as shown in our experiments, DeepWriteSYN can generate real- istic handwriting variations of a given handwritten structure corresponding to the natural variation within a given popu- lation or a given subject. These two cases are developed ex- perimentally for individual digits and handwriting signatures, respectively, achieving in both cases remarkable results. Also, we provide experimental results for the task of on-line signature verification showing the high potential of Deep- WriteSYN to improve significantly one-shot learning scenar- ios. To the best of our knowledge, this is the first synthesis approach capable of generating realistic on-line handwriting in the short term (including handwritten signatures) via deep learning. This can be very useful as a module toward long- term realistic handwriting generation either completely syn- thetic or as natural variation of given handwriting samples.",
        "authors": [
            "Ruben Tolosana",
            "Ruben Vera Rodriguez",
            " Julian Fierrez"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/DeepWriteSYNOnLineHandwritingSynthesisviaDeepShortTermRepresentations.pdf"
    },
    {
        "internal_id": 386,
        "title": "Sketch Generation with Drawing Process Guided by Vector Flow and Grayscale",
        "abstract": "We propose a novel image-to-pencil translation method that could not only generate high-quality pencil sketches but also offer the drawing process. Existing pencil sketch algorithms are based on texture rendering rather than the direct imitation of strokes, making them unable to show the drawing process but only a final result. To address this challenge, we first establish a pencil stroke imitation mechanism. Next, we develop a framework with three branches to guide stroke drawing: the first branch guides the direction of the strokes, the second branch determines the shade of the strokes, and the third branch enhances the details further. Under this framework's guidance, we can produce a pencil sketch by drawing one stroke every time. Our method is fully interpretable. Comparison with existing pencil drawing algorithms shows that our method is superior to others in terms of texture quality, style, and user evaluation. Our code and supplementary material are now available at: https://github.com/TZYSJTU/Sketch-Generation-with- Drawing-Process-Guided-by-Vector-Flow-and-Grayscale",
        "authors": [
            "Zhengyan Tong",
            " Xuanhong Chen",
            " Bingbing Ni",
            " Huawei Hisilicon"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/SketchGenerationwithDrawingProcessGuidedbyVectorFlowandGrayscale.pdf"
    },
    {
        "internal_id": 387,
        "title": "Predicting Livelihood Indicators from Community-Generated Street-Level Imagery",
        "abstract": "Major decisions from governments and other large organiza- tions rely on measurements of the populace's well-being, but making such measurements at a broad scale is expensive and thus infrequent in much of the developing world. We pro- pose an inexpensive, scalable, and interpretable approach to predict key livelihood indicators from public crowd-sourced street-level imagery. Such imagery can be cheaply collected and more frequently updated compared to traditional survey- ing methods, while containing plausibly relevant informa- tion for a range of livelihood indicators. We propose two ap- proaches to learn from the street-level imagery: (1) a method that creates multi-household cluster representations by de- tecting informative objects and (2) a graph-based approach that captures the relationships between images. By visual- izing what features are important to a model and how they are used, we can help end-user organizations understand the models and offer an alternate approach for index estimation that uses cheaply obtained roadway features. By compar- ing our results against ground data collected in nationally- representative household surveys, we demonstrate the perfor- mance of our approach in accurately predicting indicators of poverty, population, and health and its scalability by testing in two different countries, India and Kenya. Our code is avail- able at https://github.com/sustainlab-group/mapillarygcn.",
        "authors": [
            "Jihyeon Lee",
            " Dylan Grosz",
            " Burak Uzkent",
            " Sicheng Zeng",
            "Marshall Burke",
            " David Lobell"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/PredictingLivelihoodIndicatorsfromCommunityGeneratedStreetLevelImagery.pdf"
    },
    {
        "internal_id": 388,
        "title": "An Analysis of Approval-Based Committee Rules for 2D-Euclidean Elections",
        "abstract": "We study approval-based committee elections for the case where the voters' preferences come from a 2D-Euclidean model. We consider two main issues: First, we ask for the complexity of computing election results. Second, we eval- uate election outcomes experimentally, following the visual- ization technique of Elkind et al. (2017). Regarding the first issue, we find that many NP-hard rules remain intractable for 2D-Euclidean elections. For the second one, we observe that the behavior and nature of many rules strongly depend on the exact protocol for choosing the approved candidates.",
        "authors": [
            "Micha T Godziszewski",
            " Pawe Batko",
            " Piotr Skowron",
            " Piotr Faliszewski",
            "Krak ow"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/AnAnalysisofApprovalBasedCommitteeRulesfor2DEuclideanElections.pdf"
    },
    {
        "internal_id": 389,
        "title": "Deep Just-In-Time Inconsistency Detection Between Comments and Source Code",
        "abstract": "Natural language comments convey key aspects of source code such as implementation, usage, and pre- and post- conditions. Failure to update comments accordingly when the corresponding code is modified introduces inconsistencies, which is known to lead to confusion and software bugs. In this paper, we aim to detect whether a comment becomes in- consistent as a result of changes to the corresponding body of code, in order to catch potential inconsistencies just-in-time, i.e., before they are committed to a code base. To achieve this, we develop a deep-learning approach that learns to correlate a comment with code changes. By evaluating on a large corpus of comment/code pairs spanning various comment types, we show that our model outperforms multiple baselines by sig- nificant margins. For extrinsic evaluation, we show the use- fulness of our approach by combining it with a comment up- date model to build a more comprehensive automatic com- ment maintenance system which can both detect and resolve inconsistent comments based on code changes.",
        "authors": [
            "Sheena Panthaplackel",
            "Junyi Jessy Li",
            "Milos Gligoric",
            "Raymond J Mooney"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/DeepJustInTimeInconsistencyDetectionBetweenCommentsandSourceCode.pdf"
    },
    {
        "internal_id": 390,
        "title": "Symbolic Music Generation with Transformer-GANs",
        "abstract": "Autoregressive models using Transformers have emerged as the dominant approach for music generation with the goal of synthesizing minute-long compositions that exhibit large- scale musical structure. These models are commonly trained by minimizing the negative log-likelihood (NLL) of the ob- served sequence in an autoregressive manner. Unfortunately, the quality of samples from these models tends to degrade significantly for long sequences, a phenomenon attributed to exposure bias. Fortunately, we are able to detect these fail- ures with classifiers trained to distinguish between real and sampled sequences, an observation that motivates our explo- ration of adversarial losses to complement the NLL objective. We use a pre-trained Span-BERT model for the discriminator of the GAN, which in our experiments helped with training stability. We use the Gumbel-Softmax trick to obtain a differ- entiable approximation of the sampling process. This makes discrete sequences amenable to optimization in GANs. In ad- dition, we break the sequences into smaller chunks to ensure that we stay within a given memory budget. We demonstrate via human evaluations and a new discriminative metric that the music generated by our approach outperforms a baseline trained with likelihood maximization, the state-of-the-art Mu- sic Transformer, and other GANs used for sequence genera- tion. 57% of people prefer music generated via our approach while 43% prefer Music Transformer.",
        "authors": [
            "Aashiq Muhamed",
            "Liang Li",
            "Xingjian Shi",
            "Suri Yaddanapudi",
            "Wayne Chi",
            "Dylan Jackson",
            "Rahul Suresh",
            "Zachary C Lipton"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/SymbolicMusicGenerationwithTransformerGANs.pdf"
    },
    {
        "internal_id": 391,
        "title": "Hierarchically and Cooperatively Learning Traffic Signal Control",
        "abstract": "Deep reinforcement learning (RL) has been applied to traf- fic signal control recently and demonstrated superior perfor- mance to conventional control methods. However, there are still several challenges we have to address before fully apply- ing deep RL to traffic signal control. Firstly, the objective of traffic signal control is to optimize average travel time, which is a delayed reward in a long time horizon in the context of RL. However, existing work simplifies the optimization by using queue length, waiting time, delay, etc., as immedi- ate reward and presumes these short-term targets are always aligned with the objective. Nevertheless, these targets may deviate from the objective in different road networks with various traffic patterns. Secondly, it remains unsolved how to cooperatively control traffic signals to directly optimize av- erage travel time. To address these challenges, we propose a hierarchical and cooperative reinforcement learning method\u2013 HiLight. HiLight enables each agent to learn a high-level pol- icy that optimizes the objective locally by selecting among the sub-policies that respectively optimize short-term targets. Moreover, the high-level policy additionally considers the ob- jective in the neighborhood with adaptive weighting to en- courage agents to cooperate on the objective in the road net- work. Empirically, we demonstrate that HiLight outperforms state-of-the-art RL methods for traffic signal control in real road networks with real traffic.",
        "authors": [
            "Bingyu Xu",
            "Yaowei Wang",
            "Zhaozhi Wang",
            "Huizhu Jia",
            "Zongqing Lu"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/HierarchicallyandCooperativelyLearningTrafficSignalControl.pdf"
    },
    {
        "internal_id": 392,
        "title": "Physics-Informed Deep Learning for Traffic State Estimation: A Hybrid Paradigm Informed By Second-Order Traffic Models",
        "abstract": "Traffic state estimation (TSE) reconstructs the traffic vari- ables (e.g., density or average velocity) on road segments using partially observed data, which is important for traffic managements. Traditional TSE approaches mainly bifurcate into two categories: model-driven and data-driven, and each of them has shortcomings. To mitigate these limitations, hy- brid TSE methods, which combine both model-driven and data-driven, are becoming a promising solution. This paper introduces a hybrid framework, physics-informed deep learn- ing (PIDL), to combine second-order traffic flow models and neural networks to solve the TSE problem. PIDL can encode traffic flow models into deep neural networks to regularize the learning process to achieve improved data efficiency and es- timation accuracy. We focus on highway TSE with observed data from loop detectors and probe vehicles, using both den- sity and average velocity as the traffic variables. With numer- ical examples, we show the use of PIDL to solve a popular second-order traffic flow model, i.e., a Greenshields-based Aw-Rascle-Zhang (ARZ) model, and discover the model pa- rameters. We then evaluate the PIDL-based TSE method us- ing the Next Generation SIMulation (NGSIM) dataset. Ex- perimental results demonstrate the proposed PIDL-based ap- proach to outperform advanced baseline methods in terms of data efficiency and estimation accuracy.",
        "authors": [
            "Rongye Shi",
            "Zhaobin Mo",
            "Xuan Di",
            "New York"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/PhysicsInformedDeepLearningforTrafficStateEstimationAHybridParadigmInformedBySecondOrderTrafficModels.pdf"
    },
    {
        "internal_id": 393,
        "title": "Pragmatic Code Autocomplete",
        "abstract": "Human language is ambiguous, with intended meanings re- covered via pragmatic reasoning in context. Such reliance on context is essential for the efficiency of human communica- tion. Programming languages, in stark contrast, are defined by unambiguous grammars. In this work, we aim to make pro- gramming languages more concise by allowing programmers to utilize a controlled level of ambiguity. Specifically, we al- low single-character abbreviations for common keywords and identifiers. Our system first proposes a set of strings that can be abbreviated by the user. Using only 100 abbreviations, we observe that a corpus of Python code can be compressed by 15%, a number that can be improved even further by special- izing the abbreviations to a particular code base. We then use a contextualized sequence-to-sequence model to rank poten- tial expansions of inputs that include abbreviations. In an of- fline reconstruction task our model achieves accuracies rang- ing from 93% to 99%, depending on the programming lan- guage and user settings. The model is small enough to run on a commodity CPU in real-time. We evaluate the usability of our system in a user study, integrating it in Microsoft VS- Code, a popular code text editor. We observe that our system performs well and is complementary to traditional autocom- plete features.",
        "authors": [
            "Gabriel Poesia",
            "Noah Goodman",
            "CA "
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/PragmaticCodeAutocomplete.pdf"
    },
    {
        "internal_id": 394,
        "title": "Bringing UMAP Closer to the Speed of Light with GPU Acceleration",
        "abstract": "The Uniform Manifold Approximation and Projection (UMAP) algorithm has become widely popular for its ease of use, quality of results, and support for exploratory, unsuper- vised, supervised, and semi-supervised learning. While many algorithms can be ported to a GPU in a simple and direct fashion, such efforts have resulted in inefficient and inaccu- rate versions of UMAP. We show a number of techniques that can be used to make a faster and more faithful GPU version of UMAP, and obtain speedups of up to 100x in practice. Many of these design choices/lessons are general purpose and may inform the conversion of other graph and manifold learning algorithms to use GPUs. Our implementation has been made publicly available as part of the open source RAPIDS cuML library (https://github.com/rapidsai/cuml).",
        "authors": [
            "Corey J Nolet",
            "Victor Lafargue",
            "Edward Ra ",
            "Tim Oates",
            "John Zedlewski",
            "Joshua Patterson",
            "Booz Allen Hamilton"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/BringingUMAPClosertotheSpeedofLightwithGPUAcceleration.pdf"
    },
    {
        "internal_id": 395,
        "title": "The Causal Learning of Retail Delinquency",
        "abstract": "This paper focuses on the expected difference in borrower's repayment when there is a change in the lender's credit deci- sions. Classical estimators overlook the confounding effects and hence the estimation error can be magnificent. As such, we propose another approach to construct the estimators such that the error can be greatly reduced. The proposed estima- tors are shown to be unbiased, consistent, and robust through a combination of theoretical analysis and numerical testing. Moreover, we compare the power of estimating the causal quantities between the classical estimators and the proposed estimators. The comparison is tested across a wide range of models, including linear regression models, tree-based mod- els, and neural network-based models, under different simu- lated datasets that exhibit different levels of causality, differ- ent degrees of nonlinearity, and different distributional prop- erties. Most importantly, we apply our approaches to a large observational dataset provided by a global technology firm that operates in both the e-commerce and the lending busi- ness. We find that the relative reduction of estimation error is strikingly substantial if the causal effects are accounted for correctly.",
        "authors": [
            "Yiyan Huang",
            " Xing Yan",
            " Qi Wu",
            "Nanbo Peng",
            " Dongdong Wang",
            " Zhixiang Huang",
            "JD Digits"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/TheCausalLearningofRetailDelinquency.pdf"
    },
    {
        "internal_id": 396,
        "title": "Towards Balanced Defect Prediction with Better Information Propagation",
        "abstract": "Defect prediction, the task of predicting the presence of de- fects in source code artifacts, has broad application in soft- ware development. Defect prediction faces two major chal- lenges, label scarcity, where only a small percentage of code artifacts are labeled, and data imbalance, where the ma- jority of labeled artifacts are non-defective. Moreover, cur- rent defect prediction methods ignore the impact of informa- tion propagation among code artifacts, and this negligence leads to performance degradation. In this paper, we propose DPCAG, a novel model to address the above three issues. We treat code artifacts as nodes in a graph, and learn to propagate influence among neighboring nodes iteratively in an EM framework. DPCAG dynamically adjusts the contri- butions of each node and selects high-confidence nodes for data augmentation. Experimental results on real-world bench- mark datasets show that DPCAG improves performance com- pare to the state-of-the-art models. In particular, DPCAG achieves substantial performance superiority when measured by Matthews Correlation Coefficient (MCC), a metric that is widely acknowledged to be the most suitable for imbalanced data.",
        "authors": [
            " Huan Gao",
            " Yuncheng Hua"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/TowardsBalancedDefectPredictionwithBetterInformationPropagation.pdf"
    },
    {
        "internal_id": 397,
        "title": "Quantum Cognitively Motivated Decision Fusion for Video Sentiment Analysis",
        "abstract": "Video sentiment analysis as a decision-making process is in- herently complex, involving the fusion of decisions from mul- tiple modalities and the so-caused cognitive biases. Inspired by recent advances in quantum cognition, we show that the sentiment judgment from one modality could be incompat- ible with the judgment from another, i.e., the order matters and they cannot be jointly measured to produce a final de- cision. Thus the cognitive process exhibits \"quantum-like\" biases that cannot be captured by classical probability the- ories. Accordingly, we propose a fundamentally new, quan- tum cognitively motivated fusion strategy for predicting sen- timent judgments. In particular, we formulate utterances as quantum superposition states of positive and negative sen- timent judgments, and uni-modal classifiers as mutually in- compatible observables, on a complex-valued Hilbert space with positive-operator valued measures. Experiments on two benchmarking datasets illustrate that our model significantly outperforms various existing decision level and a range of state-of-the-art content-level fusion approaches. The results also show that the concept of incompatibility allows effective handling of all combination patterns, including those extreme cases that are wrongly predicted by all uni-modal classifiers.",
        "authors": [
            "Dimitris Gkoumas",
            "Qiuchi Li",
            "Shahram Dehdashti ",
            "Massimo Melucci",
            "Yijun Yu",
            "Dawei Song",
            "Milton Keynes"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/QuantumCognitivelyMotivatedDecisionFusionforVideoSentimentAnalysis.pdf"
    },
    {
        "internal_id": 398,
        "title": "Compound Word Transformer: Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs",
        "abstract": "To apply neural sequence models such as the Transformers to music generation tasks, one has to represent a piece of music by a sequence of tokens drawn from a finite set of pre-defined vocabulary. Such a vocabulary usually involves tokens of var- ious types. For example, to describe a musical note, one needs separate tokens to indicate the note's pitch, duration, velocity (dynamics), and placement (onset time) along the time grid. While different types of tokens may possess different proper- ties, existing models usually treat them equally, in the same way as modeling words in natural languages. In this paper, we present a conceptually different approach that explicitly takes into account the type of the tokens, such as note types and metric types. And, we propose a new Transformer decoder ar- chitecture that uses different feed-forward heads to model to- kens of different types. With an expansion-compression trick, we convert a piece of music to a sequence of compound words by grouping neighboring tokens, greatly reducing the length of the token sequences. We show that the resulting model can be viewed as a learner over dynamic directed hypergraphs. And, we employ it to learn to compose expressive Pop piano music of full-song length (involving up to 10K individual to- kens per song), both conditionally and unconditionally. Our experiment shows that, compared to state-of-the-art models, the proposed model converges 5 to 10 times faster at training (i.e., within a day on a single GPU with 11 GB memory), and with comparable quality in the generated music.",
        "authors": [
            "Wen Yi Hsiao",
            "Yin Cheng Yeh",
            "Academia Sinica"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/CompoundWordTransformerLearningtoComposeFullSongMusicoverDynamicDirectedHypergraphs.pdf"
    },
    {
        "internal_id": 399,
        "title": "Diagnose Like A Pathologist: Weakly-Supervised Pathologist-Tree Network for Slide-Level Immunohistochemical Scoring",
        "abstract": "The immunohistochemistry (IHC) test of biopsy tissue is crucial to develop targeted treatment and evaluate prognosis for cancer patients. The IHC staining slide is usually digitized into the whole-slide image (WSI) with gigapixels for quantitative image analysis. To perform a whole image prediction (e.g., IHC scoring, survival prediction, and cancer grading) from this kind of high-dimensional image, algorithms are often developed based on multi-instance learning (MIL) framework. However, the multi-scale infor- mation of WSI and the associations among instances are not well explored in existing MIL based studies. Inspired by the fact that pathologists jointly analyze visual fields at multiple powers of objective for diagnostic predictions, we propose a Pathologist-Tree Network (PTree-Net) to sparsely model the WSI efficiently in multi-scale manner. Specifically, we propose a Focal-Aware Module (FAM) that can approximately estimate diagnosis-related regions with an extractor trained using the thumbnail of WSI. With the initial diagnosis-related regions, we hierarchically model the multi-scale patches in a tree structure, where both the global and local information can be captured. To explore this tree structure in an end-to-end network, we propose a patch Relevance-enhanced Graph Convolutional Network (RGCN) to explicitly model the correlations of adjacent parent-child nodes, accompanied by patch relevance to exploit the implicit contextual information among distant nodes. In addition, tree-based self-supervision is devised to improve representation learning and suppress irrelevant instances adaptively. Extensive experiments are performed on a large-scale IHC HER2 dataset. The ablation study confirms the effectiveness of our design, and our approach outperforms state-of-the-art by a large margin.",
        "authors": [
            "Zhen Chen",
            "Jun Zhang",
            "Shuanlong Che",
            "Junzhou Huang",
            "Xiao Han",
            "Yixuan Yuan",
            "Hong Kong SAR",
            " KingMed Diagnostics"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/DiagnoseLikeAPathologistWeaklySupervisedPathologistTreeNetworkforSlideLevelImmunohistochemicalScoring.pdf"
    },
    {
        "internal_id": 400,
        "title": "Towards Efficient Selection of Activity Trajectories Based on Diversity and Coverage",
        "abstract": "With the prevalence of location based services, activity tra- jectories are being generated at a rapid pace. The activity tra- jectory data enriches traditional trajectory data with semantic activities of users, which not only shows where the users have been, but also the preference of users. However, the large vol- ume of data is expensive for people to explore. To address this issue, we study the problem of Diversity-aware Activ- ity Trajectory Selection (DaATS). Given a region of interest for a user, it finds a small number of representative activity trajectories that can provide the user with a broad coverage of different aspects of the region. The problem is challeng- ing in both the efficiency of trajectory similarity computation and subset selection. To tackle the two challenges, we pro- pose a novel solution by: (1) exploiting a deep metric learning method to speedup the similarity computation; and (2) prov- ing that DaATS is an NP-hard problem, and developing an efficient approximation algorithm with performance guaran- tees. Experiments on two real-world datasets show that our proposal significantly outperforms state-of-the-art baselines.",
        "authors": [
            "Chengcheng Yang",
            "Lisi Chen",
            "Hao Wang ",
            "Shuo Shang "
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/TowardsEfficientSelectionofActivityTrajectoriesbasedonDiversityandCoverage.pdf"
    },
    {
        "internal_id": 401,
        "title": "Oral-3D: Reconstructing the 3D Structure of Oral Cavity from Panoramic X-ray",
        "abstract": "Panoramic X-ray (PX) provides a 2D picture of the patient's mouth in a panoramic view to help dentists observe the in- visible disease inside the gum. However, it provides limited 2D information compared with cone-beam computed tomog- raphy (CBCT), another dental imaging method that gener- ates a 3D picture of the oral cavity but with more radiation dose and a higher price. Consequently, it is of great interest to reconstruct the 3D structure from a 2D X-ray image, which can greatly explore the application of X-ray imaging in den- tal surgeries. In this paper, we propose a framework, named Oral-3D, to reconstruct the 3D oral cavity from a single PX image and prior information of the dental arch. Specifically, we first train a generative model to learn the cross-dimension transformation from 2D to 3D. Then we restore the shape of the oral cavity with a deformation module with the dental arch curve, which can be obtained simply by taking a photo of the patient's mouth. To be noted, Oral-3D can restore both the density of bony tissues and the curved mandible surface. Experimental results show that Oral-3D can efficiently and effectively reconstruct the 3D oral structure and show criti- cal information in clinical applications, e.g., tooth pulling and dental implants. To the best of our knowledge, we are the first to explore this domain transformation problem between these two imaging methods.",
        "authors": [
            "Weinan Song",
            "Yuan Liang",
            "Jiawei Yang",
            "Kun Wang",
            "Lei He"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/Oral3DReconstructingthe3DStructureofOralCavityfromPanoramicXray.pdf"
    },
    {
        "internal_id": 402,
        "title": "Probabilistic Programming Bots in Intuitive Physics Game Play",
        "abstract": "Recent findings suggest that humans deploy cognitive mecha- nism of physics simulation engines to simulate the physics of objects. We propose a framework for bots to deploy probabilis- tic programming tools for interacting with intuitive physics environments. The framework employs a physics simulation in a probabilistic way to infer about moves performed by an agent in a setting governed by Newtonian laws of motion. However, methods of probabilistic programs can be slow in such setting due to their need to generate many samples. We complement the model with a model-free approach to aid the sampling procedures in becoming more efficient through learning from experience during game playing. We present an approach where combining model-free approaches (a con- volutional neural network in our model) and model-based ap- proaches (probabilistic physics simulation) is able to achieve what neither could alone. This way the model outperforms an all model-free or all model-based approach. We discuss a case study showing empirical results of the performance of the model on the game of Flappy Bird.",
        "authors": [
            "Fahad Alhasoun",
            "Sarah Alneghiemish"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/ProbabilisticProgrammingBotsinIntuitivePhysicsGamePlay.pdf"
    },
    {
        "internal_id": 403,
        "title": "MeInGame: Create a Game Character Face from a Single Portrait",
        "abstract": "Many deep learning based 3D face reconstruction methods have been proposed recently, however, few of them have ap- plications in games. Current game character customization systems either require players to manually adjust consider- able face attributes to obtain the desired face, or have limited freedom of facial shape and texture. In this paper, we pro- pose an automatic character face creation method that pre- dicts both facial shape and texture from a single portrait, and it can be integrated into most existing 3D games. Al- though 3D Morphable Face Model (3DMM) based meth- ods can restore accurate 3D faces from single images, the topology of 3DMM mesh is different from the meshes used in most games. To acquire fidelity texture, existing meth- ods require a large amount of face texture data for training, while building such datasets is time-consuming and labori- ous. Besides, such a dataset collected under laboratory con- ditions may not generalized well to in-the-wild situations. To tackle these problems, we propose 1) a low-cost facial tex- ture acquisition method, 2) a shape transfer algorithm that can transform the shape of a 3DMM mesh to games, and 3) a new pipeline for training 3D game face reconstruction net- works. The proposed method not only can produce detailed and vivid game characters similar to the input portrait, but can also eliminate the influence of lighting and occlusions. Experiments show that our method outperforms state-of-the- art methods used in games. Code and dataset are available at https://github.com/FuxiCV/MeInGame.",
        "authors": [
            "Jiangke Lin",
            " Yi Yuan"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/MeInGameCreateaGameCharacterFacefromaSinglePortrait.pdf"
    },
    {
        "internal_id": 404,
        "title": "PANTHER: Pathway Augmented Nonnegative Tensor Factorization for HighER-order Feature Learning",
        "abstract": "Genetic pathways usually encode molecular mechanisms that can inform targeted interventions. It is often challenging for existing machine learning approaches to jointly model ge- netic pathways (higher-order features) and variants (atomic features), and present to clinicians interpretable models. In order to build more accurate and better interpretable machine learning models for genetic medicine, we introduce Pathway Augmented Nonnegative Tensor factorization for HighER- order feature learning (PANTHER). PANTHER selects in- formative genetic pathways that directly encode molecular mechanisms. We apply genetically motivated constrained ten- sor factorization to group pathways in a way that reflects molecular mechanism interactions. We then train a soft- max classifier for disease types using the identified path- way groups. We evaluated PANTHER against multiple state- of-the-art constrained tensor/matrix factorization models, as well as group guided and Bayesian hierarchical models. PAN- THER outperforms all state-of-the-art comparison models significantly (p < 0.05). Our experiments on large scale Next Generation Sequencing (NGS) and whole-genome genotyp- ing datasets also demonstrated wide applicability of PAN- THER. We performed feature analysis in predicting disease types, which suggested insights and benefits of the identified pathway groups.",
        "authors": [
            "Yuan Luo",
            "Chengsheng Mao"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/PANTHERPathwayAugmentedNonnegativeTensorFactorizationforHighERorderFeatureLearning.pdf"
    },
    {
        "internal_id": 405,
        "title": "The Undergraduate Games Corpus: A Dataset for Machine Perception of Interactive Media",
        "abstract": "Machine perception research primarily focuses on process- ing static inputs (e.g. images and texts). We are interested in machine perception of interactive media (such as games, apps, and complex web applications) where interactive au- dience choices have long-term implications for the audience experience. While there is ample research on AI methods for the task of playing games (often just one game at a time), this work is difficult to apply to new and in-development games or to use for non-playing tasks such as similarity-based re- trieval or authoring assistance. In response, we contribute a corpus of 755 games and structured metadata, spread across several platforms (Twine, Bitsy, Construct, and Godot), with full source and assets available and appropriately licensed for use and redistribution in research. Because these games were sourced from student projects in an undergraduate game development program, they reference timely themes in their content and represent a variety of levels of design polish rather than only representing past commercial successes. This corpus could accelerate research in understanding interactive media while anchoring that work in freshly-developed games intended as legitimate human experiences (rather than lab- created AI testbeds). We validate the utility of this corpus by setting up the novel task of predicting tags relevant to the player experience from the game source code, showing that representations that better exploit the structure of the media outperform a text-only baseline.",
        "authors": [
            "Adam M Smith"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/TheUndergraduateGamesCorpusADatasetforMachinePerceptionofInteractiveMedia.pdf"
    },
    {
        "internal_id": 406,
        "title": "PHASE: PHysically-grounded Abstract Social Events for Machine Social Perception",
        "abstract": "Aviv Netanyahu*, Tianmin Shu*, Boris Katz, Andrei Barbu, Joshua B. Tenenbaum Massachusetts Institute of Technology, Cambridge, MA 02139 {avivn, tshu, boris, abarbu, jbt}@mit.edu",
        "authors": [],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/PHASEPHysicallygroundedAbstractSocialEventsforMachineSocialPerception.pdf"
    },
    {
        "internal_id": 407,
        "title": "Many-to-One Distribution Learning and K-Nearest Neighbor Smoothing for Thoracic Disease Identification",
        "abstract": "Chest X-rays are an important and accessible clinical imag- ing tool for the detection of many thoracic diseases. Over the past decade, deep learning, with a focus on the convolu- tional neural network (CNN), has become the most powerful computer-aided diagnosis technology for improving disease identification performance. However, training an effective and robust deep CNN usually requires a large amount of data with high annotation quality. For chest X-ray imaging, anno- tating large-scale data requires professional domain knowl- edge and is time-consuming. Thus, existing public chest X- ray datasets usually adopt language pattern based methods to automatically mine labels from reports. However, this re- sults in label uncertainty and inconsistency. In this paper, we propose many-to-one distribution learning (MODL) and K- nearest neighbor smoothing (KNNS) methods from two per- spectives to improve a single model's disease identification performance, rather than focusing on an ensemble of mod- els. MODL integrates multiple models to obtain a soft la- bel distribution for optimizing the single target model, which can reduce the effects of original label uncertainty. Moreover, KNNS aims to enhance the robustness of the target model to provide consistent predictions on images with similar medi- cal findings. Extensive experiments on the public NIH Chest X-ray and CheXpert datasets show that our model achieves consistent improvements over the state-of-the-art methods.",
        "authors": [
            "Yi Zhou",
            "Lei Huang",
            "Tianfei Zhou",
            "Ling Shao",
            "Abu Dhabi"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/ManytoOneDistributionLearningandKNearestNeighborSmoothingforThoracicDiseaseIdentification.pdf"
    },
    {
        "internal_id": 408,
        "title": "Dynamic Gaussian Mixture based Deep Generative Model For Robust Forecasting on Sparse Multivariate Time Series",
        "abstract": "Forecasting on sparse multivariate time series (MTS) aims to model the predictors of future values of time series given their incomplete past, which is important for many emerging ap- plications. However, most existing methods process MTS's individually, and do not leverage the dynamic distributions underlying the MTS's, leading to sub-optimal results when the sparsity is high. To address this challenge, we propose a novel generative model, which tracks the transition of latent clusters, instead of isolated feature representations, to achieve robust modeling. It is characterized by a newly designed dy- namic Gaussian mixture distribution, which captures the dy- namics of clustering structures, and is used for emitting time series. The generative model is parameterized by neural net- works. A structured inference network is also designed for enabling inductive analysis. A gating mechanism is further introduced to dynamically tune the Gaussian mixture distri- butions. Extensive experimental results on a variety of real- life datasets demonstrate the effectiveness of our method.",
        "authors": [
            "Yinjun Wu",
            "Jingchao Ni",
            "Wei Cheng",
            "Bo Zong",
            "Dongjin Song",
            "Zhengzhang Chen",
            "Yanchi Liu",
            "Xuchao Zhang",
            "Haifeng Chen"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/DynamicGaussianMixturebasedDeepGenerativeModelForRobustForecastingonSparseMultivariateTimeSeries.pdf"
    },
    {
        "internal_id": 409,
        "title": "Efficient Poverty Mapping from High Resolution Remote Sensing Images",
        "abstract": "The combination of high-resolution satellite imagery and ma- chine learning have proven useful in many sustainability- related tasks, including poverty prediction, infrastructure measurement, and forest monitoring. However, the accu- racy afforded by high-resolution imagery comes at a cost, as such imagery is extremely expensive to purchase at scale. This creates a substantial hurdle to the efficient scaling and widespread adoption of high-resolution-based approaches. To reduce acquisition costs while maintaining accuracy, we propose a reinforcement learning approach in which free low- resolution imagery is used to dynamically identify where to acquire costly high-resolution images, prior to performing a deep learning task on the high-resolution images. We apply this approach to the task of poverty prediction in Uganda, building on an earlier approach that used object detection to count objects and use these counts to predict poverty. Our approach exceeds previous performance benchmarks on this task while using 80% fewer high-resolution images, and could be useful in many domains that require high-resolution imagery.",
        "authors": [
            "IIT Kharagpur"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/EfficientPovertyMappingfromHighResolutionRemoteSensingImages.pdf"
    },
    {
        "internal_id": 410,
        "title": "KAN: Knowledge-aware Attention Network for Fake News Detection",
        "abstract": "The explosive growth of fake news on social media has drawn great concern both from industrial and academic communi- ties. There has been an increasing demand for fake news de- tection due to its detrimental effects. Generally, news content is condensed and full of knowledge entities. However, exist- ing methods usually focus on the textual contents and social context, and ignore the knowledge-level relationships among news entities. To address this limitation, in this paper, we propose a novel Knowledge-aware Attention Network (KAN) that incorporates external knowledge from knowledge graph for fake news detection. Firstly, we identify entity mentions in news contents and align them with the entities in knowl- edge graph. Then, the entities and their contexts are used as external knowledge to provide complementary informa- tion. Finally, we design News towards Entities (N-E) atten- tion and News towards Entities and Entity Contexts (N-E2C) attention to measure the importances of knowledge. Thus, our proposed model can incorporate both semantic-level and knowledge-level representations of news to detect fake news. Experimental results on three public datasets show that our model outperforms the state-of-the-art methods, and also val- idate the effectiveness of knowledge attention.",
        "authors": [
            "Yaqian Dun",
            "Kefei Tu",
            "Chen Chen",
            "Chunyan Hou",
            "Xiaojie Yuan"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/KANKnowledgeawareAttentionNetworkforFakeNewsDetection.pdf"
    },
    {
        "internal_id": 411,
        "title": "Research Reproducibility as a Survival Analysis",
        "abstract": "There has been increasing concern within the machine learn- ing community that we are in a reproducibility crisis. As many have begun to work on this problem, all work we are aware of treat the issue of reproducibility as an intrinsic bi- nary property: a paper is or is not reproducible. Instead, we consider modeling the reproducibility of a paper as a sur- vival analysis problem. We argue that this perspective repre- sents a more accurate model of the underlying meta-science question of reproducible research, and we show how a sur- vival analysis allows us to draw new insights that better ex- plain prior longitudinal data. The data and code can be found at https://github.com/EdwardRaff/Research-Reproducibility- Survival-Analysis",
        "authors": [
            "Edward Raff",
            "Booz Allen Hamilton",
            "Baltimore County"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/ResearchReproducibilityasaSurvivalAnalysis.pdf"
    },
    {
        "internal_id": 412,
        "title": "Neural Analogical Matching",
        "abstract": "Analogy is core to human cognition. It allows us to solve problems based on prior experience, it governs the way we conceptualize new information, and it even influences our vi- sual perception. The importance of analogy to humans has made it an active area of research in the broader field of artifi- cial intelligence, resulting in data-efficient models that learn and reason in human-like ways. While cognitive perspectives of analogy and deep learning have generally been studied in- dependently of one another, the integration of the two lines of research is a promising step towards more robust and efficient learning techniques. As part of a growing body of research on such an integration, we introduce the Analogical Matching Network: a neural architecture that learns to produce analogies between structured, symbolic representations that are largely consistent with the principles of Structure-Mapping Theory.",
        "authors": [
            "Maxwell Crouse",
            "Constantine Nakos",
            "Ibrahim Abdelaziz",
            "Ken Forbus",
            "Qualitative Reasoning Group"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/NeuralAnalogicalMatching.pdf"
    },
    {
        "internal_id": 413,
        "title": "DEAR: Deep Reinforcement Learning for Online Advertising Impression in Recommender Systems",
        "abstract": "With the recent prevalence of Reinforcement Learning (RL), there have been tremendous interests in utilizing RL for online advertising in recommendation platforms (e.g., e- commerce and news feed sites). However, most RL-based ad- vertising algorithms focus on optimizing ads' revenue while ignoring the possible negative influence of ads on user expe- rience of recommended items (products, articles and videos). Developing an optimal advertising algorithm in recommen- dations faces immense challenges because interpolating ads improperly or too frequently may decrease user experience, while interpolating fewer ads will reduce the advertising rev- enue. Thus, in this paper, we propose a novel advertising strategy for the rec/ads trade-off. To be specific, we develop an RL-based framework that can continuously update its ad- vertising strategies and maximize reward in the long run. Given a recommendation list, we design a novel Deep Q- network architecture that can determine three internally re- lated tasks jointly, i.e., (i) whether to interpolate an ad or not in the recommendation list, and if yes, (ii) the optimal ad and (iii) the optimal location to interpolate. The experimental re- sults based on real-world data demonstrate the effectiveness of the proposed framework.",
        "authors": [
            "Xiangyu Zhao",
            "Changsheng Gu",
            "Haoshenglun Zhang",
            "Xiwang Yang",
            "Xiaobing Liu",
            "Jiliang Tang",
            "Hui Liu"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/DEARDeepReinforcementLearningforOnlineAdvertisingImpressioninRecommenderSystems.pdf"
    },
    {
        "internal_id": 414,
        "title": "Fully Exploiting Cascade Graphs for Real-time Forwarding Prediction",
        "abstract": "Real-time forwarding prediction for predicting online con- tents' popularity is beneficial to various social applications for enhancing interactive social behaviors. Cascade graphs, formed by online contents' propagation, play a vital role in real-time forwarding prediction. Existing cascade graph mod- eling methods are inadequate to embed cascade graphs that have hub structures and deep cascade paths, or they fail to handle the short-term outbreak of forwarding amount. To this end, we propose a novel real-time forwarding prediction method that includes an effective approach for cascade graph embedding and a short-term variation sensitive method for time-series modeling, making the best of cascade graph fea- tures. Using two real world datasets, we demonstrate the sig- nificant superiority of the proposed method compared with the state-of-the-art. Our experiments also reveal interesting implications hidden in the performance differences between cascade graph embedding and time-series modeling.",
        "authors": [
            "Xiangyun Tang",
            "Dongliang Liao",
            "Weijie Huang",
            "Jin Xu",
            "Liehuang Zhu",
            "Meng Shen"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/FullyExploitingCascadeGraphsforRealtimeForwardingPrediction.pdf"
    },
    {
        "internal_id": 415,
        "title": "Deep Portfolio Optimization via Distributional Prediction of Residual Factors",
        "abstract": "Recent developments in deep learning techniques have mo- tivated intensive research in machine learning-aided stock trading strategies. However, since the financial market has a highly non-stationary nature hindering the application of typical data-hungry machine learning methods, leveraging fi- nancial inductive biases is important to ensure better sam- ple efficiency and robustness. In this study, we propose a novel method of constructing a portfolio based on predicting the distribution of a financial quantity called residual factors, which is known to be generally useful for hedging the risk exposure to common market factors. The key technical ingre- dients are twofold. First, we introduce a computationally ef- ficient extraction method for the residual information, which can be easily combined with various prediction algorithms. Second, we propose a novel neural network architecture that allows us to incorporate widely acknowledged financial in- ductive biases such as amplitude invariance and time-scale invariance. We demonstrate the efficacy of our method on U.S. and Japanese stock market data. Through ablation ex- periments, we also verify that each individual technique con- tributes to improving the performance of trading strategies. We anticipate our techniques may have wide applications in various financial problems.",
        "authors": [
            "Kentaro Imajo",
            " Kentaro Minami",
            " Katsuya Ito",
            " Kei Nakagawa",
            " Preferred Networks"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/DeepPortfolioOptimizationviaDistributionalPredictionofResidualFactors.pdf"
    },
    {
        "internal_id": 416,
        "title": "Plug-and-Play Domain Adaptation for Cross-Subject EEG-based Emotion Recognition",
        "abstract": "Human emotion decoding in affective brain-computer inter- faces suffers a major setback due to the inter-subject vari- ability of electroencephalography (EEG) signals. Existing ap- proaches usually require amassing extensive EEG data of each new subject, which is prohibitively time-consuming along with poor user experience. To tackle this issue, we di- vide EEG representations into private components specific to each subject and shared emotional components that are uni- versal to all subjects. According to this representation parti- tion, we propose a plug-and-play domain adaptation method for dealing with the inter-subject variability. In the training phase, subject-invariant emotional representations and private components of source subjects are separately captured by a shared encoder and private encoders. Furthermore, we build one emotion classifier on the shared partition and subjects' individual classifiers on the combination of these two parti- tions. In the calibration phase, the model only requires few unlabeled EEG data from incoming target subjects to model their private components. Therefore, besides the shared emo- tion classifier, we have another pipeline to use the knowledge of source subjects through the similarity of private compo- nents. In the test phase, we integrate predictions of the shared emotion classifier with those of individual classifiers ensem- ble after modulation by similarity weights. Experimental re- sults on the SEED dataset show that our model greatly short- ens the calibration time within a minute while maintaining the recognition accuracy, all of which make emotion decod- ing more generalizable and practicable.",
        "authors": [
            "Li Ming Zhao",
            " Xu Yan",
            "Ruijin Hospital"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/PlugandPlayDomainAdaptationforCrossSubjectEEGbasedEmotionRecognition.pdf"
    },
    {
        "internal_id": 417,
        "title": "Alternative Baselines for Low-Shot 3D Medical Image Segmentation\u2014An Atlas Perspective",
        "abstract": "Low-shot (one/few-shot) segmentation has attracted increas- ing attention as it works well with limited annotation. State- of-the-art low-shot segmentation methods on natural images usually focus on implicit representation learning for each novel class, such as learning prototypes, deriving guidance features via masked average pooling, and segmenting using cosine similarity in feature space. We argue that low-shot seg- mentation on medical images should step further to explicitly learn dense correspondences between images to utilize the anatomical similarity. The core ideas are inspired by the clas- sical practice of multi-atlas segmentation, where the indis- pensable parts of atlas-based segmentation, i.e., registration, label propagation, and label fusion are unified into a single framework in our work. Specifically, we propose two alter- native baselines, i.e., the Siamese-Baseline and Individual- Difference-Aware Baseline, where the former is targeted at anatomically stable structures (such as brain tissues), and the latter possesses a strong generalization ability to organs suf- fering large morphological variations (such as abdominal or- gans). In summary, this work sets up a benchmark for low- shot 3D medical image segmentation and sheds light on fur- ther understanding of atlas-based few-shot segmentation.",
        "authors": [
            "Shuxin Wang",
            " Shilei Cao",
            " Dong Wei",
            " Cong Xie",
            " Kai Ma",
            " Liansheng Wang",
            "Deyu Meng",
            " Yefeng Zheng",
            "Xi an"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/AlternativeBaselinesforLowShot3DMedicalImageSegmentationAnAtlasPerspective.pdf"
    },
    {
        "internal_id": 419,
        "title": "Interpretable Self-Supervised Facial Micro-Expression Learning to Predict Cognitive State and Neurological Disorders",
        "abstract": "Human behavior is the confluence of output from vol- untary and involuntary motor systems. The neural ac- tivities that mediate behavior, from individual cells to distributed networks, are in a state of constant flux. Ar- tificial intelligence (AI) research over the past decade shows that behavior, in the form of facial muscle ac- tivity, can reveal information about fleeting voluntary and involuntary motor system activity related to emo- tion, pain, and deception. However, the AI algorithms often lack an explanation for their decisions, and learn- ing meaningful representations requires large datasets labeled by a subject-matter expert. Motivated by the success of using facial muscle movements to clas- sify brain states and the importance of learning from small amounts of data, we propose an explainable self- supervised representation-learning paradigm that learns meaningful temporal facial muscle movement patterns from limited samples. We validate our methodology by carrying out comprehensive empirical study to predict future speech behavior in a real-world dataset of adults who stutter (AWS). Our explainability study found fa- cial muscle movements around the eyes (p <0.x001) and lips (p <0.001) differ significantly before pro- ducing fluent vs. disfluent speech. Evaluations using the AWS dataset demonstrates that the proposed self- supervised approach achieves a minimum of 2.51% ac- curacy improvement over fully-supervised approaches.",
        "authors": [
            "Arun Das",
            " Jeffrey Mock",
            " Yufei Huang",
            " Edward Golob"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/InterpretableSelfSupervisedFacialMicroExpressionLearningtoPredictCognitiveStateandNeurologicalDisorders.pdf"
    },
    {
        "internal_id": 420,
        "title": "RNA Secondary Structure Representation Network for RNA-proteins Binding Prediction",
        "abstract": "RNA-binding proteins (RBPs) play a significant part in sev- eral biological processes in the living cell, such as gene reg- ulation and mRNA localization. Several deep learning meth- ods, especially the model based on convolutional neural net- work (CNN), have been used to predict the binding sites. However, previous methods fail to represent RNA secondary structure features. The traditional deep learning methods gen- erally transform the RNA secondary structure to a regular matrix that cannot reveal the topological structure informa- tion of RNA. To effectively extract the structure features of RNA, we propose an RNA secondary structure representation network (RNASSR-Net) based on graph convolutional neural network (GCN) and convolution neural network (CNN) for RBP binding prediction. RNASSR-Net constructs the graph model derived from the RNA secondary structure to learn the topological properties of RNA. Then, it obtains the spatial im- portance of each base in RNA with CNN to guide the repre- sentation of the RNA secondary structure. Finally, RNASSR- Net combines the structure and sequence features to predict the binding sites. Experimental results demonstrate the pro- posed method outperforms a few state-of-the-art methods on the benchmark datasets and gets a higher improvement on the small-size data. Besides, the proposed RNASSR-Net is also used to detect the accurate motifs compared with the experi- mentally verified motifs, which reveals the binding region lo- cation and RNA structure interpretation for some biological guidance in the future.",
        "authors": [
            "Ziyi Liu",
            " Fulin Luo",
            " Bo Du"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/RNASecondaryStructureRepresentationNetworkforRNAproteinsBindingPrediction.pdf"
    },
    {
        "internal_id": 421,
        "title": "GTA: Graph Truncated Attention for Retrosynthesis",
        "abstract": "Retrosynthesis is the task of predicting reactant molecules from a given product molecule and is, important in organic chemistry because the identification of a synthetic path is as demanding as the discovery of new chemical compounds. Re- cently, the retrosynthesis task has been solved automatically without human expertise using powerful deep learning mod- els. Recent deep models are primarily based on seq2seq or graph neural networks depending on the function of molecu- lar representation, sequence, or graph. Current state-of-the- art models represent a molecule as a graph, but they re- quire joint training with auxiliary prediction tasks, such as the most probable reaction template or reaction center prediction. Furthermore, they require additional labels by experienced chemists, thereby incurring additional cost. Herein, we pro- pose a novel template-free model, i.e., Graph Truncated At- tention (GTA), which leverages both sequence and graph rep- resentations by inserting graphical information into a seq2seq model. The proposed GTA model masks the self-attention layer using the adjacency matrix of product molecule in the encoder and applies a new loss using atom mapping acquired from an automated algorithm to the cross-attention layer in the decoder. Our model achieves new state-of-the-art records, i.e., exact match top-1 and top-10 accuracies of 51.1 % and 81.6 % on the USPTO-50k benchmark dataset, respectively, and 46.0 % and 70.0 % on the USPTO-full dataset, respec- tively, both without any reaction class information. The GTA model surpasses prior graph-based template-free models by 2 % and 7 % in terms of the top-1 and top-10 accuracies on the USPTO-50k dataset, respectively, and by over 6 % for both the top-1 and top-10 accuracies on the USPTO-full dataset.",
        "authors": [
            "June Yong Yang",
            "Seohui Bae",
            "Hankook Lee",
            "Jinwoo Shin",
            "Sung Ju Hwang",
            "Eunho Yang"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/GTAGraphTruncatedAttentionforRetrosynthesis.pdf"
    },
    {
        "internal_id": 422,
        "title": "Universal Trading for Order Execution with Oracle Policy Distillation",
        "abstract": "As a fundamental problem in algorithmic trading, order exe- cution aims at fulfilling a specific trading order, either liqui- dation or acquirement, for a given instrument. Towards effec- tive execution strategy, recent years have witnessed the shift from the analytical view with model-based market assump- tions to model-free perspective, i.e., reinforcement learning, due to its nature of sequential decision optimization. How- ever, the noisy and yet imperfect market information that can be leveraged by the policy has made it quite challenging to build up sample efficient reinforcement learning methods to achieve effective order execution. In this paper, we propose a novel universal trading policy optimization framework to bridge the gap between the noisy yet imperfect market states and the optimal action sequences for order execution. Partic- ularly, this framework leverages a policy distillation method that can better guide the learning of the common policy to- wards practically optimal execution by an oracle teacher with perfect information to approximate the optimal trading strat- egy. The extensive experiments have shown significant im- provements of our method over various strong baselines, with reasonable trading actions.",
        "authors": [
            "Yuchen Fang",
            " Kan Ren",
            " Weiqing Liu",
            " Dong Zhou",
            "Weinan Zhang",
            " Jiang Bian",
            " Yong Yu"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/UniversalTradingforOrderExecutionwithOraclePolicyDistillation.pdf"
    },
    {
        "internal_id": 423,
        "title": "Riemannian Embedding Banks for Common Spatial Patterns with EEG-based SPD Neural Networks",
        "abstract": "Modeling non-linear data as symmetric positive definite (SPD) matrices on Riemannian manifolds has attracted much attention for various classification tasks. In the context of deep learning, SPD matrix-based Riemannian networks have been shown to be a promising solution for classifying elec- troencephalogram (EEG) signals, capturing the Riemannian geometry within their structured 2D feature representation. However, existing approaches usually learn spatial-temporal structures in an embedding space for all available EEG sig- nals, and their optimization procedures rely on computation- ally expensive iterations. Furthermore, these approaches of- ten struggle to encode all of the various types of relation- ships into a single distance metric, resulting in a loss of gen- erality. To address the above limitations, we propose a Rie- mannian Embedding Banks method, which divides the prob- lem of common spatial patterns learning in an entire em- bedding space into K-subproblems and builds one model for each subproblem, to be combined with SPD neural net- works. By leveraging the concept of the \"separate-to-learn\" technology on a Riemannian manifold, REB divides the data and the embedding space into K non-overlapping subsets and learns K separate distance metrics in a Riemannian ge- ometric space instead of the vector space. Then, the learned K non-overlapping subsets are grouped into neurons in the SPD neural network's embedding layer. Experimental results on public EEG datasets demonstrate the superiority of the proposed approach for learning common spatial patterns of EEG signals despite their non-stationary nature, increasing the convergence speed while maintaining generalization.",
        "authors": [
            "Yoon Je Suh",
            "Byung Hyung Kim"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/RiemannianEmbeddingBanksforCommonSpatialPatternswithEEGbasedSPDNeuralNetworks.pdf"
    },
    {
        "internal_id": 424,
        "title": "Low-Rank Registration Based Manifolds for Convection-Dominated PDEs",
        "abstract": "We develop an auto-encoder-type nonlinear dimensionality reduction algorithm to enable the construction of reduced order models of systems governed by convection-dominated nonlinear partial differential equations (PDEs), i.e. snapshots of solutions with large Kolmogorov n-width. Although sev- eral existing nonlinear manifold learning methods, such as LLE, ISOMAP, MDS, etc., appear as compelling candidates to reduce the dimensionality of such data, most are not ap- plicable to reduced order modeling of PDEs, because: (i) they typically lack a straightforward mapping from the la- tent space to the high-dimensional physical space, and (ii) the identified latent variables are often difficult to interpret. In our proposed method, these limitations are overcome by training a low-rank diffeomorphic spatio-temporal grid that registers the output sequence of the PDEs on a non-uniform parameter/time-varying grid, such that the Kolmogorov n- width of the mapped data on the learned grid is minimized. We demonstrate the efficacy and interpretability of our pro- posed approach on several challenging manufactured com- puter vision-inspired tasks and physical systems.",
        "authors": [
            "Rambod Mojgani"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/LowRankRegistrationBasedManifoldsforConvectionDominatedPDEs.pdf"
    },
    {
        "internal_id": 425,
        "title": "Traffic Shaping in E-Commercial Search Engine: Multi-Objective Online Welfare Maximization",
        "abstract": "The e-commercial search engine is the primary gateway for customers to find desired products and engage in online shopping. Besides displaying items to optimize for a sin- gle objective (i.e., relevance), ranking items needs to satisfy some other business requirements in practice. Recently, traf- fic shaping was introduced to incorporate multiple objectives in a constrained optimization framework. However, many practical business requirements can not explicitly represented by linear constraints as in the existing work, and this may limit the scalability of their framework. This paper presents a unified framework from the aspect of multi-objective welfare maximization where we regard all business requirements as objectives to optimize. Our framework can naturally incorpo- rate a wide range of application-driven requirements. In ad- dition to formulating the problem, we design an online traffic splitting algorithm that allows us to flexibly adjust the pri- orities of different objectives, and it has rigorous theoretical guarantees over the adversarial scenario. We also run exper- iments on both synthetic and real-world datasets to validate our algorithms.",
        "authors": [
            "Liucheng Sun",
            "Chenwei Weng",
            "Chengfu Huo",
            "Weijun Ren",
            "Guochuan Zhang",
            "Xin Li",
            "Alibaba Group"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/TrafficShapinginECommercialSearchEngineMultiObjectiveOnlineWelfareMaximization.pdf"
    },
    {
        "internal_id": 426,
        "title": "When Hashing Met Matching: Efficient Spatio-Temporal Search for Ridesharing",
        "abstract": "Shared on-demand mobility holds immense potential for ur- ban transportation. However, finding ride matches in real- time at urban scale is a very difficult combinatorial optimiza- tion problem and mostly heuristic methods are applied. In this work, we introduce a principled approach to this combinato- rial problem. Our approach proceeds by constructing suitable representations for rides and driver routes capturing their es- sential spatio-temporal aspects in an appropriate vector space, and defining a similarity metric in this space that expresses matching utility. This then lets us mathematically model the problem of finding ride matches as that of Near Neighbor Search (NNS). Exploiting this modeling, we devise a novel randomized spatio-temporal search algorithm for finding ride matches based on the theory of Locality Sensitive Hashing (LSH). Apart from being highly efficient, our algorithm en- joys several practically useful properties and extension pos- sibilities. Experiments with large real-world datasets show that our algorithm consistently outperforms state-of-the-art heuristic methods thereby proving its practical applicability.",
        "authors": [
            "Chinmoy Dutta"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/WhenHashingMetMatchingEfficientSpatioTemporalSearchforRidesharing.pdf"
    },
    {
        "internal_id": 427,
        "title": "Bigram and Unigram Based Text Attack via Adaptive Monotonic Heuristic Search",
        "abstract": "Deep neural networks (DNNs) are known to be vulnerable to adversarial images, while their robustness in text classifica- tion are rarely studied. Several lines of text attack methods have been proposed in the literature, such as character-level, word-level, and sentence-level attacks. However, it is still a challenge to minimize the number of word distortions neces- sary to induce misclassification, while simultaneously ensur- ing the lexical correctness, syntactic correctness, and seman- tic similarity. In this paper, we propose the Bigram and Uni- gram based Monotonic Heuristic Search (BU-MHS) method to examine the vulnerability of deep models. Our method has three major merits. Firstly, we propose to attack text docu- ments not only at the unigram word level but also at the bi- gram level to avoid producing meaningless outputs. Secondly, we propose a hybrid method to replace the input words with both their synonyms and sememe candidates, which greatly enriches potential substitutions compared to only using syn- onyms. Lastly, we design a search algorithm, i.e., Monotonic Heuristic Search (MHS), to determine the priority of word replacements, aiming to reduce the modification cost in an adversarial attack. We evaluate the effectiveness of BU-MHS on IMDB, AG's News, and Yahoo! Answers text datasets by attacking four popular DNNs models. Results show that our BU-MHS achieves the highest attack success rate by chang- ing the smallest number of words compared with baselines.",
        "authors": [
            "Xinghao Yang",
            "Weifeng Liu",
            "James Bailey",
            "Dacheng Tao",
            "Wei Liu"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/BigramandUnigramBasedTextAttackviaAdaptiveMonotonicHeuristicSearch.pdf"
    },
    {
        "internal_id": 428,
        "title": "The LOB Recreation Model: Predicting the Limit Order Book from TAQ History Using an Ordinary Differential Equation Recurrent Neural Network",
        "abstract": "In an order-driven financial market, the price of a financial as- set is discovered through the interaction of orders - requests to buy or sell at a particular price - that are posted to the public limit order book (LOB). Therefore, LOB data is ex- tremely valuable for modelling market dynamics. However, LOB data is not freely accessible, which poses a challenge to market participants and researchers wishing to exploit this information. Fortunately, trades and quotes (TAQ) data - or- ders arriving at the top of the LOB, and trades executing in the market - are more readily available. In this paper, we present the LOB recreation model, a first attempt from a deep learning perspective to recreate the top five price levels of the LOB for small-tick stocks using only TAQ data. Volumes of orders sitting deep in the LOB are predicted by combining outputs from: (1) a history compiler that uses a Gated Re- current Unit (GRU) module to selectively compile prediction relevant quote history; (2) a market events simulator, which uses an Ordinary Differential Equation Recurrent Neural Net- work (ODE-RNN) to simulate the accumulation of net order arrivals; and (3) a weighting scheme to adaptively combine the predictions generated by (1) and (2). By the paradigm of transfer learning, the source model trained on one stock can be fine-tuned to enable application to other financial assets of the same class with much lower demand on additional data. Comprehensive experiments conducted on two real world in- traday LOB datasets demonstrate that the proposed model can efficiently recreate the LOB with high accuracy using only TAQ data as input.",
        "authors": [
            "Zijian Shi",
            "Yu Chen",
            "John Cartlidge",
            "BS UB"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/TheLOBRecreationModelPredictingtheLimitOrderBookfromTAQHistoryUsinganOrdinaryDifferentialEquationRecurrentNeuralNetwork.pdf"
    },
    {
        "internal_id": 429,
        "title": "TreeCaps: Tree-Based Capsule Networks for Source Code Processing",
        "abstract": "Introduction Software developers often spend the majority of their time in navigating existing program code bases to understand the functionality of existing source code before implementing new features or fixing bugs (Xia et al. 2018; Britton et al. 2012). Learning a model of programs has been found useful for their tasks such as classifying the functionality of pro- grams (Nix and Zhang 2017; Dahl et al. 2013; Pascanu et al. 2015; Rastogi, Chen, and Jiang 2013), predicting bugs (Yang et al. 2015; Li et al. 2017, 2018; Zhou et al. 2019), translat- ing programs (Chen, Liu, and Song 2018; Gu et al. 2017; Bui, Jiang, and Yu 2018; Bui, Yu, and Jiang 2019; Nghi, Yu, and Jiang 2019), etc. It is common that adding semantic descriptions (e.g., via code comments, visualizing code control flow graphs, etc.)",
        "authors": [
            "named TreeCaps",
            "by fus",
            "TreeCaps mod"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/TreeCapsTreeBasedCapsuleNetworksforSourceCodeProcessing.pdf"
    },
    {
        "internal_id": 430,
        "title": "Deep Style Transfer for Line Drawings",
        "abstract": "Line drawings are frequently used to illustrate ideas and con- cepts in digital documents and presentations. To compose a line drawing, it is common for users to retrieve multiple line drawings from the Internet and combine them as one im- age. However, different line drawings may have different line styles and are visually inconsistent when put together. In or- der that the line drawings can have consistent looks, in this paper, we make the first attempt to perform style transfer for line drawings. The key of our design lies in the fact that cen- terline plays a very important role in preserving line topology and extracting style features. With this finding, we propose to formulate the style transfer problem as a centerline styliza- tion problem and solve it via a novel style-guided image-to- image translation network. Results and statistics show that our method significantly outperforms the existing methods both visually and quantitatively.",
        "authors": [
            "Xueting Liu",
            "Wenliang Wu",
            "Huisi Wu",
            "Zhenkun Wen"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/DeepStyleTransferforLineDrawings.pdf"
    },
    {
        "internal_id": 431,
        "title": "Towards a Better Understanding of VR Sickness: Physical Symptom Prediction for VR Contents",
        "abstract": "We address the black-box issue of VR sickness assessment (VRSA) by evaluating the level of physical symptoms of VR sickness. For the VR contents inducing the similar VR sick- ness level, the physical symptoms can vary depending on the characteristics of the contents. Most of existing VRSA meth- ods focused on assessing the overall VR sickness score. To make better understanding of VR sickness, it is required to predict and provide the level of major symptoms of VR sick- ness rather than overall degree of VR sickness. In this paper, we predict the degrees of main physical symptoms affect- ing the overall degree of VR sickness, which are disorienta- tion, nausea, and oculomotor. In addition, we introduce a new large-scale dataset for VRSA including 360 videos with vari- ous frame rates, physiological signals, and subjective scores. On VRSA benchmark and our newly collected dataset, our approach shows a potential to not only achieve the highest correlation with subjective scores, but also to better under- stand which symptoms are the main causes of VR sickness.",
        "authors": [
            "Hak Gu Kim",
            "Sangmin Lee",
            "Seongyeop Kim",
            "Heoun taek Lim",
            "Yong Man Ro"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/TowardsaBetterUnderstandingofVRSicknessPhysicalSymptomPredictionforVRContents.pdf"
    },
    {
        "internal_id": 432,
        "title": "Minimizing Labeling Cost for Nuclei Instance Segmentation and Classification with Cross-domain Images and Weak Labels",
        "abstract": "Nucleus instance segmentation and classification in histopathological images is an essential prerequisite in pathology diagnosis/prognosis. However, nucleus annota- tions (e.g., segmentation and labeling) require domain ex- perts, and annotating nuclei at pixel-level is time-consuming and labor-intensive. Moreover, nuclei from different cancer types vary in shapes and appearances. These inter-cancer variations require careful annotations for specific cancer types. Therefore, to minimize the labeling cost, we propose a novel application that considers each cancer type as an individual domain and apply domain adaptation techniques to improve the segmentation/classification performance among different cancer types. Unlike the previous studies that focus on unsupervised or weakly-supervised domain adaptation independently, we would like to discover what kinds of labeling can achieve the most cost-effective domain adaptation performance in nucleus instance segmentation and classification. Specifically, we propose a unified frame- work that is applicable to different level annotations: no annotations, image-level, and point-level annotations. Cyclic adaptation with pseudo labels and adversarial discriminator are utilized for unsupervised domain alignment. Image-level or point-level annotations are additionally adopted to super- vise the nucleus classification and refine the pseudo labels. Experiments demonstrate the effectiveness and efficacy of the proposed framework (jointly using unsupervised and weakly supervised learning) on adapting the segmentation and classification model from one cancer type to 18 other cancer types.",
        "authors": [
            "Siqi Yang",
            "Jun Zhang",
            "Junzhou Huang ",
            "Brian C Lovell",
            "Xiao Han"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/MinimizingLabelingCostforNucleiInstanceSegmentationandClassificationwithCrossdomainImagesandWeakLabels.pdf"
    },
    {
        "internal_id": 433,
        "title": "Automated Lay Language Summarization of Biomedical Scientific Reviews",
        "abstract": "Health literacy has emerged as a crucial factor in making ap- propriate health decisions and ensuring treatment outcomes. However, medical jargon and the complex structure of profes- sional language in this domain make health information espe- cially hard to interpret. Thus, there is an urgent unmet need for automated methods to enhance the accessibility of the biomedical literature to the general population. This problem can be framed as a type of translation problem between the language of healthcare professionals, and that of the general public. In this paper, we introduce the novel task of automated generation of lay language summaries of biomedical scien- tific reviews, and construct a dataset to support the devel- opment and evaluation of automated methods through which to enhance the accessibility of the biomedical literature. We conduct analyses of the various challenges in performing this task, including not only summarization of the key points but also explanation of background knowledge and simplification of professional language. We experiment with state-of-the-art summarization models as well as several data augmentation techniques, and evaluate their performance using both auto- mated metrics and human assessment. Results indicate that automatically generated summaries produced using contem- porary neural architectures can achieve promising quality and readability as compared with reference summaries developed for the lay public by experts (best ROUGE-L of 50.24 and Flesch-Kincaid readability score of 13.30). We also discuss the limitations of the current effort, providing insights and di- rections for future work.",
        "authors": [
            "Yue Guo",
            " Wei Qiu",
            " Yizhong Wang",
            " Trevor Cohen"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/AutomatedLayLanguageSummarizationofBiomedicalScientificReviews.pdf"
    },
    {
        "internal_id": 434,
        "title": "Apparently Irrational Choice as Optimal Sequential Decision Making",
        "abstract": "In this paper, we propose a normative approach to model- ing apparently human irrational decision making (cognitive biases) that makes use of inherently rational computational mechanisms. We view preferential choice tasks as sequential decision making problems and formulate them as Partially Observable Markov Decision Processes (POMDPs). The re- sulting sequential decision model learns what information to gather about which options, whether to calculate option val- ues or make comparisons between options and when to make a choice. We apply the model to choice problems where con- text is known to influence human choice, an effect that has been taken as evidence that human cognition is irrational. Our results show that the new model approximates a bounded op- timal cognitive policy and makes quantitative predictions that correspond well to evidence about human choice. Further- more, the model uses context to help infer which option has a maximum expected value while taking into account com- putational cost and cognitive limits. In addition, it predicts when, and explains why, people stop evidence accumulation and make a decision. We argue that the model provides evi- dence that apparent human irrationalities are emergent conse- quences of processes that prefer higher value (rational) poli- cies.",
        "authors": [
            "Haiyang Chen"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/ApparentlyIrrationalChoiceasOptimalSequentialDecisionMaking.pdf"
    },
    {
        "internal_id": 435,
        "title": "PSSM-Distil: Protein Secondary Structure Prediction (PSSP) on Low-Quality PSSM by Knowledge Distillation with Contrastive Learning",
        "abstract": "Protein secondary structure prediction (PSSP) is an essen- tial task in computational biology. To achieve the accurate PSSP, the general and vital feature engineering is to use multi- ple sequence alignment (MSA) for Position-Specific Scoring Matrix (PSSM) extraction. However, when only low-quality PSSM can be obtained due to poor sequence homology, pre- vious PSSP accuracy (merely around 65%) is far from prac- tical usage for subsequent tasks. In this paper, we propose a novel PSSM-Distil framework for PSSP on low-quality PSSM, which not only enhances the PSSM feature at a lower level but also aligns the feature distribution at a higher level. In practice, the PSSM-Distil first exploits the proteins with high-quality PSSM to achieve a teacher network for PSSP in a full-supervised way. Under the guidance of the teacher network, the low-quality PSSM and corresponding student network with low discriminating capacity are effectively re- solved by feature enhancement through EnhanceNet and dis- tribution alignment through knowledge distillation with con- trastive learning. Further, our PSSM-Distil supports the input from a pre-trained protein sequence language BERT model to provide auxiliary information, which is designed to address the extremely low-quality PSSM cases, i.e., no homologous sequence. Extensive experiments demonstrate the proposed PSSM-Distil outperforms state-of-the-art models on PSSP by 6% on average and nearly 8% in extremely low-quality cases on public benchmarks, BC40 and CB513.",
        "authors": [
            "Qin Wang",
            "Boyuan Wang ",
            "Zhenlei Xu ",
            "Jiaxiang Wu ",
            "Peilin Zhao ",
            "Zhen Li ",
            "Wang ",
            "Junzhou Huang ",
            "Shuguang Cui "
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/PSSMDistilProteinSecondaryStructurePredictionPSSPonLowQualityPSSMbyKnowledgeDistillationwithContrastiveLearning.pdf"
    },
    {
        "internal_id": 436,
        "title": "SDGNN: Learning Node Representation for Signed Directed Networks",
        "abstract": "Network embedding is aimed at mapping nodes in a network into low-dimensional vector representations. Graph Neural Networks (GNNs) have received widespread attention and lead to state-of-the-art performance in learning node repre- sentations. However, most GNNs only work in unsigned net- works, where only positive links exist. It is not trivial to transfer these models to signed directed networks, which are widely observed in the real world yet less studied. In this paper, we first review two fundamental sociological theories (i.e., status theory and balance theory) and conduct empirical studies on real-world datasets to analyze the social mecha- nism in signed directed networks. Guided by related socio- logical theories, we propose a novel Signed Directed Graph Neural Networks model named SDGNN to learn node em- beddings for signed directed networks. The proposed model simultaneously reconstructs link signs, link directions, and signed directed triangles. We validate our model's effective- ness on five real-world datasets, which are commonly used as the benchmark for signed network embeddings. Experi- ments demonstrate the proposed model outperforms existing models, including feature-based methods, network embed- ding methods, and several GNN methods.",
        "authors": [
            "Junjie Huang",
            "Huawei Shen",
            "Liang Hou",
            "Xueqi Cheng"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/SDGNNLearningNodeRepresentationforSignedDirectedNetworks.pdf"
    },
    {
        "internal_id": 437,
        "title": "Stock Selection via Spatiotemporal Hypergraph Attention Network: A Learning to Rank Approach",
        "abstract": "Quantitative trading and investment decision making are in- tricate financial tasks that rely on accurate stock selection. Despite advances in deep learning that have made signifi- cant progress in the complex and highly stochastic stock pre- diction problem, modern solutions face two significant lim- itations. They do not directly optimize the target of invest- ment in terms of profit, and treat each stock as independent from the others, ignoring the rich signals between related stocks' temporal price movements. Building on these limi- tations, we reformulate stock prediction as a learning to rank problem and propose STHAN-SR, a neural hypergraph archi- tecture for stock selection. The key novelty of our work is the proposal of modeling the complex relations between stocks through a hypergraph and a temporal Hawkes attention mech- anism to tailor a new spatiotemporal attention hypergraph network architecture to rank stocks based on profit by jointly modeling stock interdependence and the temporal evolution of their prices. Through experiments on three markets span- ning over six years of data, we show that STHAN-SR signif- icantly outperforms state-of-the-art neural stock forecasting methods. We validate our design choices through ablative and exploratory analyses over STHAN-SR's spatial and temporal components and demonstrate its practical applicability.",
        "authors": [
            "Ramit Sawhney",
            "Shivam Agarwal",
            "Arnav Wadhwa",
            "Tyler Derr",
            "Rajiv Ratn Shah"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/StockSelectionviaSpatiotemporalHypergraphAttentionNetworkALearningtoRankApproach.pdf"
    },
    {
        "internal_id": 438,
        "title": "StatEcoNet: Statistical Ecology Neural Networks for Species Distribution Modeling",
        "abstract": "This paper focuses on a core task in computational sustain- ability and statistical ecology: species distribution modeling (SDM). In SDM, the occurrence pattern of a species on a landscape is predicted by environmental features based on observations at a set of locations. At first, SDM may ap- pear to be a binary classification problem, and one might be inclined to employ classic tools (e.g., logistic regression, support vector machines, neural networks) to tackle it. How- ever, wildlife surveys introduce structured noise (especially under-counting) in the species observations. If unaccounted for, these observation errors systematically bias SDMs. To address the unique challenges of SDM, this paper proposes a framework called StatEcoNet. Specifically, this work employs a graphical generative model in statistical ecol- ogy to serve as the skeleton of the proposed computational framework and carefully integrates neural networks under the framework. The advantages of StatEcoNet over related approaches are demonstrated on simulated datasets as well as bird species data. Since SDMs are critical tools for ecologi- cal science and natural resource management, StatEcoNet may offer boosted computational and analytical powers to a wide range of applications that have significant social im- pacts, e.g., the study and conservation of threatened species.",
        "authors": [
            "Eugene Seo",
            " Xiao Fu",
            " Chelsea Li",
            "Tyler A Hallman",
            " John Kilbride",
            "OR ",
            "OR ",
            "OR "
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/StatEcoNetStatisticalEcologyNeuralNetworksforSpeciesDistributionModeling.pdf"
    },
    {
        "internal_id": 439,
        "title": "Learning to Stop: Dynamic Simulation Monte Carlo Tree Search",
        "abstract": "Monte Carlo tree search (MCTS) has achieved state-of-the-art results in many domains such as Go and Atari games when combining with deep neural networks (DNNs). When more simulations are executed, MCTS can achieve higher perfor- mance but also requires enormous amounts of CPU and GPU resources. However, not all states require a long searching time to identify the best action that the agent can find. For example, in 19x19 Go and NoGo, we found that for more than half of the states, the best action predicted by DNN remains unchanged even after searching 2 minutes. This implies that a significant amount of resources can be saved if we are able to stop the searching earlier when we are confident with the current searching result. In this paper, we propose to achieve this goal by predicting the uncertainty of the current searching status and use the result to decide whether we should stop searching. With our algorithm, called Dynamic Simulation MCTS (DS-MCTS), we can speed up a NoGo agent trained by AlphaZero 2.5 times faster while maintaining a similar winning rate, which is critical for training and conducting ex- periments. Also, under the same average simulation count, our method can achieve a 61% winning rate against the original program.",
        "authors": [
            "Li Cheng Lan",
            "Ti Rong Wu",
            "I Chen Wu",
            "Cho Jui Hsieh",
            "Academia Sinica"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/LearningtoStopDynamicSimulationMonteCarloTreeSearch.pdf"
    },
    {
        "internal_id": 440,
        "title": "DeepTrader: A Deep Reinforcement Learning Approach for Risk-Return Balanced Portfolio Management with Market Conditions Embedding",
        "abstract": "Most existing reinforcement learning (RL)-based port- folio management models do not take into account the market conditions, which limits their performance in risk-return balancing. In this paper, we propose Deep- Trader, a deep RL method to optimize the investment policy. In particular, to tackle the risk-return balancing problem, our model embeds macro market conditions as an indicator to dynamically adjust the proportion be- tween long and short funds, to lower the risk of market fluctuations, with the negative maximum drawdown as the reward function. Additionally, the model involves a unit to evaluate individual assets, which learns dynamic patterns from historical data with the price rising rate as the reward function. Both temporal and spatial de- pendencies between assets are captured hierarchically by a specific type of graph structure. Particularly, we find that the estimated causal structure best captures the interrelationships between assets, compared to industry classification and correlation. The two units are comple- mentary and integrated to generate a suitable portfolio which fits the market trend well and strikes a balance be- tween return and risk effectively. Experiments on three well-known stock indexes demonstrate the superiority of DeepTrader in terms of risk-gain criteria.",
        "authors": [
            "Zhicheng Wang",
            "Biwei Huang",
            "Shikui Tu",
            "Kun Zhang",
            "Lei Xu"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/DeepTraderADeepReinforcementLearningApproachforRiskReturnBalancedPortfolioManagementwithMarketConditionsEmbedding.pdf"
    },
    {
        "internal_id": 441,
        "title": "Online 3D Bin Packing with Constrained Deep Reinforcement Learning",
        "abstract": "We solve a challenging yet practically useful variant of 3D Bin Packing Problem (3D-BPP). In our problem, the agent has limited information about the items to be packed into a single bin, and an item must be packed immediately after its arrival without buffering or readjusting. The item's place- ment also subjects to the constraints of order dependence and physical stability. We formulate this online 3D-BPP as a constrained Markov decision process (CMDP). To solve the problem, we propose an effective and easy-to-implement constrained deep reinforcement learning (DRL) method un- der the actor-critic framework. In particular, we introduce a prediction-and-projection scheme: The agent first predicts a feasibility mask for the placement actions as an auxiliary task and then uses the mask to modulate the action probabilities output by the actor during training. Such supervision and pro- jection facilitate the agent to learn feasible policies very effi- ciently. Our method can be easily extended to handle looka- head items, multi-bin packing, and item re-orienting. We have conducted extensive evaluation showing that the learned pol- icy significantly outperforms the state-of-the-art methods. A preliminary user study even suggests that our method might attain a human-level performance.",
        "authors": [
            "Hang Zhao",
            "Qijin She",
            "Chenyang Zhu",
            "Yin Yang",
            "Kai Xu"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/Online3DBinPackingwithConstrainedDeepReinforcementLearning.pdf"
    },
    {
        "internal_id": 442,
        "title": "Two-Stream Convolution Augmented Transformer for Human Activity Recognition",
        "abstract": "Recognition of human activities is an important task due to its far-reaching applications such as healthcare system, context-aware applications, and security monitoring. Recent- ly, WiFi based human activity recognition (HAR) is becom- ing ubiquitous due to its non-invasiveness. Existing WiFi- based HAR methods regard WiFi signals as a temporal se- quence of channel state information (CSI), and employ deep sequential models (e.g., RNN, LSTM) to automatically cap- ture channel-over-time features. Although being remarkably effective, they suffer from two major drawbacks. Firstly, the granularity of a single temporal point is blindly elementary for representing meaningful CSI patterns. Secondly, the time- over-channel features are also important, and could be a natu- ral data augmentation. To address the drawbacks, we propose a novel Two-stream Convolution Augmented Human Activi- ty Transformer (THAT) model. Our model proposes to uti- lize a two-stream structure to capture both time-over-channel and channel-over-time features, and use the multi-scale con- volution augmented transformer to capture range-based pat- terns. Extensive experiments on four real experiment datasets demonstrate that our model outperforms state-of-the-art mod- els in terms of both effectiveness and efficiency 1.",
        "authors": [
            "Bing Li",
            "Wei Cui",
            "Wei Wang",
            "Le Zhang",
            "Zhenghua Chen",
            "Min Wu"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/TwoStreamConvolutionAugmentedTransformerforHumanActivityRecognition.pdf"
    },
    {
        "internal_id": 443,
        "title": "Capturing Uncertainty in Unsupervised GPS Trajectory Segmentation Using Bayesian Deep Learning",
        "abstract": "Intelligent transportation management requires not only statistical information on users' mobility patterns, but also knowledge of their corresponding transportation modes. While GPS trajectories can be readily obtained from GPS sensors found in modern smartphones and vehicles, these massive geospatial data are neither au- tomatically annotated nor segmented by transportation mode, subsequently complicating transportation mode identification. In addition, predictive uncertainty caused by the learned model parameters or variable noise in GPS sensor readings typically remains unaccounted for. To jointly address the above issues, we propose a Bayesian deep learning framework for unsupervised GPS trajectory segmentation. After unlabeled GPS tra- jectories are preprocessed into sequences of motion features, they are used in unsupervised training of a channel-calibrated temporal convolutional neural net- work for timestep-level transportation mode identifi- cation. At test time, we approximate variational infer- ence via Monte Carlo dropout sampling, leveraging the mean and variance of the predicted distributions to clas- sify each input timestep and estimate its predictive un- certainty, respectively. The proposed approach outper- forms both its non-Bayesian variant and established GPS trajectory segmentation baselines on Microsoft's Geolife dataset without using any labels.",
        "authors": [
            "Christos Markos"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/CapturingUncertaintyinUnsupervisedGPSTrajectorySegmentationUsingBayesianDeepLearning.pdf"
    },
    {
        "internal_id": 444,
        "title": "Traffic Flow Prediction with Vehicle Trajectories",
        "abstract": "This paper proposes a spatiotemporal deep learning frame- work, Trajectory-based Graph Neural Network (TrGNN), that mines the underlying causality of flows from historical vehi- cle trajectories and incorporates that into road traffic predic- tion. The vehicle trajectory transition patterns are studied to explicitly model the spatial traffic demand via graph propa- gation along the road network; an attention mechanism is de- signed to learn the temporal dependencies based on neighbor- hood traffic status; and finally, a fusion of multi-step predic- tion is integrated into the graph neural network design. The proposed approach is evaluated with a real-world trajectory dataset. Experiment results show that the proposed TrGNN model achieves over 5% error reduction when compared with the state-of-the-art approaches across all metrics for normal traffic, and up to 14% for atypical traffic during peak hours or abnormal events. The advantage of trajectory transitions es- pecially manifest itself in inferring high fluctuation of flows as well as non-recurrent flow patterns.",
        "authors": [
            "Mingqian Li",
            " Panrong Tong",
            " Mo Li",
            " Zhongming Jin",
            " Jianqiang Huang",
            " Alibaba Group"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/TrafficFlowPredictionwithVehicleTrajectories.pdf"
    },
    {
        "internal_id": 445,
        "title": "MIMOSA: Multi-constraint Molecule Sampling for Molecule Optimization",
        "abstract": "Molecule optimization is a fundamental task for accelerat- ing drug discovery, with the goal of generating new valid molecules that maximize multiple drug properties while maintaining similarity to the input molecule. Existing gen- erative models and reinforcement learning approaches made initial success, but still face difficulties in simultaneously optimizing multiple drug properties. To address such chal- lenges, we propose the MultI-constraint MOlecule SAm- pling (MIMOSA) approach, a sampling framework to use in- put molecule as an initial guess and sample molecules from the target distribution. MIMOSA first pretrains two property- agnostic graph neural networks (GNNs) for molecule topol- ogy and substructure-type prediction, where a substructure can be either atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and employs three basic sub- structure operations (add, replace, delete) to generate new molecules and associated weights. The weights can encode multiple constraints including similarity and drug property constraints, upon which we select promising molecules for next iteration. MIMOSA enables flexible encoding of multiple property- and similarity-constraints and can efficiently gen- erate new molecules that satisfy various property constraints and achieved up to 49.1% relative improvement over the best baseline in terms of success rate.",
        "authors": [
            "Tianfan Fu",
            " Cao Xiao",
            " Xinhao Li",
            " Lucas Glass",
            " Jimeng Sun",
            "Urbana Champaign"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/MIMOSAMulticonstraintMoleculeSamplingforMoleculeOptimization.pdf"
    },
    {
        "internal_id": 446,
        "title": "Embracing Domain Differences in Fake News: Cross-domain Fake News Detection using Multi-modal Data",
        "abstract": "With the rapid evolution of social media, fake news has be- come a significant social problem, which cannot be addressed in a timely manner using manual investigation. This has mo- tivated numerous studies on automating fake news detection. Most studies explore supervised training models with differ- ent modalities (e.g., text, images, and propagation networks) of news records to identify fake news. However, the perfor- mance of such techniques generally drops if news records are coming from different domains (e.g., politics, entertain- ment), especially for domains that are unseen or rarely-seen during training. As motivation, we empirically show that news records from different domains have significantly dif- ferent word usage and propagation patterns. Furthermore, due to the sheer volume of unlabelled news records, it is chal- lenging to select news records for manual labelling so that the domain-coverage of the labelled dataset is maximized. Hence, this work: (1) proposes a novel framework that jointly preserves domain-specific and cross-domain knowledge in news records to detect fake news from different domains; and (2) introduces an unsupervised technique to select a set of unlabelled informative news records for manual labelling, which can be ultimately used to train a fake news detection model that performs well for many domains while minimiz- ing the labelling cost. Our experiments show that the integra- tion of the proposed fake news model and the selective an- notation approach achieves state-of-the-art performance for cross-domain news datasets, while yielding notable improve- ments for rarely-appearing domains in news datasets.",
        "authors": [
            "Amila Silva",
            "Ling Luo",
            "Shanika Karunasekera",
            "Christopher Leckie"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/EmbracingDomainDifferencesinFakeNewsCrossdomainFakeNewsDetectionusingMultimodalData.pdf"
    },
    {
        "internal_id": 447,
        "title": "Community-Aware Multi-Task Transportation Demand Prediction",
        "abstract": "Transportation demand prediction is of great importance to urban governance and has become an essential function in many online applications. While many efforts have been made for regional transportation demand prediction, predict- ing the diversified transportation demand for different com- munities (e.g., the aged, the juveniles) remains an unexplored problem. However, this task is challenging because of the joint influence of spatio-temporal correlation among regions and implicit correlation among different communities. To this end, in this paper, we propose the Multi-task Spatio- Temporal Network with Mutually-supervised Adaptive task grouping (Ada-MSTNet) for community-aware transporta- tion demand prediction. Specifically, we first construct a se- quence of multi-view graphs from both spatial and commu- nity perspectives, and devise a spatio-temporal neural net- work to simultaneously capture the sophisticated correla- tions between regions and communities, respectively. Then, we propose an adaptively clustered multi-task learning mod- ule, where the prediction of each region-community specific transportation demand is regarded as distinct task. Moreover, a mutually supervised adaptive task grouping strategy is in- troduced to softly cluster each task into different task groups, by leveraging the supervision signal from one another graph view. In such a way, Ada-MSTNet is not only able to share common knowledge among highly related communities and regions, but also shield the noise from unrelated tasks in an end-to-end fashion. Finally, extensive experiments on two real-world datasets demonstrate the effectiveness of our ap- proach compared with seven baselines.",
        "authors": [
            "Hao Liu",
            "Qiyu Wu",
            "Fuzhen Zhuang",
            "Xinjiang Lu",
            "Dejing Dou",
            "Hui Xiong"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/CommunityAwareMultiTaskTransportationDemandPrediction.pdf"
    },
    {
        "internal_id": 448,
        "title": "Automated Symbolic Law Discovery: A Computer Vision Approach",
        "abstract": "One of the most exciting applications of modern artificial in- telligence is to automatically discover scientific laws from ex- perimental data. This is not a trivial problem as it involves searching for a complex mathematical relationship over a large set of explanatory variables and operators that can be combined in an infinite number of ways. Inspired by the incredible success of deep learning in com- puter vision, we tackle this problem by adapting various suc- cessful network architectures into the symbolic law discovery pipeline. The novelty of our approach is in (1) encoding the input data as an image with super-resolution, (2) developing an appropriate deep network pipeline, and (3) predicting the importance of each mathematical operator from the relation- ship image. This allows us to prior the exponentially large search with the predicted importance of the symbolic opera- tors, which can significantly accelerate the discovery process. We apply our model to a variety of plausible relationships\u2014 both simulated and from physics and mathematics domains\u2014 involving different dimensions and constituents. We show that our model is able to identify the underlying operators from data, achieving a high accuracy and AUC (91% and 0.96 on average resp.) for systems with as many as ten inde- pendent variables. Our method significantly outperforms the current state of the art in terms of data fitting (R2), discov- ery rate (recovering the true relationship), and succinctness (output formula complexity). The discovered equations can be seen as first drafts of scientific laws that can be helpful to the scientists for (1) hypothesis building, and (2) understand- ing the complex underlying structure of the studied phenom- ena. Our approach holds a real promise to help speed up the rate of scientific discovery.",
        "authors": [
            "Hengrui Xing",
            "Ansaf Salleb Aouissi",
            "Nakul Verma",
            "New York"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/AutomatedSymbolicLawDiscoveryAComputerVisionApproach.pdf"
    },
    {
        "internal_id": 449,
        "title": "Deep Partial Rank Aggregation for Personalized Attributes",
        "abstract": "In this paper, we study the problem of how to aggregate pair- wise personalized attributes (PA) annotations (e.g., Shoes A is more comfortable than B) from different annotators on the crowdsourcing platforms, which is an emerging topic gaining increasing attention in recent years. Given the crowdsourced annotations, the majority of the traditional literature assumes that all the pairs in the collected dataset are distinguishable. However, this assumption is incompatible with how humans perceive attributes since indistinguishable pairs are ubiqui- tous for the annotators due to the limitation of human percep- tion. To attack this problem, we propose a novel deep pre- diction model that could simultaneously detect the indistin- guishable pairs and aggregate ranking results for distinguish- able pairs. First of all, we represent the pairwise annotations as a multi-graph. Based on such data structure, we propose an end-to-end partial ranking model which consists of a deep backbone architecture and a probabilistic model that captures the generative process of the partial rank annotations. Specif- ically, to recognize the indistinguishable pairs, the probabilis- tic model we proposed is equipped with an adaptive percep- tion threshold, where indistinguishable pairs could be auto- matically detected when the absolute value of the score dif- ference is below the learned threshold. In our empirical stud- ies, we perform a series of experiments on three real-world datasets: LFW-10, Shoes, and Sun. The corresponding results consistently show the superiority of our proposed model.",
        "authors": [
            "Qianqian Xu ",
            "Zhiyong Yang ",
            "Zuyao Chen ",
            "Yangbangyan Jiang ",
            "Xiaochun Cao ",
            "Yuan Yao ",
            "Qingming Huang ",
            "Hong Kong"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/DeepPartialRankAggregationforPersonalizedAttributes.pdf"
    },
    {
        "internal_id": 450,
        "title": "Model-Agnostic Fits for Understanding Information Seeking Patterns in Humans",
        "abstract": "In decision making tasks under uncertainty, humans dis- play characteristic biases in seeking, integrating, and acting upon information relevant to the task. Here, we reexamine data from previous carefully designed exper- iments, collected at scale, that measured and catalogued these biases in aggregate form. We design deep learning models that replicate these biases in aggregate, while also capturing individual variation in behavior. A key finding of our work is that paucity of data collected from each individual subject can be overcome by sampling large numbers of subjects from the population, while still capturing individual differences. We predict human behavior with high accuracy without making any as- sumptions about task goals, reward structure, or individ- ual biases, thus providing a model-agnostic fit to human behavior in the task. Such an approach can sidestep po- tential limitations in modeler-specified inductive biases, and has implications for computational modeling of hu- man cognitive function in general, and of human-AI in- terfaces in particular.",
        "authors": [
            "Soumya Chatterjee",
            "Pradeep Shenoy "
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/ModelAgnosticFitsforUnderstandingInformationSeekingPatternsinHumans.pdf"
    },
    {
        "internal_id": 451,
        "title": "Deep Contextual Clinical Prediction with Reverse Distillation",
        "abstract": "Healthcare providers are increasingly using machine learn- ing to predict patient outcomes to make meaningful interven- tions. However, despite innovations in this area, deep learn- ing models often struggle to match performance of shallow linear models in predicting these outcomes, making it dif- ficult to leverage such techniques in practice. In this work, motivated by the task of clinical prediction from insurance claims, we present a new technique called reverse distilla- tion which pretrains deep models by using high-performing linear models for initialization. We make use of the longi- tudinal structure of insurance claims datasets to develop Self Attention with Reverse Distillation, or SARD, an architecture that utilizes a combination of contextual embedding, tempo- ral embedding and self-attention mechanisms and most crit- ically is trained via reverse distillation. SARD outperforms state-of-the-art methods on multiple clinical prediction out- comes, with ablation studies revealing that reverse distillation is a primary driver of these improvements. Code is available at https://github.com/clinicalml/omop-learn.",
        "authors": [
            "Rohan Kodialam",
            " Rebecca Boiarsky",
            " Justin Lim",
            "Aditya Sai",
            " Neil Dixit",
            " David Sontag",
            "MIT CSAIL IMES",
            "Independence Blue Cross"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/DeepContextualClinicalPredictionwithReverseDistillation.pdf"
    },
    {
        "internal_id": 452,
        "title": "A Bottom-Up DAG Structure Extraction Model for Math Word Problems",
        "abstract": "Research on automatically solving mathematical word prob- lems (MWP) has a long history. Most recent works adopt the Seq2Seq approach to predict the result equations as a se- quence of quantities and operators. Although result equations can be written as a sequence, it is essentially a structure. More precisely, it is a Direct Acyclic Graph (DAG) whose leaf nodes are the quantities, and internal and root nodes are arith- metic or comparison operators. In this paper, we propose a novel Seq2DAG approach to extract the equation set directly as a DAG structure. It extracts the structure in a bottom-up fashion by aggregating quantities and sub-expressions layer by layer iteratively. The advantages of our approach are three- fold: it is intrinsically suitable to solve multivariate problems, it always outputs valid structure, and its computation satis- fies commutative law for +, \u00d7 and =. Experimental results on DRAW1K and Math23K datasets demonstrate that our model outperforms state-of-the-art deep learning methods. We also conduct detailed analysis on the results to show the strengths and limitations of our approach.",
        "authors": [
            "Yixuan Cao",
            "Feng Hong",
            "Hongwei Li",
            "Ping Luo"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/ABottomUpDAGStructureExtractionModelforMathWordProblems.pdf"
    },
    {
        "internal_id": 453,
        "title": "Graph Neural Network to Dilute Outliers for Refactoring Monolith Application",
        "abstract": "Microservices are becoming the defacto design choice for software architecture. It involves partitioning the software components into finer modules such that the development can happen independently. It also provides natural benefits when deployed on the cloud since resources can be allocated dy- namically to necessary components based on demand. There- fore, enterprises as part of their journey to cloud, are increas- ingly looking to refactor their monolith application into one or more candidate microservices; wherein each service con- tains a group of software entities (e.g., classes) that are re- sponsible for a common functionality. Graphs are a natural choice to represent a software system. Each software entity can be represented as nodes and its dependencies with other entities as links. Therefore, this problem of refactoring can be viewed as a graph based clustering task. In this work, we propose a novel method to adapt the recent advancements in graph neural networks in the context of code to better under- stand the software and apply them in the clustering task. In that process, we also identify the outliers in the graph which can be directly mapped to top refactor candidates in the soft- ware. Our solution is able to improve state-of-the-art perfor- mance compared to works from both software engineering and existing graph representation based techniques.",
        "authors": [
            "Utkarsh Desai",
            "Srikanth Tamilselvam"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/GraphNeuralNetworktoDiluteOutliersforRefactoringMonolithApplication.pdf"
    },
    {
        "internal_id": 454,
        "title": "Window Loss for Bone Fracture Detection and Localization in X-ray Images with Point-based Annotation",
        "abstract": "Object detection methods are widely adopted for computer- aided diagnosis using medical images. Anomalous findings are usually treated as objects that are described by bound- ing boxes. Yet, many pathological findings, e.g., bone frac- tures, cannot be clearly defined by bounding boxes, owing to considerable instance, shape and boundary ambiguities. This makes bounding box annotations, and their associated losses, highly ill-suited. In this work, we propose a new bone fracture detection method for X-ray images, based on a labor effective and flexible annotation scheme suitable for abnormal findings with no clear object-level spatial extents or boundaries. Our method employs a simple, intuitive, and informative point- based annotation protocol to mark localized pathology infor- mation. To address the uncertainty in the fracture scales anno- tated via point(s), we convert the annotations into pixel-wise supervision that uses lower and upper bounds with positive, negative, and uncertain regions. A novel Window Loss is sub- sequently proposed to only penalize the predictions outside of the uncertain regions. Our method has been extensively evaluated on 4410 pelvic X-ray images of unique patients. Experiments demonstrate that our method outperforms previ- ous state-of-the-art image classification and object detection baselines by healthy margins, with an AUROC of 0.983 and FROC score of 89.6%.",
        "authors": [
            "Xinyu Zhang",
            "Yirui Wang",
            "Chi Tung Cheng",
            "Le Lu",
            "Adam P Harrison",
            "Jing Xiao",
            "Chien Hung Liao",
            "Shun Miao"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/WindowLossforBoneFractureDetectionandLocalizationinXrayImageswithPointbasedAnnotation.pdf"
    },
    {
        "internal_id": 455,
        "title": "Estimating Calibrated Individualized Survival Curves with Deep Learning",
        "abstract": "In survival analysis, deep learning approaches have been pro- posed for estimating an individual's probability of survival over some time horizon. Such approaches can capture com- plex non-linear relationships, without relying on restrictive assumptions regarding the relationship between an individ- ual's characteristics and their underlying survival process. To date, however, these methods have focused primarily on op- timizing discriminative performance and have ignored model calibration. Well-calibrated survival curves present realistic and meaningful probabilistic estimates of the true underly- ing survival process for an individual. However, due to the lack of ground-truth regarding the underlying stochastic pro- cess of survival for an individual, optimizing and measur- ing calibration in survival analysis is an inherently difficult task. In this work, we i) highlight the shortcomings of exist- ing approaches in terms of calibration and ii) propose a new training scheme for optimizing deep survival analysis models that maximizes discriminative performance, subject to good calibration. Compared to state-of-the-art approaches across two publicly available datasets, our proposed training scheme leads to significant improvements in calibration, while main- taining good discriminative performance.",
        "authors": [
            "Fahad Kamran",
            "Jenna Wiens",
            "Ann Arbor"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/EstimatingCalibratedIndividualizedSurvivalCurveswithDeepLearning.pdf"
    },
    {
        "internal_id": 456,
        "title": "Modeling the Momentum Spillover Effect for Stock Prediction via Attribute-Driven Graph Attention Networks",
        "abstract": "In finance, the momentum spillovers of listed firms is well acknowledged. Only few studies predicted the trend of one firm in terms of its relevant firms. A common strategy of the pilot work is to adopt graph convolution networks (GCNs) with some predefined firm relations. However, momentum spillovers are propagated via a variety of firm relations, of which the bridging importance varies with time. Restrict- ing to several predefined relations inevitably makes noise and thus misleads stock predictions. In addition, traditional GCNs transfer and aggregate the peer influences without con- sidering the states of both connected firms once a connec- tion is built. Such non-attribute sensibility makes traditional GCNs inappropriate to deal with the attribute-sensitive mo- mentum spillovers of listed firms wherein the abnormal price drop of one firm may not spill over if the trade volume of this decreasing price is small or the prices of the linked firms are undervalued. In this study, we propose an attribute-driven graph attention network (AD-GAT) to address both prob- lems in modeling momentum spillovers. This is achieved by element-wisely multiplying the nonlinear transformation of the attributes of the connected firms with the attributes of the source firm to consider its attribute-sensitive momentum spillovers, and applying the unmasked attention mechanism to infer the general dynamic firm relation from observed mar- ket signals fused by a novel tensor-based feature extractor. Experiments on the three-year data of the S&P 500 demon- strate the superiority of the proposed framework over state- of-the-art algorithms, including GCN, eLSTM, and TGC.",
        "authors": [
            "Rui Cheng",
            "Qing Li",
            "Fintech Innovation Center"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/ModelingtheMomentumSpilloverEffectforStockPredictionviaAttributeDrivenGraphAttentionNetworks.pdf"
    },
    {
        "internal_id": 457,
        "title": "A Spatial Regulated Patch-Wise Approach for Cervical Dysplasia Diagnosis",
        "abstract": "Cervical dysplasia diagnosis via visual investigation is a chal- lenging problem. Recent approaches use deep learning tech- niques to extract features and require the downsampling of high-resolution cervical screening images to smaller sizes for training. Such a reduction may result in the loss of visual de- tails that appear weakly and locally within a cervical image. To overcome this challenge, our work divides an image into patches and then represents it from patch features. We aggre- gate patch patterns into an image feature in a weighted man- ner by considering the patch\u2013image relationship. The weights are visualized as a heatmap to explain where the diagnosis results come from. We further introduce a spatial regulator to guide the classifier to focus on the cervix region and to adjust the weight distribution, without requiring any manual annotations of the cervix region. A novel iterative algorithm is designed to refine the regulator, which is able to capture the variations in cervix center locations and shapes. Experi- ments on an 18-year real-world dataset indicate a minimal of 3.47%, 4.59%, 8.54% improvements over the state-of-the-art in accuracy, F1, and recall measures, respectively.",
        "authors": [
            "Ying Zhang",
            "Yifang Yin",
            "Zhenguang Liu",
            "Roger Zimmermann"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/ASpatialRegulatedPatchWiseApproachforCervicalDysplasiaDiagnosis.pdf"
    },
    {
        "internal_id": 458,
        "title": "CardioGAN: Attentive Generative Adversarial Network with Dual Discriminators for Synthesis of ECG from PPG",
        "abstract": "Electrocardiogram (ECG) is the electrical measurement of cardiac activity, whereas Photoplethysmogram (PPG) is the optical measurement of volumetric changes in blood circula- tion. While both signals are used for heart rate monitoring, from a medical perspective, ECG is more useful as it car- ries additional cardiac information. Despite many attempts toward incorporating ECG sensing in smartwatches or simi- lar wearable devices for continuous and reliable cardiac mon- itoring, PPG sensors are the main feasible sensing solution available. In order to tackle this problem, we propose Car- dioGAN, an adversarial model which takes PPG as input and generates ECG as output. The proposed network utilizes an attention-based generator to learn local salient features, as well as dual discriminators to preserve the integrity of gen- erated data in both time and frequency domains. Our experi- ments show that the ECG generated by CardioGAN provides more reliable heart rate measurements compared to the origi- nal input PPG, reducing the error from 9.74 beats per minute (measured from the PPG) to 2.89 (measured from the gener- ated ECG).",
        "authors": [
            "Pritam Sarkar",
            "Ali Etemad"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/CardioGANAttentiveGenerativeAdversarialNetworkwithDualDiscriminatorsforSynthesisofECGfromPPG.pdf"
    },
    {
        "internal_id": 459,
        "title": "A Hierarchical Approach to Multi-Event Survival Analysis \u2217",
        "abstract": "In multi-event survival analysis, one aims to predict the prob- ability of multiple different events occurring over some time horizon. One typically assumes that the timing of events is drawn from some distribution conditioned on an indi- vidual's covariates. However, during training, one does not have access to this distribution, and the natural variation in the observed event times makes the task of survival pre- diction challenging, on top of the potential interdependence among events. To address this issue, we introduce a novel approach for multi-event survival analysis that models the probability of event occurrence hierarchically at different time scales, using coarse predictions (e.g., monthly predic- tions) to iteratively guide predictions at finer and finer grained time scales (e.g., daily predictions). We evaluate the pro- posed approach across several publicly available datasets in terms of both intra-event, inter-individual (global) and intra- individual, inter-event (local) consistency. We show that the proposed method consistently outperforms well-accepted and commonly used approaches to multi-event survival analysis. When estimating survival curves for Alzheimer's disease and mortality, our approach achieves a C-index of 0.91 (95% CI 0.88-0.93) and a local consistency score of 0.97 (95% CI 0.94-0.98) compared to a C-index of 0.75 (95% CI 0.70-0.80) and a local consistency score of 0.94 (95% CI 0.91-0.97) when modeling each event separately. Overall, our approach improves the accuracy of survival predictions by iteratively reducing the original task to a set of nested, simpler subtasks.",
        "authors": [
            " Yifei He",
            "Ann Arbor MI"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/AHierarchicalApproachtoMultiEventSurvivalAnalysis.pdf"
    },
    {
        "internal_id": 460,
        "title": "RevMan: Revenue-aware Multi-task Online Insurance Recommendation",
        "abstract": "Online insurance is a new type of e-commerce with exponen- tial growth. An effective recommendation model that maxi- mizes the total revenue of insurance products listed in mul- tiple customized sales scenarios is crucial for the success of online insurance business. Prior recommendation models are ineffective because they fail to characterize the complex re- latedness of insurance products in multiple sales scenarios and maximize the overall conversion rate rather than the to- tal revenue. Even worse, it is impractical to collect training data online for total revenue maximization due to the business logic of online insurance. We propose RevMan, a Revenue- aware Multi-task Network for online insurance recommenda- tion. RevMan adopts an adaptive attention mechanism to al- low effective feature sharing among complex insurance prod- ucts and sales scenarios. It also designs an efficient offline learning mechanism to learn the rank that maximizes the ex- pected total revenue, by reusing training data and model for conversion rate maximization. Extensive offline and online evaluations show that RevMan outperforms the state-of-the- art recommendation systems for e-commerce.",
        "authors": [
            "Yu Li",
            " Yi Zhang",
            " Lu Gan",
            " Gengwei Hong",
            " Zimu Zhou"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/RevManRevenueawareMultitaskOnlineInsuranceRecommendation.pdf"
    },
    {
        "internal_id": 461,
        "title": "Asynchronous Stochastic Gradient Descent for Extreme-Scale Recommender Systems",
        "abstract": "Recommender systems are influential for many internet ap- plications. As the size of the dataset provided for a recom- mendation model grows rapidly, how to utilize such amount of data effectively matters a lot. For a typical Click-Through- Rate(CTR) prediction model, the amount of daily samples can probably be up to hundreds of terabytes, which reaches dozens of petabytes at an extreme-scale when we take sev- eral days into consideration. Such data makes it essential to train the model parallelly and continuously. Traditional asyn- chronous stochastic gradient descent (ASGD) and its vari- ants are proved efficient but often suffer from stale gradients. Hence, the model convergence tends to be worse as more workers are used. Moreover, the existing adaptive optimiz- ers, which are friendly to sparse data, stagger in long-term training due to the significant imbalance between new and accumulated gradients. To address the challenges posed by extreme-scale data, we propose: 1) Staleness normalization and data normalization to eliminate the turbulence of stale gradients when training asynchronously in hundreds and thousands of workers; 2) SWAP, a novel framework for adaptive optimizers to bal- ance the new and historical gradients by taking sampling pe- riod into consideration. We implement these approaches in TensorFlow and apply them to CTR tasks in real-world e- commerce scenarios. Experiments show that the number of workers in asynchronous training can be extended to 3000 with guaranteed convergence, and the final AUC is improved by more than 5 percentage.",
        "authors": [
            "Lewis Liu",
            "Kun Zhao",
            " Alibaba Group"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/AsynchronousStochasticGradientDescentforExtremeScaleRecommenderSystems.pdf"
    },
    {
        "internal_id": 462,
        "title": "Sub-Seasonal Climate Forecasting via Machine Learning: Challenges, Analysis, and Advances",
        "abstract": "Sub-seasonal forecasting (SSF) focuses on predicting key variables such as temperature and precipitation on the 2-week to 2-month time scale. Skillful SSF would have immense so- cietal value in such areas as agricultural productivity, water resource management, and emergency planning for extreme weather events. However, SSF is considered more challeng- ing than either weather prediction or even seasonal predic- tion, and is still a largely understudied problem. In this pa- per, we carefully investigate 10 Machine Learning (ML) ap- proaches to sub-seasonal temperature forecasting over the contiguous U.S. on the SSF dataset we collect, including a variety of climate variables from the atmosphere, ocean, and land. Because of the complicated atmosphere-land-ocean couplings and the limited amount of good quality observa- tional data, SSF imposes a great challenge for ML despite the recent advances in various domains. Our results indicate that suitable ML models, e.g., XGBoost, to some extent, capture the predictability on sub-seasonal time scales and can outper- form the climatological baselines, while Deep Learning (DL) models barely manage to match the best results with carefully designed architecture. Besides, our analysis and exploration provide insights on important aspects to improve the qual- ity of sub-seasonal forecasts, e.g., feature representation and model architecture. The SSF dataset and code 1 are released with this paper for use by the broader research community.",
        "authors": [
            "Sijie He ",
            "Xinyan Li ",
            "Timothy DelSole ",
            "Pradeep Ravikumar ",
            "Arindam Banerjee ",
            "Twin Cities"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/SubSeasonalClimateForecastingviaMachineLearningChallengesAnalysisandAdvances.pdf"
    },
    {
        "internal_id": 463,
        "title": "Differentially Private Link Prediction with Protected Connections",
        "abstract": "Link prediction (LP) algorithms propose to each node a ranked list of nodes that are currently non-neighbors, as the most likely candidates for future linkage. Owing to increas- ing concerns about privacy, users (nodes) may prefer to keep some of their connections protected or private. Motivated by this observation, our goal is to design a differentially pri- vate LP algorithm, which trades off between privacy of the protected node-pairs and the link prediction accuracy. More specifically, we first propose a form of differential privacy on graphs, which models the privacy loss only of those node- pairs which are marked as protected. Next, we develop DPLP, a learning to rank algorithm, which applies a monotone trans- form to base scores from a non-private LP system, and then adds noise. DPLP is trained with a privacy induced ranking loss, which optimizes the ranking utility for a given maximum allowed level of privacy leakage of the protected node-pairs. Under a recently-introduced latent node embedding model, we present a formal trade-off between privacy and LP util- ity. Extensive experiments with several real-life graphs and several LP heuristics show that DPLP can trade off between privacy and predictive performance more effectively than sev- eral alternatives.",
        "authors": [
            "Abir De",
            "Soumen Chakrabarti"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/DifferentiallyPrivateLinkPredictionwithProtectedConnections.pdf"
    },
    {
        "internal_id": 464,
        "title": "Complex Coordinate-Based Meta-Analysis with Probabilistic Programming",
        "abstract": "With the growing number of published functional magnetic resonance imaging (fMRI) studies, meta-analysis databases and models have become an integral part of brain mapping research. Coordinate-based meta-analysis (CBMA) databases are built by extracting both coordinates of reported peak ac- tivations and term associations using natural language pro- cessing techniques from neuroimaging studies. Solving term- based queries on these databases makes it possible to obtain statistical maps of the brain related to specific cognitive pro- cesses. However, existing tools for analysing CBMA data are limited in their expressivity to propositional logic, restricting the variety of their queries. Moreover, with tools like Neu- rosynth, term-based queries on multiple terms often lead to power failure, because too few studies from the database con- tribute to the statistical estimations. We design a probabilistic domain-specific language (DSL) standing on Datalog and one of its probabilistic extensions, CP-Logic, for expressing and solving complex logic-based queries. We show how CBMA databases can be encoded as probabilistic programs. Using the joint distribution of their Bayesian network translation, we show that solutions of queries on these programs compute the right probability distributions of voxel activations. We explain how recent lifted query processing algorithms make it possible to scale to the size of large neuroimaging data, where knowledge compilation techniques fail to solve queries fast enough for practical applications. Finally, we introduce a method for relating studies to terms probabilistically, leading to better solutions for two-term conjunctive queries (CQs) on smaller databases. We demonstrate results for two-term CQs, both on simulated meta-analysis databases and on the widely used Neurosynth database.",
        "authors": [
            "Valentin Iovene",
            "Gaston E Zanitti",
            "Demian Wassermann"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/ComplexCoordinateBasedMetaAnalysiswithProbabilisticProgramming.pdf"
    },
    {
        "internal_id": 465,
        "title": "Commission Fee is not Enough: A Hierarchical Reinforced Framework for Portfolio Management",
        "abstract": "Portfolio management via reinforcement learning is at the forefront of fintech research, which explores how to opti- mally reallocate a fund into different financial assets over the long term by trial-and-error. Existing methods are impractical since they usually assume each reallocation can be finished immediately and thus ignoring the price slippage as part of the trading cost. To address these issues, we propose a hierar- chical reinforced stock trading system for portfolio manage- ment (HRPM). Concretely, we decompose the trading process into a hierarchy of portfolio management over trade execu- tion and train the corresponding policies. The high-level pol- icy gives portfolio weights at a lower frequency to maximize the long term profit and invokes the low-level policy to sell or buy the corresponding shares within a short time window at a higher frequency to minimize the trading cost. We train two levels of policies via pre-training scheme and iterative training scheme for data efficiency. Extensive experimental results in the U.S. market and the China market demonstrate that HRPM achieves significant improvement against many state-of-the-art approaches.",
        "authors": [
            "Rundong Wang",
            "Hongxin Wei",
            "Bo An",
            "Zhouyan Feng",
            "Jun Yao"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/CommissionFeeisnotEnoughAHierarchicalReinforcedFrameworkforPortfolioManagement.pdf"
    },
    {
        "internal_id": 466,
        "title": "In-game Residential Home Planning via Visual Context-aware Global Relation Learning",
        "abstract": "In this paper, we propose an effective global relation learning algorithm to recommend an appropriate location of a building unit for in-game customization of residential home complex. Given a construction layout, we propose a visual context- aware graph generation network that learns the implicit global relations among the scene components and infers the loca- tion of a new building unit. The proposed network takes as input the scene graph and the corresponding top-view depth image. It provides the location recommendations for a newly- added building units by learning an auto-regressive edge dis- tribution conditioned on existing scenes. We also introduce a global graph-image matching loss to enhance the awareness of essential geometry semantics of the site. Qualitative and quantitative experiments demonstrate that the recommended location well reflects the implicit spatial rules of components in the residential estates, and it is instructive and practical to locate the building units in the 3D scene of the complex con- struction.",
        "authors": [
            "Lijuan Liu",
            "Yin Yang",
            "Yi Yuan",
            "Tianjia Shao",
            "He Wang",
            "Kun Zhou"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/IngameResidentialHomePlanningviaVisualContextawareGlobalRelationLearning.pdf"
    },
    {
        "internal_id": 467,
        "title": "XraySyn: Realistic View Synthesis From a Single Radiograph Through CT Priors",
        "abstract": "A radiograph visualizes the internal anatomy of a patient through the use of X-ray, which projects 3D information onto a 2D plane. Hence, radiograph analysis naturally requires physi- cians to relate their prior knowledge about 3D human anatomy to 2D radiographs. Synthesizing novel radiographic views in a small range can assist physicians in interpreting anatomy more reliably; however, radiograph view synthesis is heavily ill-posed, lacking in paired data, and lacking in differentiable operations to leverage learning-based approaches. To address these problems, we use Computed Tomography (CT) for ra- diograph simulation and design a differentiable projection algorithm, which enables us to achieve geometrically consis- tent transformations between the radiography and CT domains. Our method, XraySyn, can synthesize novel views on real ra- diographs through a combination of realistic simulation and finetuning on real radiographs. To the best of our knowledge, this is the first work on radiograph view synthesis. We show that by gaining an understanding of radiography in 3D space, our method can be applied to radiograph bone extraction and suppression without requiring groundtruth bone labels.",
        "authors": [
            "Cheng Peng",
            "Haofu Liao",
            "Gina Wong ",
            "Jiebo Luo",
            "S Kevin Zhou",
            "Rama Chellappa"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/XraySynRealisticViewSynthesisFromaSingleRadiographThroughCTPriors.pdf"
    },
    {
        "internal_id": 468,
        "title": "Dual-Octave Convolution for Accelerated Parallel MR Image Reconstruction",
        "abstract": "Magnetic resonance (MR) image acquisition is an inherently prolonged process, whose acceleration by obtaining multiple undersampled images simultaneously through parallel imaging has always been the subject of research. In this paper, we propose the Dual-Octave Convolution (Dual-OctConv), which is capable of learning multi-scale spatial-frequency features from both real and imaginary components, for fast parallel MR image reconstruction. By reformulating the complex operations using octave convolutions, our model shows a strong ability to capture richer representations of MR images, while at the same time greatly reducing the spatial redundancy. More specifically, the input feature maps and convolutional kernels are first split into two components (i.e., real and imaginary), which are then divided into four groups according to their spatial frequencies. Then, our Dual-OctConv conducts intra-group information updating and inter-group information exchange to aggregate the contextual information across different groups. Our framework provides two appealing benefits: (i) it encourages interactions between real and imaginary components at various spatial frequencies to achieve richer representational capacity, and (ii) it enlarges the receptive field by learning multiple spatial-frequency features of both the real and imaginary components. We evaluate the performance of the proposed model on the acceleration of multi-coil MR image reconstruction. Extensive experiments are conducted on an in vivo knee dataset under different undersampling patterns and acceleration factors. The experimental results demonstrate the superiority of our model in accelerated parallel MR image reconstruction. Our code is available at: github.com/chunmeifeng/Dual-OctConv.",
        "authors": [
            "Chun Mei Feng",
            "Zhanyuan Yang",
            "Geng Chen",
            "Yong Xu",
            "Ling Shao",
            "Abu Dhabi"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/DualOctaveConvolutionforAcceleratedParallelMRImageReconstruction.pdf"
    },
    {
        "internal_id": 469,
        "title": "Towered Actor Critic For Handling Multiple Action Types In Reinforcement Learning For Drug Discovery",
        "abstract": "Introduction Reinforcement Learning (RL) has pushed the frontiers of various domains such as robotics (OpenAI et al. 2019; Kahn, Abbeel, and Levine 2020), game playing agents (Silver et al. 2017; Anthony, Tian, and Barber 2017), and economics (Zheng et al. 2020). RL is a core problem of artificial in- telligence where an agent is situated in an environment and must learn to adapt its policy to maximize a reward signal. At each time step t, the agent interacts with an MDP described by the tuple M = (S, A, P, r), where S is the state space, A is the action space, P is the state transition function, and r is the reward function. The agent observes a state s \u2208 S, chooses an action a \u2208 A according to its policy \u03c0 : S \u2192 A, and then receives a reward and next state s\u2032 \u2208 S from the environment. Most of the existing RL algorithms are fo- cused on choosing \"monotonic\" actions i.e, the action space is restricted to A \u2208 Rm, where m is the dimension of action",
        "authors": [
            "Sai Krishna Gottipati",
            "Yashaswi Pathak",
            "Boris Sattarov",
            "Rohan Nuttall",
            "Matthew E Taylor",
            "based molecule generation",
            "actor critic TAC",
            "not hurt"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/ToweredActorCriticForHandlingMultipleActionTypesInReinforcementLearningForDrugDiscovery.pdf"
    },
    {
        "internal_id": 470,
        "title": "Gene Regulatory Network Inference using 3D Convolutional Neural Network",
        "abstract": "Gene regulatory networks (GRNs) consist of gene regulations between transcription factors (TFs) and their target genes. Single-cell RNA sequencing (scRNA-seq) brings both oppor- tunities and challenges to the inference of GRNs. On the one hand, scRNA-seq data reveals statistic information of gene expressions at the single-cell resolution, which is conducive to the construction of GRNs; on the other hand, noises and dropouts pose great difficulties on the analysis of scRNA-seq data, causing low prediction accuracy by traditional methods. In this paper, we propose 3D Co-Expression Matrix Analysis (3DCEMA), which predicts regulatory relationships by clas- sifying 3D co-expression matrices of gene triplets using a 3D convolutional neural network. We found that by introducing a third gene as a comparison factor, our method can avoid the disturbance of noises and dropouts, and significantly increase the prediction accuracy of regulations between gene pairs. Compared with other existing GRN inference algorithms on both in-silico datasets and scRNA-Seq datasets, our algorithm based on deep learning shows higher stability and accuracy in the task of GRN inference.",
        "authors": [
            "Yue Fan",
            "Xiuli Ma"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/GeneRegulatoryNetworkInferenceusing3DConvolutionalNeuralNetwork.pdf"
    },
    {
        "internal_id": 471,
        "title": "Modeling the Compatibility of Stem Tracks to Generate Music Mashups",
        "abstract": "A music mashup combines audio elements from two or more songs to create a new work. To reduce the time and effort re- quired to make them, researchers have developed algorithms that predict the compatibility of audio elements. Prior work has focused on mixing unaltered excerpts, but advances in source separation enable the creation of mashups from iso- lated stems (e.g., vocals, drums, bass, etc.). In this work, we take advantage of separated stems not just for creating mashups, but for training a model that predicts the mutual compatibility of groups of excerpts, using self-supervised and semi-supervised methods. Specifically, we first produce a ran- dom mashup creation pipeline that combines stem tracks ob- tained via source separation, with key and tempo automati- cally adjusted to match, since these are prerequisites for high- quality mashups. To train a model to predict compatibility, we use stem tracks obtained from the same song as posi- tive examples, and random combinations of stems with key and/or tempo unadjusted as negative examples. To improve the model and use more data, we also train on \"average\" ex- amples: random combinations with matching key and tempo, where we treat them as unlabeled data as their true compati- bility is unknown. To determine whether the combined signal or the set of stem signals is more indicative of the quality of the result, we experiment on two model architectures and train them using semi-supervised learning technique. Finally, we conduct objective and subjective evaluations of the sys- tem, comparing them to a standard rule-based system.",
        "authors": [
            "Jiawen Huang",
            "Ju Chiang Wang",
            "Xuchen Song",
            " ByteDance"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/ModelingtheCompatibilityofStemTrackstoGenerateMusicMashups.pdf"
    },
    {
        "internal_id": 472,
        "title": "GRASP: Generic Framework for Health Status Representation Learning Based on Incorporating Knowledge from Similar Patients",
        "abstract": "Deep learning models have been applied to many healthcare tasks based on electronic medical records (EMR) data and shown substantial performance. Existing methods commonly embed the records of a single patient into a representation for medical tasks. Such methods learn inadequate represen- tations and lead to inferior performance, especially when the patient's data is sparse or low-quality. Aiming at the above problem, we propose GRASP, a generic framework for health- care models. For a given patient, GRASP first finds patients in the dataset who have similar conditions and similar results (i.e., the similar patients), and then enhances the representa- tion learning and prognosis of the given patient by leverag- ing knowledge extracted from these similar patients. GRASP defines similarities with different meanings between patients for different clinical tasks, and finds similar patients with use- ful information accordingly, and then learns cohort represen- tation to extract valuable knowledge contained in the simi- lar patients. The cohort information is fused with the cur- rent patient's representation to conduct final clinical tasks. Experimental evaluations on two real-world datasets show that GRASP can be seamlessly integrated into state-of-the-art models with consistent performance improvements. Besides, under the guidance of medical experts, we verified the find- ings extracted by GRASP, and the findings are consistent with the existing medical knowledge, indicating that GRASP can generate useful insights for relevant predictions.",
        "authors": [
            "Chaohe Zhang",
            "Xin Gao",
            "Liantao Ma",
            "Yasha Wang",
            "Ministry of Education",
            "Division of Nephrology"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/GRASPGenericFrameworkforHealthStatusRepresentationLearningBasedonIncorporatingKnowledgefromSimilarPatients.pdf"
    },
    {
        "internal_id": 473,
        "title": "Optimal Kidney Exchange with Immunosuppressants",
        "abstract": "Algorithms for exchange of kidneys is one of the key success- ful applications in market design, artificial intelligence, and operations research. Potent immunosuppressant drugs sup- press the body's ability to reject a transplanted organ up to the point that a transplant across blood- or tissue-type incom- patibility becomes possible. In contrast to the standard kidney exchange problem, we consider a setting that also involves the decision about which recipients receive from the limited sup- ply of immunosuppressants that make them compatible with originally incompatible kidneys. We firstly present a general computational framework to model this problem. Our main contribution is a range of efficient algorithms that provide flexibility in terms of meeting meaningful objectives. Moti- vated by the current reality of kidney exchanges using so- phisticated mathematical-programming-based clearing algo- rithms, we then present a general but scalable approach to optimal clearing with immunosuppression; we validate our approach on realistic data from a large fielded exchange.",
        "authors": [
            "Haris Aziz",
            " Agnes Cseh"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/OptimalKidneyExchangewithImmunosuppressants.pdf"
    },
    {
        "internal_id": 474,
        "title": "Who You Would Like to Share With? A Study of Share Recommendation in Social E-commerce",
        "abstract": "The prosperous development of social e-commerce has spawned diverse recommendation demands, and accompa- nied a new recommendation paradigm, share recommenda- tion. Significantly different from traditional binary recom- mendations (e.g., item recommendation and friend recom- mendation), share recommendation models ternary interac- tions among \u27e8User, Item, Friend\u27e9, which aims to recom- mend a most likely friend to a user who would like to share a specific item, progressively becoming an indispensable ser- vice in social e-commerce. Seamlessly integrating the so- cial relations and purchase behaviours, share recommenda- tion improves user stickiness and monetizes the user influ- ence, meanwhile encountering three unique challenges: rich heterogeneous information, complex ternary interaction, and asymmetric share action. In this paper, we first study the share recommendation problem and propose a heterogeneous graph neural network based share recommendation model, called HGSRec. Specifically, HGSRec delicately designs a tripartite heterogeneous GNNs to describe the multifold characteristics of users and items, and then dynamically fuses them via cap- turing potential ternary dependency with a dual co-attention mechanism, followed by a transitive triplet representation to depict the asymmetry of share action and predict whether share action happens. Offline experiments demonstrate the superiority of the proposed HGSRec with significant im- provements (11.7%-14.5%) over the state-of-the-arts, and on- line A/B testing on Taobao platform further demonstrates the high industrial practicability and stability of HGSRec.",
        "authors": [
            "Houye Ji",
            "Junxiong Zhu",
            "Xiao Wang",
            "Chuan Shi",
            "Bai Wang",
            "Xiaoye Tan",
            "Yanghua Li",
            "Shaojian He",
            "Alibaba Group"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/WhoYouWouldLiketoShareWithAStudyofShareRecommendationinSocialEcommerce.pdf"
    },
    {
        "internal_id": 475,
        "title": "Relational Classification of Biological Cells in Microscopy Images",
        "abstract": "We investigate the relational classification of biological cells in 2D microscopy images. Rather than treating each cell im- age independently, we investigate whether and how the neigh- borhood information of a cell can be informative for its pre- diction. We propose a Relational Long Short-Term Memory (R-LSTM) algorithm, coupled with auto-encoders and convo- lutional neural networks, that can learn from both annotated and unlabeled microscopy images and that can utilize both the local and neighborhood information to perform an improved classification of biological cells. Experimental results on both synthetic and real datasets show that R-LSTM performs com- parable to or better than six baselines.",
        "authors": [],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/RelationalClassificationofBiologicalCellsinMicroscopyImages.pdf"
    },
    {
        "internal_id": 476,
        "title": "Hierarchical Graph Convolution Networks for Traffic Forecasting",
        "abstract": "Traffic forecasting is attracting considerable interest due to it- s widespread application in intelligent transportation system- s. Given the complex and dynamic traffic data, many meth- ods focus on how to establish a spatial-temporal model to express the non-stationary traffic patterns. Recently, the lat- est Graph Convolution Network (GCN) has been introduced to learn spatial features while the time neural networks are used to learn temporal features. These GCN based method- s obtain state-of-the-art performance. However, the curren- t GCN based methods ignore the natural hierarchical struc- ture of traffic systems which is composed of the micro layers of road networks and the macro layers of region network- s, in which the nodes are obtained through pooling method and could include some hot traffic regions such as down- town and CBD etc., while the current GCN is only applied on the micro graph of road networks. In this paper, we propose a novel Hierarchical Graph Convolution Networks (HGC- N) for traffic forecasting by operating on both the micro and macro traffic graphs. The proposed method is evaluat- ed on two complex city traffic speed datasets. Compared to the latest GCN based methods like Graph WaveNet, the pro- posed HGCN gets higher traffic forecasting precision with lower computational cost.The website of the code is http- s://github.com/guokan987/HGCN.git.",
        "authors": [
            "Kan Guo",
            "Yongli Hu",
            "Yanfeng Sun",
            "Sean Qian",
            "Junbin Gao",
            "Baocai Yin"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/HierarchicalGraphConvolutionNetworkforTrafficForecasting.pdf"
    },
    {
        "internal_id": 477,
        "title": "DeepPseudo: Pseudo Value Based Deep Learning Models for Competing Risk Analysis",
        "abstract": "Competing Risk Analysis (CRA) aims at the correct estima- tion of the marginal probability of occurrence of an event in the presence of competing events. Many of the statistical ap- proaches developed for CRA are limited by strong assump- tions about the underlying stochastic processes. To overcome these issues and to handle censoring, machine learning ap- proaches for CRA have designed specialized cost functions. However, these approaches are not generalizable and are computationally expensive. This paper formulates CRA as a cause-specific regression problem and proposes DeepPseudo models, which use simple and effective feed-forward deep neural networks, to predict the cumulative incidence func- tion (CIF) using Aalen-Johansen estimator-based pseudo val- ues. DeepPseudo models capture the time-varying covariate effect on CIF while handling the censored observations. We show how DeepPseudo models can address covariate depen- dent censoring by using modified pseudo values. Experiments on real and synthetic datasets demonstrate that our proposed models obtain promising and statistically significant results compared to the state-of-the-art CRA approaches. Further- more, we show that explainable methods such as Layer-wise Relevance Propagation can be used to interpret the predic- tions of our DeepPseudo models.",
        "authors": [
            "Md Mahmudur Rahman",
            "Koji Matsuo",
            "Shinya Matsuzaki",
            "Sanjay Purushotham",
            "Baltimore County"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/DeepPseudoPseudoValueBasedDeepLearningModelsforCompetingRiskAnalysis.pdf"
    },
    {
        "internal_id": 479,
        "title": "ECG ODE-GAN: Learning Ordinary Differential Equations of ECG Dynamics via Generative Adversarial Learning",
        "abstract": "Understanding the dynamics of complex biological and phys- iological systems has been explored for many years in the form of physically-based mathematical simulators. The be- havior of a physical system is often described via ordinary differential equations (ODE), referred to as the dynamics. In the standard case, the dynamics are derived from purely phys- ical considerations. By contrast, in this work we study how the dynamics can be learned by a generative adversarial net- work which combines both physical and data considerations. As a use case, we focus on the dynamics of the heart sig- nal electrocardiogram (ECG). We begin by introducing a new GAN framework, dubbed ODE-GAN, in which the generator learns the dynamics of a physical system in the form of an ordinary differential equation. Specifically, the generator net- work receives as input a value at a specific time step, and produces the derivative of the system at that time step. Thus, the ODE-GAN learns purely data-driven dynamics. We then show how to incorporate physical considerations into ODE- GAN. We achieve this through the introduction of an addi- tional input to the ODE-GAN generator: physical parameters, which partially characterize the signal of interest. As we fo- cus on ECG signals, we refer to this new framework as ECG- ODE-GAN. We perform an empirical evaluation and show that generating ECG heartbeats from our learned dynamics improves ECG heartbeat classification.",
        "authors": [
            "Tomer Golany",
            " Daniel Freedman"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/ECGODEGANLearningOrdinaryDifferentialEquationsofECGDynamicsviaGenerativeAdversarialLearning.pdf"
    },
    {
        "internal_id": 481,
        "title": "Deep Conservation: A Latent-Dynamics Model for Exact Satisfaction of Physical Conservation Laws",
        "abstract": "This work proposes an approach for latent-dynamics learning that exactly enforces physical conservation laws. The method comprises two steps. First, the method computes a low- dimensional embedding of the high-dimensional dynamical- system state using deep convolutional autoencoders. This de- fines a low-dimensional nonlinear manifold on which the state is subsequently enforced to evolve. Second, the method defines a latent-dynamics model that associates with the solu- tion to a constrained optimization problem. Here, the objec- tive function is defined as the sum of squares of conservation- law violations over control volumes within a finite-volume discretization of the problem; nonlinear equality constraints explicitly enforce conservation over prescribed subdomains of the problem. Under modest conditions, the resulting dy- namics model guarantees that the time-evolution of the latent state exactly satisfies conservation laws over the prescribed subdomains.",
        "authors": [
            "Kookjin Lee",
            "Kevin T Carlberg"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/DeepConservationALatentDynamicsModelforExactSatisfactionofPhysicalConservationLaws.pdf"
    },
    {
        "internal_id": 482,
        "title": "Programmatic Strategies for Real-Time Strategy Games",
        "abstract": "Search-based systems have shown to be effective for plan- ning in zero-sum games. However, search-based approaches have important disadvantages. First, the decisions of search algorithms are mostly non-interpretable, which is problem- atic in domains where predictability and trust are desired such as commercial games. Second, the computational complex- ity of search-based algorithms might limit their applicabil- ity, especially in contexts where resources are shared among other tasks such as graphic rendering. In this work we intro- duce a system for synthesizing programmatic strategies for a real-time strategy (RTS) game. In contrast with search al- gorithms, programmatic strategies are more amenable to ex- planations and tend to be efficient, once the program is syn- thesized. Our system uses a novel algorithm for simplifying domain-specific languages (DSLs) and a local search algo- rithm that synthesizes programs with self play. We performed a user study where we enlisted four professional program- mers to develop programmatic strategies for \u00b5RTS, a mini- malist RTS game. Our results show that the programs synthe- sized by our approach can outperform search algorithms and be competitive with programs written by the programmers.",
        "authors": [
            " Claudio Toledo"
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/ProgrammaticStrategiesforRealTimeStrategyGames.pdf"
    },
    {
        "internal_id": 483,
        "title": "Integrating Static and Dynamic Data for Improved Prediction of Cognitive Declines Using Augmented Genotype-Phenotype Representations",
        "abstract": "Alzheimer's Disease (AD) is a chronic neurodegenerative disease that causes severe problems in patients' thinking, memory, and behavior. An early diagnosis is crucial to pre- vent AD progression; to this end, many algorithmic ap- proaches have recently been proposed to predict cognitive decline. However, these predictive models often fail to in- tegrate heterogeneous genetic and neuroimaging biomarkers and struggle to handle missing data. In this work we propose a novel objective function and an associated optimization al- gorithm to identify cognitive decline related to AD. Our ap- proach is designed to incorporate dynamic neuroimaging data by way of a participant-specific augmentation combined with multimodal data integration aligned via a regression task. Our approach, in order to incorporate additional side-information, utilizes structured regularization techniques popularized in recent AD literature. Armed with the fixed-length vector rep- resentation learned from the multimodal dynamic and static modalities, conventional machine learning methods can be used to predict the clinical outcomes associated with AD. Our experimental results show that the proposed augmenta- tion model improves the prediction performance on cognitive assessment scores for a collection of popular machine learn- ing algorithms. The results of our approach are interpreted to validate existing genetic and neuroimaging biomarkers that have been shown to be predictive of cognitive decline.",
        "authors": [
            "Hoon Seo",
            "Hua Wang",
            "Feiping Nie",
            "CO ",
            "U S A",
            "Xi an "
        ],
        "created_time": "16 May 2021",
        "conference": "AAAI",
        "filepath": "/Users/adit/papers/AAAI/IntegratingStaticandDynamicDataforImprovedPredictionofCognitiveDeclinesUsingAugmentedGenotypePhenotypeRepresentations.pdf"
    },
    {
        "internal_id": 484,
        "title": "Airbert: In-domain Pretraining for Vision-and-Language Navigation",
        "abstract": "Vision-and-language navigation (VLN) aims to enable embodied agents to navigate in realistic environments us- ing natural language instructions. Given the scarcity of domain-specific training data and the high diversity of im- age and language inputs, the generalization of VLN agents to unseen environments remains challenging. Recent meth- ods explore pretraining to improve generalization, however, the use of generic image-caption datasets or existing small- scale VLN environments is suboptimal and results in limited improvements. In this work, we introduce BnB1, a large- scale and diverse in-domain VLN dataset. We first col- lect image-caption (IC) pairs from hundreds of thousands of listings from online rental marketplaces. Using IC pairs we next propose automatic strategies to generate millions of VLN path-instruction (PI) pairs. We further propose a shuffling loss that improves the learning of temporal or- der inside PI pairs. We use BnB to pretrain our Airbert2",
        "authors": [
            "Pierre Louis Guhur",
            "Shizhe Chen",
            "Ivan Laptev",
            "Cordelia Schmid",
            "bpc ",
            "bpc "
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/AirbertInDomainPretrainingforVisionandLanguageNavigation.pdf"
    },
    {
        "internal_id": 485,
        "title": "Interacting Two-Hand 3D Pose and Shape Reconstruction from Single Color Image",
        "abstract": "In this paper, we propose a novel deep learning frame- work to reconstruct 3D hand poses and shapes of two interacting hands from a single color image. Previous methods designed for single hand cannot be easily ap- plied for the two hand scenario because of the heavy inter-hand occlusion and larger solution space. In or- der to address the occlusion and similar appearance be- tween hands that may confuse the network, we design a hand pose-aware attention module to extract features asso- ciated to each individual hand respectively. We then lever- age the two hand context presented in interaction to pro- pose a context-aware cascaded refinement that improves the hand pose and shape accuracy of each hand condi- tioned on the context between interacting hands. Exten- sive experiments on the main benchmark datasets demon- strate that our method predicts accurate 3D hand pose and shape from single color image, and achieves the state-of- the-art performance. Code is available in project webpage https://baowenz.github.io/Intershape/.",
        "authors": [
            "Baowen Zhang",
            "Yangang Wang",
            "Xiaoming Deng",
            "Yinda Zhang",
            "Ping Tan",
            "Cuixia Ma",
            "Hongan Wang"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/InteractingTwoHand3DPoseandShapeReconstructionFromSingleColorImage.pdf"
    },
    {
        "internal_id": 486,
        "title": "Compressing Visual-linguistic Model via Knowledge Distillation",
        "abstract": "Despite exciting progress in pre-training for visual- linguistic (VL) representations, very few aspire to a small VL model. In this paper, we study knowledge distillation (KD) to effectively compress a transformer based large VL model into a small VL model. The major challenge arises from the inconsistent regional visual tokens extracted from different detectors of Teacher and Student, resulting in the misalignment of hidden representations and attention dis- tributions. To address the problem, we retrain and adapt the Teacher by using the same region proposals from Stu- dent's detector while the features are from Teacher's own object detector. With aligned network inputs, the adapted Teacher is capable of transferring the knowledge through the intermediate representations. Specifically, we use the mean square error loss to mimic the attention distribution inside the transformer block, and present a token-wise noise contrastive loss to align the hidden state by contrasting with negative representations stored in a sample queue. To this end, we show that our proposed distillation significantly im- proves the performance of small VL models on image cap- tioning and visual question answering tasks. It reaches 120.8 in CIDEr score on COCO captioning, an improve- ment of 5.1 over its non-distilled counterpart; and an accu- racy of 69.8 on VQA 2.0, a 0.8 gain from the baseline. Our extensive experiments and ablations confirm the effective- ness of VL distillation in both pre-training and fine-tuning stages.",
        "authors": [
            "Zhiyuan Fang",
            "Jianfeng Wang",
            "Xiaowei Hu",
            "Lijuan Wang",
            "Yezhou Yang",
            "Zicheng Liu",
            "Microsoft Corporation"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/CompressingVisualLinguisticModelviaKnowledgeDistillation.pdf"
    },
    {
        "internal_id": 487,
        "title": "Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning",
        "abstract": "Encouraged by the success of contrastive learning on im- age classification tasks, we propose a new self-supervised method for the structured regression task of 3D hand pose estimation. Contrastive learning makes use of unlabeled data for the purpose of representation learning via a loss formulation that encourages the learned feature represen- tations to be invariant under any image transformation. For 3D hand pose estimation, it too is desirable to have in- variance to appearance transformation such as color jit- ter. However, the task requires equivariance under affine transformations, such as rotation and translation. To ad- dress this issue, we propose an equivariant contrastive ob- jective and demonstrate its effectiveness in the context of 3D hand pose estimation. We experimentally investigate the impact of invariant and equivariant contrastive objec- tives and show that learning equivariant features leads to better representations for the task of 3D hand pose esti- mation. Furthermore, we show that standard ResNets with sufficient depth, trained on additional unlabeled data, at- tain improvements of up to 14.5% in PA-EPE on FreiHAND and thus achieves state-of-the-art performance without any task specific, specialized architectures. Code and models are available at https://ait.ethz.ch/projects/2021/PeCLR/",
        "authors": [
            "Adrian Spurr",
            "Aneesh Dahiya",
            "Xi Wang",
            "Xucong Zhang",
            "Otmar Hilliges"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/SelfSupervised3DHandPoseEstimationFromMonocularRGBviaContrastiveLearning.pdf"
    },
    {
        "internal_id": 488,
        "title": "Differentiable Convolution Search for Point Cloud Processing",
        "abstract": "Exploiting convolutional neural networks for point cloud processing is quite challenging, due to the inherent irregu- lar distribution and discrete shape representation of point clouds. To address these problems, many handcrafted con- volution variants have sprung up in recent years. Though with elaborate design, these variants could be far from op- timal in sufficiently capturing diverse shapes formed by dis- crete points. In this paper, we propose PointSeaConv, i.e., a novel differential convolution search paradigm on point clouds. It can work in a purely data-driven manner and thus is capable of auto-creating a group of suitable con- volutions for geometric shape modeling. We also propose a joint optimization framework for simultaneous search of internal convolution and external architecture, and intro- duce epsilon-greedy algorithm to alleviate the effect of dis- cretization error. As a result, PointSeaNet, a deep network that is sufficient to capture geometric shapes at both con- volution level and architecture level, can be searched out for point cloud processing. Extensive experiments strong- ly evidence that our proposed PointSeaNet surpasses cur- rent handcrafted deep models on challenging benchmarks across multiple tasks with remarkable margins.",
        "authors": [
            "Xing Nie",
            "Yongcheng Liu",
            "Shaohong Chen",
            "Jianlong Chang",
            "Chunlei Huo",
            "Gaofeng Meng",
            "Qi Tian",
            "Weiming Hu",
            "Chunhong Pan"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/DifferentiableConvolutionSearchforPointCloudProcessing.pdf"
    },
    {
        "internal_id": 489,
        "title": "Switchable K-class Hyperplanes for Noise-Robust Representation Learning",
        "abstract": "Optimizing the K-class hyperplanes in the latent space has become the standard paradigm for efficient represen- tation learning. However, it's almost impossible to find an optimal K-class hyperplane to accurately describe the la- tent space of massive noisy data. For this potential problem, we constructively propose a new method, named Switch- able K-class Hyperplanes (SKH), to sufficiently describe the latent space by the mixture of K-class hyperplanes. It can directly replace the conventional single K-class hyper- plane optimization as the new paradigm for noise-robust representation learning. When collaborated with the pop- ular ArcFace on million-level data representation learn- ing, we found that the switchable manner in SKH can ef- fectively eliminate the gradient conflict generated by real- world label noise on a single K-class hyperplane. More- over, combined with the margin-based loss functions (e.g. ArcFace), we propose a simple Posterior Data Clean strat- egy to reduce the model optimization deviation on clean dataset caused by the reduction of valid categories in each K-class hyperplane. Extensive experiments demonstrate that the proposed SKH easily achieves new state-of-the-art on IJB-B and IJB-C by encouraging noise-robust represen- tation learning. Our code will be available at https: //github.com/liubx07/SKH.git.",
        "authors": [
            "Boxiao Liu",
            "Guanglu Song",
            "Manyuan Zhang",
            "Haihang You"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/SwitchableKClassHyperplanesforNoiseRobustRepresentationLearning.pdf"
    },
    {
        "internal_id": 490,
        "title": "Continual Neural Mapping: Learning An Implicit Scene Representation from Sequential Observations",
        "abstract": "Recent advances have enabled a single neural network to serve as an implicit scene representation, establishing the mapping function between spatial coordinates and scene properties. In this paper, we make a further step towards continual learning of the implicit scene representation di- rectly from sequential observations, namely Continual Neu- ral Mapping. The proposed problem setting bridges the gap between batch-trained implicit neural representations and commonly used streaming data in robotics and vision com- munities. We introduce an experience replay approach to tackle an exemplary task of continual neural mapping: ap- proximating a continuous signed distance function (SDF) from sequential depth images as a scene geometry repre- sentation. We show for the first time that a single network can represent scene geometry over time continually without catastrophic forgetting, while achieving promising trade- offs between accuracy and efficiency.",
        "authors": [
            "Zike Yan",
            "Yuxin Tian",
            "Xuesong Shi",
            "Ping Guo",
            "Peng Wang",
            "Hongbin Zha",
            "bpc ",
            "bpc ",
            "bpc ",
            "bpc "
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/ContinualNeuralMappingLearninganImplicitSceneRepresentationFromSequentialObservations.pdf"
    },
    {
        "internal_id": 491,
        "title": "Lifelong Infinite Mixture Model Based on Knowledge-Driven Dirichlet Process",
        "abstract": "Recent research efforts in lifelong learning propose to grow a mixture of models to adapt to an increasing num- ber of tasks. The proposed methodology shows promising results in overcoming catastrophic forgetting. However, the theory behind these successful models is still not well un- derstood. In this paper, we perform the theoretical analy- sis for lifelong learning models by deriving the risk bounds based on the discrepancy distance between the probabilis- tic representation of data generated by the model and that corresponding to the target dataset. Inspired by the the- oretical analysis, we introduce a new lifelong learning ap- proach, namely the Lifelong Infinite Mixture (LIMix) model, which can automatically expand its network architectures or choose an appropriate component to adapt its param- eters for learning a new task, while preserving its previ- ously learnt information. We propose to incorporate the knowledge by means of Dirichlet processes by using a gat- ing mechanism which computes the dependence between the knowledge learnt previously and stored in each com- ponent, and a new set of data. Besides, we train a compact Student model which can accumulate cross-domain repre- sentations over time and make quick inferences. The code is available at https://github.com/dtuzi123/ Lifelong-infinite-mixture-model.",
        "authors": [
            "York YO GH"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/LifelongInfiniteMixtureModelBasedonKnowledgeDrivenDirichletProcess.pdf"
    },
    {
        "internal_id": 492,
        "title": "EM-POSE: 3D Human Pose Estimation from Sparse Electromagnetic Trackers",
        "abstract": "Fully immersive experiences in AR/VR depend on re- constructing the full body pose of the user without re- stricting their motion. In this paper we study the use of body-worn electromagnetic (EM) field-based sensing for the task of 3D human pose reconstruction. To this end, we present a method to estimate SMPL parameters from 6-12 EM sensors. We leverage a customized wearable system consisting of wireless EM sensors measuring time- synchronized 6D poses at 120 Hz. To provide accurate poses even with little user instrumentation, we adopt a re- cently proposed hybrid framework, learned gradient de- scent (LGD), to iteratively estimate SMPL pose and shape from our input measurements. This allows us to harness powerful pose priors to cope with the idiosyncrasies of the input data and achieve accurate pose estimates. The pro- posed method uses AMASS to synthesize virtual EM-sensor data and we show that it generalizes well to a newly cap- tured real dataset consisting of a total of 36 minutes of motion from 5 subjects. We achieve reconstruction errors as low as 31.8 mm and 13.3 degrees, outperforming both pure learning- and pure optimization-based methods. Code and data is available under https://ait.ethz.ch/ projects/2021/em-pose.",
        "authors": [
            "Manuel Kaufmann",
            "Yi Zhao",
            "Chengcheng Tang",
            "Lingling Tao",
            "Christopher Twigg",
            "Jie Song",
            "Robert Wang",
            "Otmar Hilliges",
            "ETH Z urich",
            "bpc "
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/EMPOSE3DHumanPoseEstimationFromSparseElectromagneticTrackers.pdf"
    },
    {
        "internal_id": 493,
        "title": "Pose Correction for Highly Accurate Visual Localization in Large-scale Indoor Spaces",
        "abstract": "Indoor visual localization is significant for various ap- plications such as autonomous robots, augmented reality, and mixed reality. Recent advances in visual localization have demonstrated their feasibility in large-scale indoor spaces through coarse-to-fine methods that typically em- ploy three steps: image retrieval, pose estimation, and pose selection. However, further research is needed to improve the accuracy of large-scale indoor visual localization. We demonstrate that the limitations in the previous methods can be attributed to the sparsity of image positions in the database, which causes view-differences between a query and a retrieved image from the database. In this paper, to address this problem, we propose a novel module, named pose correction, that enables re-estimation of the pose with local feature matching in a similar view by reorganizing the local features. This module enhances the accuracy of the initially estimated pose and assigns more reliable ranks. Furthermore, the proposed method achieves a new state- of-the-art performance with an accuracy of more than 90 % within 1.0 m in the challenging indoor benchmark dataset InLoc for the first time. 1",
        "authors": [
            "Janghun Hyeon",
            "Joohyung Kim",
            "Nakju Doh"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/PoseCorrectionforHighlyAccurateVisualLocalizationinLargeScaleIndoorSpaces.pdf"
    },
    {
        "internal_id": 494,
        "title": "Neural Radiance Flow for 4D View Synthesis and Video Processing",
        "abstract": "We present a method, Neural Radiance Flow (NeRFlow), to learn a 4D spatial-temporal representation of a dynamic scene from a set of RGB images. Key to our approach is the use of a neural implicit representation that learns to capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing consistency across different modalities, our representation enables multi-view rendering in diverse dynamic scenes, including water pouring, robotic interaction, and real images, outperforming state-of-the-art methods for spatial-temporal view synthesis. Our approach works even when being provided only a single monocular real video. We further demonstrate that the learned representation can serve as an implicit scene prior, enabling video processing tasks such as image super-resolution and de-noising without any additional supervision.",
        "authors": [
            "Yilun Du",
            "MIT CSAIL",
            "Yinan Zhang",
            "Hong Xing Yu",
            "Joshua B Tenenbaum",
            "MIT CSAIL",
            "Jiajun Wu"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/NeuralRadianceFlowfor4DViewSynthesisandVideoProcessing.pdf"
    },
    {
        "internal_id": 495,
        "title": "From Culture to Clothing: Discovering the World Events Behind A Century of Fashion Images",
        "abstract": "Fashion is intertwined with external cultural factors, but identifying these links remains a manual process limited to only the most salient phenomena. We propose a data-driven approach to identify specific cultural factors affecting the clothes people wear. Using large-scale datasets of news ar- ticles and vintage photos spanning a century, we present a multi-modal statistical model to detect influence relation- ships between happenings in the world and people's choice of clothing. Furthermore, on two image datasets we ap- ply our model to improve the concrete vision tasks of visual style forecasting and photo timestamping. Our work is a first step towards a computational, scalable, and easily re- freshable approach to link culture to clothing.",
        "authors": [
            "Wei Lin Hsiao",
            "UT Austin",
            "Kristen Grauman",
            "UT Austin",
            "Display P",
            "bpc ",
            "during the s"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/FromCulturetoClothingDiscoveringtheWorldEventsBehindaCenturyofFashionImages.pdf"
    },
    {
        "internal_id": 496,
        "title": "PR-GCN: A Deep Graph Convolutional Network with Point Refinement for 6D Pose Estimation",
        "abstract": "RGB-D based 6D pose estimation has recently achieved remarkable progress, but still suffers from two major limita- tions: (1) ineffective representation of depth data and (2) in- sufficient integration of different modalities. This paper pro- poses a novel deep learning approach, namely Graph Con- volutional Network with Point Refinement (PR-GCN), to si- multaneously address the issues above in a unified way. It first introduces the Point Refinement Network (PRN) to pol- ish 3D point clouds, recovering missing parts with noise re- moved. Subsequently, the Multi-Modal Fusion Graph Con- volutional Network (MMF-GCN) is presented to strengthen RGB-D combination, which captures geometry-aware inter- modality correlation through local information propagation in the graph convolutional network. Extensive experiments are conducted on three widely used benchmarks, and state- of-the-art performance is reached. Besides, it is also shown that the proposed PRN and MMF-GCN modules are well generalized to other frameworks.",
        "authors": [
            "Guangyuan Zhou",
            "Huiqun Wang"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/PRGCNADeepGraphConvolutionalNetworkWithPointRefinementfor6DPoseEstimation.pdf"
    },
    {
        "internal_id": 497,
        "title": "Learning Realistic Human Reposing using Cyclic Self-Supervision with 3D Shape, Pose, and Appearance Consistency",
        "abstract": "Synthesizing images of a person in novel poses from a single image is a highly ambiguous task. Most existing ap- proaches require paired training images; i.e. images of the same person with the same clothing in different poses. How- ever, obtaining sufficiently large datasets with paired data is challenging and costly. Previous methods that forego paired supervision lack realism. We propose a self-supervised framework named SPICE (Self-supervised Person Image CrEation) that closes the image quality gap with super- vised methods. The key insight enabling self-supervision is to exploit 3D information about the human body in sev- eral ways. First, the 3D body shape must remain unchanged when reposing. Second, representing body pose in 3D en- ables reasoning about self occlusions. Third, 3D body parts that are visible before and after reposing, should have simi- lar appearance features. Once trained, SPICE takes an im- age of a person and generates a new image of that person",
        "authors": [
            "Soubhik Sanyal",
            "Alex Vorobiov",
            "Timo Bolkart",
            "Matthew Loper",
            "Betty Mohler",
            "Larry Davis",
            "Javier Romero",
            "Michael J Black",
            "T ubingen",
            "bpc "
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/LearningRealisticHumanReposingUsingCyclicSelfSupervisionWith3DShapePoseandAppearanceConsistency.pdf"
    },
    {
        "internal_id": 498,
        "title": "Learning Multiple Pixelwise Tasks Based on Loss Scale Balancing",
        "abstract": "We propose a novel loss weighting algorithm, called loss scale balancing (LSB), for multi-task learning (MTL) of pix- elwise vision tasks. An MTL model is trained to estimate multiple pixelwise predictions using an overall loss, which is a linear combination of individual task losses. The pro- posed algorithm dynamically adjusts the linear weights to learn all tasks effectively. Instead of controlling the trend of each loss value directly, we balance the loss scale \u2014 the product of the loss value and its weight \u2014 periodically. In addition, by evaluating the difficulty of each task based on the previous loss record, the proposed algorithm focuses more on difficult tasks during training. Experimental re- sults show that the proposed algorithm outperforms conven- tional weighting algorithms for MTL of various pixelwise tasks. Codes are available at https://github.com/jaehanlee- mcl/LSB-MTL.",
        "authors": [
            "Jae Han Lee",
            "Chul Lee",
            "Chang Su Kim"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/LearningMultiplePixelwiseTasksBasedonLossScaleBalancing.pdf"
    },
    {
        "internal_id": 499,
        "title": "ShapeConv: Shape-aware Convolutional Layer for Indoor RGB-D Semantic Segmentation",
        "abstract": "RGB-D semantic segmentation has attracted increasing attention over the past few years. Existing methods mostly employ homogeneous convolution operators to consume the RGB and depth features, ignoring their intrinsic differences. In fact, the RGB values capture the photometric appearance properties in the projected image space, while the depth fea- ture encodes both the shape of a local geometry as well as the base (whereabout) of it in a larger context. Compared with the base, the shape probably is more inherent and has a stronger connection to the semantics, and thus is more critical for segmentation accuracy. Inspired by this obser- vation, we introduce a Shape-aware Convolutional layer (ShapeConv) for processing the depth feature, where the depth feature is firstly decomposed into a shape-component and a base-component, next two learnable weights are in- troduced to cooperate with them independently, and finally a convolution is applied on the re-weighted combination of these two components. ShapeConv is model-agnostic and can be easily integrated into most CNNs to replace vanilla convolutional layers for semantic segmentation. Ex- tensive experiments on three challenging indoor RGB-D se- mantic segmentation benchmarks, i.e., NYU-Dv2(-13,-40), SUN RGB-D, and SID, demonstrate the effectiveness of our ShapeConv when employing it over five popular architec- tures. Moreover, the performance of CNNs with ShapeConv is boosted without introducing any computation and mem- ory increase in the inference phase. The reason is that the learnt weights for balancing the importance between the shape and base components in ShapeConv become con- stants in the inference phase, and thus can be fused into the following convolution, resulting in a network that is identi- cal to one with vanilla convolutional layers.",
        "authors": [
            "Jinming Cao",
            "Hanchao Leng",
            "Dani Lischinski",
            "Danny Cohen Or",
            "Alibaba Group"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/ShapeConvShapeAwareConvolutionalLayerforIndoorRGBDSemanticSegmentation.pdf"
    },
    {
        "internal_id": 500,
        "title": "Cross-Camera Convolutional Color Constancy",
        "abstract": "We present \"Cross-Camera Convolutional Color Con- stancy\" (C5), a learning-based method, trained on images from multiple cameras, that accurately estimates a scene's illuminant color from raw images captured by a new camera previously unseen during training. C5 is a hypernetwork- like extension of the convolutional color constancy (CCC) approach: C5 learns to generate the weights of a CCC model that is then evaluated on the input image, with the CCC weights dynamically adapted to different input con- tent. Unlike prior cross-camera color constancy models, which are usually designed to be agnostic to the spectral properties of test-set images from unobserved cameras, C5 approaches this problem through the lens of transductive in- ference: additional unlabeled images are provided as input to the model at test time, which allows the model to cali- brate itself to the spectral properties of the test-set camera during inference. C5 achieves state-of-the-art accuracy for cross-camera color constancy on several datasets, is fast to evaluate (\u223c7 and \u223c90 ms per image on a GPU or CPU, respectively), and requires little memory (\u223c2 MB), and thus is a practical solution to the problem of calibration-free au- tomatic white balance for mobile photography.",
        "authors": [
            "Mahmoud Afifi",
            "Jonathan T Barron",
            "Chloe LeGendre",
            "Yun Ta Tsai",
            "Francois Bleibel"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/CrossCameraConvolutionalColorConstancy.pdf"
    },
    {
        "internal_id": 501,
        "title": "SIMstack: A Generative Shape and Instance Model for Unordered Object Stacks",
        "abstract": "By estimating 3D shape and instances from a single view, we can capture information about an environment quickly, without the need for comprehensive scanning and multi- view fusion. Solving this task for composite scenes (such as object stacks) is challenging: occluded areas are not only ambiguous in shape but also in instance segmenta- tion; multiple decompositions could be valid. We observe that physics constrains decomposition as well as shape in occluded regions and hypothesise that a latent space learned from scenes built under physics simulation can serve as a prior to better predict shape and instances in oc- cluded regions. To this end we propose SIMstack, a depth- conditioned Variational Auto-Encoder (VAE), trained on a dataset of objects stacked under physics simulation. We for- mulate instance segmentation as a centre voting task which allows for class-agnostic detection and doesn't require set- ting the maximum number of objects in the scene. At test time, our model can generate 3D shape and instance seg- mentation from a single depth view, probabilistically sam- pling proposals for the occluded region from the learned latent space. Our method has practical applications in providing robots some of the ability humans have to make rapid intuitive inferences of partially observed scenes. We demonstrate an application for precise (non-disruptive) ob- ject grasping of unknown objects from a single depth view.",
        "authors": [
            "Raluca Scona",
            "Tristan Laidlow",
            "Stephen James"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/SIMstackAGenerativeShapeandInstanceModelforUnorderedObjectStacks.pdf"
    },
    {
        "internal_id": 502,
        "title": "Manifold Matching via Deep Metric Learning for Generative Modeling",
        "abstract": "We propose a manifold matching approach to genera- tive models which includes a distribution generator (or data generator) and a metric generator. In our framework, we view the real data set as some manifold embedded in a high- dimensional Euclidean space. The distribution generator aims at generating samples that follow some distribution condensed around the real data manifold. It is achieved by matching two sets of points using their geometric shape descriptors, such as centroid and p-diameter, with learned distance metric; the metric generator utilizes both real data and generated samples to learn a distance metric which is close to some intrinsic geodesic distance on the real data manifold. The produced distance metric is further used for manifold matching. The two networks learn simultaneously during the training process. We apply the approach on both unsupervised and supervised learning tasks: in uncondi- tional image generation task, the proposed method obtains competitive results compared with existing generative mod- els; in super-resolution task, we incorporate the framework in perception-based models and improve visual qualities by producing samples with more natural textures. Experiments and analysis demonstrate the feasibility and effectiveness of the proposed framework.",
        "authors": [
            "Mengyu Dai",
            "Haibin Hang"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/ManifoldMatchingviaDeepMetricLearningforGenerativeModeling.pdf"
    },
    {
        "internal_id": 503,
        "title": "Unsupervised Image Generation with Infinite Generative Adversarial Networks",
        "abstract": "Image generation has been heavily investigated in com- puter vision, where one core research challenge is to gen- erate images from arbitrarily complex distributions with lit- tle supervision. Generative Adversarial Networks (GANs) as an implicit approach have achieved great successes in this direction and therefore been employed widely. How- ever, GANs are known to suffer from issues such as mode collapse, non-structured latent space, being unable to com- pute likelihoods, etc. In this paper, we propose a new unsu- pervised non-parametric method named mixture of infinite conditional GANs or MIC-GANs, to tackle several GAN is- sues together, aiming for image generation with parsimo- nious prior knowledge. Through comprehensive evalua- tions across different datasets, we show that MIC-GANs are effective in structuring the latent space and avoiding mode collapse, and outperform state-of-the-art methods. MIC- GANs are adaptive, versatile, and robust. They offer a promising solution to several well-known GAN issues. Code available: github.com/yinghdb/MICGANs.",
        "authors": [
            "Hui Ying",
            "He Wang",
            "Tianjia Shao",
            "Yin Yang",
            "Kun Zhou"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/UnsupervisedImageGenerationWithInfiniteGenerativeAdversarialNetworks.pdf"
    },
    {
        "internal_id": 504,
        "title": "LaLaLoc: Latent Layout Localisation in Dynamic, Unvisited Environments",
        "abstract": "We present LaLaLoc to localise in environments with- out the need for prior visitation, and in a manner that is robust to large changes in scene appearance, such as a full rearrangement of furniture. Specifically, LaLaLoc per- forms localisation through latent representations of room layout. LaLaLoc learns a rich embedding space shared be- tween RGB panoramas and layouts inferred from a known floor plan that encodes the structural similarity between lo- cations. Further, LaLaLoc introduces direct, cross-modal pose optimisation in its latent space. Thus, LaLaLoc en- ables fine-grained pose estimation in a scene without the need for prior visitation, as well as being robust to dy- namics, such as a change in furniture configuration. We show that in a domestic environment LaLaLoc is able to ac- curately localise a single RGB panorama image to within 8.3cm, given only a floor plan as a prior.",
        "authors": [
            "Henry Howard Jenkins",
            "Intelligent Robotics Group",
            "Victor Adrian Prisacariu"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/LaLaLocLatentLayoutLocalisationinDynamicUnvisitedEnvironments.pdf"
    },
    {
        "internal_id": 505,
        "title": "3D Local Convolutional Neural Networks for Gait Recognition",
        "abstract": "The goal of gait recognition is to learn the unique spatio- temporal pattern about the human body shape from its tem- poral changing characteristics. As different body parts be- have differently during walking, it is intuitive to model the spatio-temporal patterns of each part separately. However, existing part-based methods equally divide the feature maps of each frame into fixed horizontal stripes to get local parts. It is obvious that these stripe partition-based methods can- not accurately locate the body parts. First, different body parts can appear at the same stripe (e.g., arms and the torso), and one part can appear at different stripes in dif- ferent frames (e.g., hands). Second, different body parts possess different scales, and even the same part in different frames can appear at different locations and scales. Third, different parts also exhibit distinct movement patterns (e.g., at which frame the movement starts, the position change frequency, how long it lasts). To overcome these issues, we propose novel 3D local operations as a generic fam- ily of building blocks for 3D gait recognition backbones. The proposed 3D local operations support the extraction of local 3D volumes of body parts in a sequence with adap- tive spatial and temporal scales, locations and lengths. In this way, the spatio-temporal patterns of the body parts are well learned from the 3D local neighborhood in part- specific scales, locations, frequencies and lengths. Experi- ments demonstrate that our 3D local convolutional neural networks achieve state-of-the-art performance on popular gait datasets. Code is available at: https://github. com/yellowtownhz/3DLocalCNN.",
        "authors": [
            "Zhen Huang",
            "Dixiu Xue",
            "Xu Shen",
            "Xinmei Tian",
            "Houqiang Li",
            "Jianqiang Huang",
            "Xian Sheng Hua",
            "Alibaba Group"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/3DLocalConvolutionalNeuralNetworksforGaitRecognition.pdf"
    },
    {
        "internal_id": 506,
        "title": "Interaction via Bi-directional Graph of Semantic Region Affinity for Scene Parsing",
        "abstract": "In this work, we devote to address the challenging problem of scene parsing. It is well known that pixels in an image are highly correlated with each other, especially those from the same semantic region, while treating pixels independently fails to take advantage of such correlations. In this work, we treat each respective region in an image as a whole, and capture the structure topology as well as the affinity among different regions. To this end, we first divide the entire feature maps to different regions and extract respective global features from them. Next, we construct a directed graph whose nodes are regional features, and the bi-directional edges connecting every two nodes are the affinities between the regional features they represent. After that, we transfer the affinity-aware nodes in the directed graph back to corresponding regions of the image, which helps to model the region dependencies and mitigate unrealistic results. In addition, to further boost the correlation among pixels, we propose a region-level loss that evaluates all pixels in a region as a whole and motivates the network to learn the exclusive regional feature per class. With the proposed approach, we achieves new state-of-the-art segmentation results on PASCAL-Context, ADE20K, and COCO-Stuff consistently.",
        "authors": [
            "Henghui Ding",
            "Hui Zhang",
            "Jun Liu",
            "Jiaxin Li",
            "Zijian Feng",
            "Xudong Jiang"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/InteractionviaBiDirectionalGraphofSemanticRegionAffinityforSceneParsing.pdf"
    },
    {
        "internal_id": 507,
        "title": "C3-SemiSeg: Contrastive Semi-supervised Segmentation via Cross-set Learning and Dynamic Class-balancing",
        "abstract": "The semi-supervised semantic segmentation methods uti- lize the unlabeled data to increase the feature discrimina- tive ability to alleviate the burden of the annotated data. However, the dominant consistency learning diagram is limited by a) the misalignment between features from la- beled and unlabeled data; b) treating each image and re- gion separately without considering crucial semantic de- pendencies among classes. In this work, we introduce a novel C3-SemiSeg to improve consistency-based semi- supervised learning by exploiting better feature alignment under perturbations and enhancing the capability of dis- criminative feature cross images. Specifically, we first in- troduce a cross-set region-level data augmentation strat- egy to reduce the feature discrepancy between labeled data and unlabeled data. Cross-set pixel-wise contrastive learn- ing is further integrated into the pipeline to facilitate fea- ture representation ability. To stabilize training from the noisy label, we propose a dynamic confidence region se- lection strategy to focus on the high confidence region for loss calculation. We validate the proposed approach on Cityscapes and BDD100K dataset, which significantly out- performs other state-of-the-art semi-supervised semantic segmentation methods.",
        "authors": [
            "Yanning Zhou",
            "Hang Xu",
            "Wei Zhang",
            "Bin Gao",
            "Pheng Ann Heng"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/C3SemiSegContrastiveSemiSupervisedSegmentationviaCrossSetLearningandDynamicClassBalancing.pdf"
    },
    {
        "internal_id": 508,
        "title": "DeepPanoContext: Panoramic 3D Scene Understanding with Holistic Scene Context Graph and Relation-based Optimization",
        "abstract": "Panorama images have a much larger field-of-view thus naturally encode enriched scene context information com- pared to standard perspective images, which however is not well exploited in the previous scene understanding methods. In this paper, we propose a novel method for panoramic 3D scene understanding which recovers the 3D room lay- out and the shape, pose, position, and semantic category for each object from a single full-view panorama image. In order to fully utilize the rich context information, we design a novel graph neural network based context model to pre- dict the relationship among objects and room layout, and a differentiable relationship-based optimization module to optimize object arrangement with well-designed objective functions on-the-fly. Realizing the existing data are either with incomplete ground truth or overly-simplified scene, we present a new synthetic dataset with good diversity in room layout and furniture placement, and realistic image quality for total panoramic 3D scene understanding. Experiments demonstrate that our method outperforms existing methods on panoramic scene understanding in terms of both geome- try accuracy and object arrangement. Code is available at https://chengzhag.github.io/publication/dpc.",
        "authors": [
            " Google"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/DeepPanoContextPanoramic3DSceneUnderstandingWithHolisticSceneContextGraphandRelationBasedOptimization.pdf"
    },
    {
        "internal_id": 509,
        "title": "Prior to Segment: Foreground Cues for Weakly Annotated Classes in Partially Supervised Instance Segmentation",
        "abstract": "Instance segmentation methods require large datasets with expensive and thus limited instance-level mask labels. Partially supervised instance segmentation aims to improve mask prediction with limited mask labels by utilizing the more abundant weak box labels. In this work, we show that a class agnostic mask head, commonly used in partially supervised instance segmentation, has difficulties learning a general concept of foreground for the weakly annotated classes using box supervision only. To resolve this problem, we introduce an object mask prior (OMP) that provides the mask head with the general concept of foreground implicitly learned by the box classification head under the supervision of all classes. This helps the class agnostic mask head to fo- cus on the primary object in a region of interest (RoI) and improves generalization to the weakly annotated classes. We test our approach on the COCO dataset using different splits of strongly and weakly supervised classes. Our ap- proach significantly improves over the Mask R-CNN base- line and obtains competitive performance with the state-of- the-art, while offering a much simpler architecture. 1",
        "authors": [
            "David Biertimpel",
            " Sindi Shkodrani",
            "Anil S Baslamisli",
            "N ora Baka"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/PriortoSegmentForegroundCuesforWeaklyAnnotatedClassesinPartiallySupervisedInstanceSegmentation.pdf"
    },
    {
        "internal_id": 510,
        "title": "Self-born Wiring for Neural Trees",
        "abstract": "Neural trees aim at integrating deep neural networks and decision trees so as to bring the best of the two worlds, in- cluding representation learning from the former and faster inference from the latter. In this paper, we introduce a novel approach, termed as Self-born Wiring (SeBoW), to learn neural trees from a mother deep neural network. In contrast to prior neural-tree approaches that either adopt a pre-defined structure or grow hierarchical layers in a progressive manner, task-adaptive neural trees in SeBoW evolve from a deep neural network through a construction- by-destruction process, enabling a global-level parameter optimization that further yields favorable results. Specifi- cally, given a designated network configuration like VGG, SeBoW disconnects all the layers and derives isolated fil- ter groups, based on which a global-level wiring process is conducted to attach a subset of filter groups, eventually bearing a lightweight neural tree. Extensive experiments demonstrate that, with a lower computational cost, SeBoW outperforms all prior neural trees by a significant margin and even achieves results on par with predominant non-tree networks like ResNets. Moreover, SeBoW proves its scala- bility to large-scale datasets like ImageNet, which has been barely explored by prior tree networks.",
        "authors": [
            "Ying Chen",
            "Feng Mao",
            "Jie Song",
            "Huiqiong Wang",
            "Alibaba Group"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/SelfBornWiringforNeuralTrees.pdf"
    },
    {
        "internal_id": 511,
        "title": "Just One Moment: Structural Vulnerability of Deep Action Recognition against One Frame Attack",
        "abstract": "The video-based action recognition task has been ex- tensively studied in recent years. In this paper, we study the structural vulnerability of deep learning-based action recognition models against the adversarial attack using the one frame attack that adds an inconspicuous perturbation to only a single frame of a given video clip. Our analysis shows that the models are highly vulnerable against the one frame attack due to their structural properties. Experiments demonstrate high fooling rates and inconspicuous charac- teristics of the attack. Furthermore, we show that strong universal one frame perturbations can be obtained under various scenarios. Our work raises the serious issue of ad- versarial vulnerability of the state-of-the-art action recog- nition models in various perspectives.",
        "authors": [
            "Jaehui Hwang",
            "Jun Ho Choi"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/JustOneMomentStructuralVulnerabilityofDeepActionRecognitionAgainstOneFrameAttack.pdf"
    },
    {
        "internal_id": 512,
        "title": "Active Universal Domain Adaptation",
        "abstract": "Most unsupervised domain adaptation methods rely on rich prior knowledge about the source-target label set rela- tionship, and they cannot recognize categories beyond the source classes, which limits their applicability in practi- cal scenarios. This paper proposes a new paradigm for unsupervised domain adaptation, termed as Active Univer- sal Domain Adaptation (AUDA), which removes all label set assumptions and aims for not only recognizing target samples from source classes but also inferring those from target-private classes by using active learning to annotate a small budget of target data. For AUDA, it is challenging to jointly adapt the model to the target domain and select informative target samples for annotations under a large domain gap and significant semantic shift. To address the problems, we propose an Active Universal Adaptation Net- work (AUAN). Specifically, we first introduce Adversarial and Diverse Curriculum Learning (ADCL), which progres- sively aligns source and target domains to classify whether target samples are from source classes. Then, we pro- pose a Clustering Non-transferable Gradient Embedding (CNTGE) strategy, which utilizes the clues of transferabil- ity, diversity, and uncertainty to annotate target informative sample, making it possible to infer labels for target samples of target-private classes. Finally, we propose to jointly train ADCL and CNTGE with target supervision to promote do- main adaptation and target-private class recognition. Ex- tensive experiments demonstrate that the proposed AUDA model equipped with ADCL and CNTGE achieves signifi- cant results on four popular benchmarks.",
        "authors": [
            "Xinhong Ma",
            "Junyu Gao"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/ActiveUniversalDomainAdaptation.pdf"
    },
    {
        "internal_id": 513,
        "title": "FMODetect: Robust Detection of Fast Moving Objects",
        "abstract": "We propose the first learning-based approach for fast moving objects detection. Such objects are highly blurred and move over large distances within one video frame. Fast moving objects are associated with a deblurring and mat- ting problem, also called deblatting. We show that the sep- aration of deblatting into consecutive matting and deblur- ring allows achieving real-time performance, i.e. an order of magnitude speed-up, and thus enabling new classes of application. The proposed method detects fast moving ob- jects as a truncated distance function to the trajectory by learning from synthetic data. For the sharp appearance es- timation and accurate trajectory estimation, we propose a matting and fitting network that estimates the blurred ap- pearance without background, followed by an energy min- imization based deblurring. The state-of-the-art methods are outperformed in terms of recall, precision, trajectory estimation, and sharp appearance reconstruction. Com- pared to other methods, such as deblatting, the inference is of several orders of magnitude faster and allows appli- cations such as real-time fast moving object detection and retrieval in large video collections.",
        "authors": [
            "Denys Rozumnyi",
            "Ji r Matas",
            "Filip Sroubek",
            "Marc Pollefeys",
            "Martin R Oswald",
            "Visual Recognition Group"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/FMODetectRobustDetectionofFastMovingObjects.pdf"
    },
    {
        "internal_id": 514,
        "title": "Exploration and Estimation for Model Compression",
        "abstract": "Deep neural networks achieve great success in many vi- sual recognition tasks. However, the model deployment is usually subject to some computational resources. Model pruning under computational budget has attracted growing attention. In this paper, we focus on the discrimination- aware compression of Convolutional Neural Networks (CNNs). In prior arts, directly searching the optimal sub- network is an integer programming problem, which is non- smooth, non-convex, and NP-hard. Meanwhile, the heuris- tic pruning criterion lacks clear interpretability and doesn't generalize well in applications. To address this problem, we formulate sub-networks as samples from a multivari- ate Bernoulli distribution and resort to the approximation of continuous problem. We propose a new flexible search scheme via alternating exploration and estimation. In the exploration step, we employ stochastic gradient Hamilto- nian Monte Carlo with budget-awareness to generate sub- networks, which allows large search space with efficient computation. In the estimation step, we deduce the sub- network sampler to a near-optimal point, to promote the generation of high-quality sub-networks. Unifying the ex- ploration and estimation, our approach avoids early falling into local minimum via a fast gradient-based search in a larger space. Extensive experiments on CIFAR-10 and Im- ageNet show that our method achieves state-of-the-art per- formances on pruning several popular CNNs.",
        "authors": [
            "Yanfu Zhang",
            "Shangqian Gao",
            "Heng Huang"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/ExplorationandEstimationforModelCompression.pdf"
    },
    {
        "internal_id": 515,
        "title": "TransferI2I: Transfer Learning for Image-to-Image Translation from Small Datasets",
        "abstract": "Image-to-image (I2I) translation has matured in recent years and is able to generate high-quality realistic im- ages. However, despite current success, it still faces im- portant challenges when applied to small domains. Ex- isting methods use transfer learning for I2I translation, but they still require the learning of millions of parame- ters from scratch. This drawback severely limits its ap- plication on small domains. In this paper, we propose a new transfer learning for I2I translation (TransferI2I). We decouple our learning process into the image generation step and the I2I translation step. In the first step we pro- pose two novel techniques: source-target initialization and self-initialization of the adaptor layer. The former fine- tunes the pretrained generative model (e.g., StyleGAN) on source and target data. The latter allows to initialize all non-pretrained network parameters without the need of any data. These techniques provide a better initialization for the I2I translation step. In addition, we introduce an auxil- iary GAN that further facilitates the training of deep I2I sys- tems even from small datasets. In extensive experiments on three datasets, (Animal faces, Birds, and Foods), we show that we outperform existing methods and that mFID im- proves on several datasets with over 25 points. Our code is available at: https://github.com/yaxingwang/ TransferI2I.",
        "authors": [
            "Yaxing Wang",
            "Laura Lopez Fuentes",
            "Bogdan Raducanu"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/TransferI2ITransferLearningforImagetoImageTranslationFromSmallDatasets.pdf"
    },
    {
        "internal_id": 516,
        "title": "Deep Hybrid Self-Prior for Full 3D Mesh Generation",
        "abstract": "We present a deep learning pipeline that leverages net- work self-prior to recover a full 3D model consisting of both a triangular mesh and a texture map from the colored 3D point cloud. Different from previous methods either exploit- ing 2D self-prior for image editing or 3D self-prior for pure surface reconstruction, we propose to exploit a novel hybrid 2D-3D self-prior in deep neural networks to significantly improve the geometry quality and produce a high-resolution texture map, which is typically missing from the output of commodity-level 3D scanners. In particular, we first gen- erate an initial mesh using a 3D convolutional neural net- work with 3D self-prior, and then encode both 3D informa- tion and color information in the 2D UV atlas, which is fur- ther refined by 2D convolutional neural networks with the self-prior. In this way, both 2D and 3D self-priors are uti- lized for the mesh and texture recovery. Experiments show that, without the need of any additional training data, our method recovers the 3D textured mesh model of high qual- ity from sparse input, and outperforms the state-of-the-art methods in terms of both geometry and texture quality.",
        "authors": [
            "Xingkui Wei",
            "Zhengqing Chen",
            "Yanwei Fu",
            "Zhaopeng Cui",
            "Yinda Zhang",
            " Google",
            "bpc ",
            "bpc ",
            "bpc ",
            "bpc "
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/DeepHybridSelfPriorforFull3DMeshGeneration.pdf"
    },
    {
        "internal_id": 518,
        "title": "ELSD: Efficient Line Segment Detector and Descriptor",
        "abstract": "We present the novel Efficient Line Segment Detector and Descriptor (ELSD) to simultaneously detect line segments and extract their descriptors in an image. Unlike the tradi- tional pipelines that conduct detection and description sep- arately, ELSD utilizes a shared feature extractor for both detection and description, to provide the essential line fea- tures to the higher-level tasks like SLAM and image match- ing in real time. First, we design a one-stage compact model, and propose to use the mid-point, angle and length as the minimal representation of line segment, which also guarantees the center-symmetry. The non-centerness sup- pression is proposed to filter out the fragmented line seg- ments caused by lines' intersections. The fine offset predic- tion is designed to refine the mid-point localization. Sec- ond, the line descriptor branch is integrated with the detec- tor branch, and the two branches are jointly trained in an end-to-end manner. In the experiments, the proposed ELSD achieves the state-of-the-art performance on the Wireframe dataset and YorkUrban dataset, in both accuracy and effi- ciency. The line description ability of ELSD also outper- forms the previous works on the line matching task.",
        "authors": [
            "Haotian Zhang",
            "Yicheng Luo",
            "Fangbo Qin",
            "Yijia He",
            "Xiao Liu"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/ELSDEfficientLineSegmentDetectorandDescriptor.pdf"
    },
    {
        "internal_id": 519,
        "title": "Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies",
        "abstract": "This paper addresses the challenge of reconstructing an animatable human model from a multi-view video. Some recent works have proposed to decompose a non-rigidly de- forming scene into a canonical neural radiance field and a set of deformation fields that map observation-space points to the canonical space, thereby enabling them to learn the dynamic scene from images. However, they represent the deformation field as translational vector field or SE(3) field, which makes the optimization highly under-constrained. Moreover, these representations cannot be explicitly con- trolled by input motions. Instead, we introduce neural blend weight fields to produce the deformation fields. Based on the skeleton-driven deformation, blend weight fields are used with 3D human skeletons to generate observation-to- canonical and canonical-to-observation correspondences. Since 3D human skeletons are more observable, they can regularize the learning of deformation fields. Moreover, the learned blend weight fields can be combined with in- put skeletal motions to generate new deformation fields to animate the human model. Experiments show that our ap- proach significantly outperforms recent human synthesis methods. The code and supplementary materials are avail- able at https://zju3dv.github.io/animatable nerf/.",
        "authors": [
            "Sida Peng",
            "Junting Dong",
            "Qianqian Wang",
            "Shangzhan Zhang",
            "Qing Shuai",
            "Xiaowei Zhou",
            "Hujun Bao",
            "bpc ",
            "bpc ",
            "bpc "
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/AnimatableNeuralRadianceFieldsforModelingDynamicHumanBodies.pdf"
    },
    {
        "internal_id": 520,
        "title": "Student Customized Knowledge Distillation: Bridging the Gap Between Student and Teacher",
        "abstract": "Knowledge distillation (KD) transfers the dark knowl- edge from cumbersome networks (teacher) to lightweight (student) networks and expects the student to achieve more promising performance than training without the teacher's knowledge. However, a counter-intuitive argument is that better teachers do not make better students due to the ca- pacity mismatch. To this end, we present a novel adaptive knowledge distillation method to complement traditional approaches. The proposed method, named as Student Cus- tomized Knowledge Distillation (SCKD), examines the ca- pacity mismatch between teacher and student from the per- spective of gradient similarity. We formulate the knowl- edge distillation as a multi-task learning problem so that the teacher transfers knowledge to the student only if the student can benefit from learning such knowledge. We vali- date our methods on multiple datasets with various teacher- student configurations on image classification, object detec- tion, and semantic segmentation.",
        "authors": [
            "Yichen Zhu",
            "Yi Wang",
            "Midea Group"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/StudentCustomizedKnowledgeDistillationBridgingtheGapBetweenStudentandTeacher.pdf"
    },
    {
        "internal_id": 521,
        "title": "Self-Knowledge Distillation with Progressive Refinement of Targets",
        "abstract": "The generalization capability of deep neural networks has been substantially improved by applying a wide spec- trum of regularization methods, e.g., restricting function space, injecting randomness during training, augmenting data, etc. In this work, we propose a simple yet effective regularization method named progressive self-knowledge distillation (PS-KD), which progressively distills a model's own knowledge to soften hard targets (i.e., one-hot vec- tors) during training. Hence, it can be interpreted within a framework of knowledge distillation as a student becomes a teacher itself. Specifically, targets are adjusted adaptively by combining the ground-truth and past predictions from the model itself. We show that PS-KD provides an effect of hard example mining by rescaling gradients according to difficulty in classifying examples. The proposed method is applicable to any supervised learning tasks with hard tar- gets and can be easily combined with existing regularization methods to further enhance the generalization performance. Furthermore, it is confirmed that PS-KD achieves not only better accuracy, but also provides high quality of confidence estimates in terms of calibration as well as ordinal rank- ing. Extensive experimental results on three different tasks, image classification, object detection, and machine trans- lation, demonstrate that our method consistently improves the performance of the state-of-the-art baselines. The code is available at https://github.com/lgcnsai/PS-KD-Pytorch.",
        "authors": [],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/SelfKnowledgeDistillationWithProgressiveRefinementofTargets.pdf"
    },
    {
        "internal_id": 522,
        "title": "GyroFlow: Gyroscope-Guided Unsupervised Optical Flow Learning",
        "abstract": "Existing optical flow methods are erroneous in challeng- ing scenes, such as fog, rain, and night because the ba- sic optical flow assumptions such as brightness and gra- dient constancy are broken. To address this problem, we present an unsupervised learning approach that fuses gy- roscope into optical flow learning. Specifically, we first convert gyroscope readings into motion fields named gyro field. Second, we design a self-guided fusion module to fuse the background motion extracted from the gyro field with the optical flow and guide the network to focus on motion details. To the best of our knowledge, this is the first deep learning-based framework that fuses gyroscope data and image content for optical flow learning. To vali- date our method, we propose a new dataset that covers reg- ular and challenging scenes. Experiments show that our method outperforms the state-of-art methods in both regu- lar and challenging scenes. Code and dataset are available at https://github.com/megvii-research/GyroFlow.",
        "authors": [
            "Haipeng Li",
            "Kunming Luo",
            "Shuaicheng Liu"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/GyroFlowGyroscopeGuidedUnsupervisedOpticalFlowLearning.pdf"
    },
    {
        "internal_id": 523,
        "title": "Re-Aging GAN: Toward Personalized Face Age Transformation",
        "abstract": "Face age transformation aims to synthesize past or fu- ture face images by reflecting the age factor on given faces. Ideally, this task should synthesize natural-looking faces across various age groups while maintaining identity. How- ever, most of the existing work has focused on only one of these or is difficult to train while unnatural artifacts still ap- pear. In this work, we propose Re-Aging GAN (RAGAN), a novel single framework considering all the critical factors in age transformation. Our framework achieves state-of- the-art personalized face age transformation by compelling the input identity to perform the self-guidance of the gener- ation process. Specifically, RAGAN can learn the personal- ized age features by using high-order interactions between given identity and target age. Learned personalized age fea- tures are identity information that is recalibrated according to the target age. Hence, such features encompass identity and target age information that provides important clues on how an input identity should be at a certain age. Exper- imental result shows the lowest FID and KID scores and the highest age recognition accuracy compared to previous methods. The proposed method also demonstrates the vi- sual superiority with fewer artifacts, identity preservation, and natural transformation across various age groups.",
        "authors": [
            "Farkhod Makhmudkhujaev",
            "Sungeun Hong",
            "bpc ",
            "bpc ",
            "bpc ",
            "bpc ",
            "bpc ",
            "bpc ",
            "bpc ",
            "bpc "
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/ReAgingGANTowardPersonalizedFaceAgeTransformation.pdf"
    },
    {
        "internal_id": 524,
        "title": "Unsupervised Point Cloud Object Co-segmentation by Co-contrastive Learning and Mutual Attention Sampling",
        "abstract": "This paper presents a new task, point cloud object co- segmentation, aiming to segment the common 3D objects in a set of point clouds. We formulate this task as an ob- ject point sampling problem, and develop two techniques, the mutual attention module and co-contrastive learning, to enable it. The proposed method employs two point sam- plers based on deep neural networks, the object sampler and the background sampler. The former targets at sam- pling points of common objects while the latter focuses on the rest. The mutual attention module explores point-wise correlation across point clouds. It is embedded in both sam- plers and can identify points with strong cross-cloud cor- relation from the rest. After extracting features for points selected by the two samplers, we optimize the networks by developing the co-contrastive loss, which minimizes feature discrepancy of the estimated object points while maximizing feature separation between the estimated object and back- ground points. Our method works on point clouds of an arbitrary object class. It is end-to-end trainable and does not need point-level annotations. It is evaluated on the ScanObjectNN and S3DIS datasets and achieves promis- ing results. The source code will be available at https: //github.com/jimmy15923/unsup_point_coseg.",
        "authors": [
            "Cheng Kun Yang",
            "Yung Yu Chuang",
            "Yen Yu Lin",
            "Academia Sinica"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/UnsupervisedPointCloudObjectCoSegmentationbyCoContrastiveLearningandMutualAttentionSampling.pdf"
    },
    {
        "internal_id": 525,
        "title": "Generalized Source-free Domain Adaptation",
        "abstract": "Domain adaptation (DA) aims to transfer the knowledge learned from a source domain to an unlabeled target domain. Some recent works tackle source-free domain adaptation (SFDA) where only a source pre-trained model is available for adaptation to the target domain. However, those methods do not consider keeping source performance which is of high practical value in real world applications. In this paper, we propose a new domain adaptation paradigm called General- ized Source-free Domain Adaptation (G-SFDA), where the learned model needs to perform well on both the target and source domains, with only access to current unlabeled target data during adaptation. First, we propose local structure clustering (LSC), aiming to cluster the target features with its semantically similar neighbors, which successfully adapts the model to the target domain in the absence of source data. Second, we propose sparse domain attention (SDA), it produces a binary domain specific attention to activate dif- ferent feature channels for different domains, meanwhile the domain attention will be utilized to regularize the gradient during adaptation to keep source information. In the exper- iments, for target performance our method is on par with or better than existing DA and SFDA methods, specifically it achieves state-of-the-art performance (85.4%) on VisDA, and our method works well for all domains after adapting to single or multiple target domains. Code is available in https://github.com/Albert0147/G-SFDA.",
        "authors": [
            "Shiqi Yang",
            "Yaxing Wang",
            "Luis Herranz",
            "Shangling Jui"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/GeneralizedSourceFreeDomainAdaptation.pdf"
    },
    {
        "internal_id": 526,
        "title": "AutoSpace: Neural Architecture Search with Less Human Interference",
        "abstract": "Current neural architecture search (NAS) algorithms still require expert knowledge and effort to design a search space for network construction. In this paper, we consider automating the search space design to minimize human in- terference, which however faces two challenges: the ex- plosive complexity of the exploration space and the expen- sive computation cost to evaluate the quality of different search spaces. To solve them, we propose a novel differ- entiable evolutionary framework named AutoSpace, which evolves the search space to an optimal one with follow- ing novel techniques: a differentiable fitness scoring func- tion to efficiently evaluate the performance of cells and a reference architecture to speedup the evolution procedure and avoid falling into sub-optimal solutions. The frame- work is generic and compatible with additional compu- tational constraints, making it feasible to learn special- ized search spaces that fit different computational bud- gets. With the learned search space, the performance of recent NAS algorithms can be improved significantly compared with using previously manually designed spaces. Remarkably, the models generated from the new search space achieve 77.8% top-1 accuracy on ImageNet under the mobile setting (MAdds\u2264500M), outperforming previ- ous SOTA EfficientNet-B0 by 0.7%. https://github. com/zhoudaquan/AutoSpace.git.",
        "authors": [
            "Daquan Zhou",
            "Xiaojie Jin ",
            "Xiaochen Lian",
            "Linjie Yang",
            "Yujing Xue",
            "Qibin Hou",
            "Jiashi Feng"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/AutoSpaceNeuralArchitectureSearchWithLessHumanInterference.pdf"
    },
    {
        "internal_id": 527,
        "title": "Super Resolve Dynamic Scene from Continuous Spike Streams",
        "abstract": "Recently, a novel retina-inspired camera, namely spike camera, has shown great potential for recording high-speed dynamic scenes. Unlike conventional digital cameras that compact the visual information within an exposure inter- val into a single snapshot, the spike camera continuously outputs binary spike streams to record the dynamic scenes, yielding a very high temporal resolution. Most of the ex- isting reconstruction methods for spike camera focus on re- constructing images with the same resolution as spike cam- era. However, as a trade-off of high temporal resolution, the spatial resolution of spike camera is limited, resulting in in- ferior details of the reconstruction. To address this issue, we develop a spike camera super-resolution framework, aim- ing to super resolve high-resolution intensity images from the low-resolution binary spike streams. Due to the relative motion between the camera and the objects to capture, the spikes fired by the same sensor pixel no longer describes the same points in the external scene. In this paper, we ex- ploit the relative motion and derive the relationship between light intensity and each spike, so as to recover the external scene with both high temporal and high spatial resolution. Experimental results demonstrate that the proposed method can reconstruct pleasant high-resolution images from low- resolution spike streams.",
        "authors": [
            "Jing Zhao",
            "Jiyu Xie",
            "Ruiqin Xiong",
            "Jian Zhang",
            "Zhaofei Yu",
            "Tiejun Huang"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/SuperResolveDynamicSceneFromContinuousSpikeStreams.pdf"
    },
    {
        "internal_id": 528,
        "title": "Pyramid Architecture Search for Real-Time Image Deblurring",
        "abstract": "Multi-scale and multi-patch deep models have been shown effective in removing blurs of dynamic scenes. How- ever, these methods still suffer from one major obstacle: manually designing a lightweight and high-efficiency net- work is challenging and time-consuming. To tackle this obstacle, we propose a novel deblurring method, dubbed PyNAS (pyramid neural architecture search network), to- wards automatically designing hyper-parameters includ- ing the scales, patches, and standard cell operators. The proposed PyNAS adopts gradient-based search strategies and innovatively searches the hierarchy patch and scale scheme not limited to cell searching. Specifically, we in- troduce a hierarchical search strategy tailored to the multi- scale and multi-patch deblurring task. The strategy follows the principle that the first distinguishes between the top- level (pyramid-scales and pyramid-patches) and bottom- level variables (cell operators) and then searches multi- scale variables using the top-to-bottom principle. During the search stage, PyNAS employs an early stopping strat- egy to avoid the collapse and computational issues. Fur- thermore, we use a path-level binarization mechanism for multi-scale cell searching to save the memory consumption. Our primary contribution is a real-time deblurring algo- rithm (around 58 fps) for 720p images while achieves state- of-the-art deblurring performance on the GoPro and Video Deblurring datasets.",
        "authors": [
            "Xiaobin Hu",
            "Wenqi Ren",
            "Kaicheng Yu",
            "Kaihao Zhang",
            "Xiaochun Cao",
            "Wei Liu",
            "Bjoern Menze ",
            "TU M unchen"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/PyramidArchitectureSearchforRealTimeImageDeblurring.pdf"
    },
    {
        "internal_id": 529,
        "title": "NAS-OoD: Neural Architecture Search for Out-of-Distribution Generalization",
        "abstract": "Recent advances on Out-of-Distribution (OoD) gener- alization reveal the robustness of deep learning models against distribution shifts. However, existing works focus on OoD algorithms, such as invariant risk minimization, domain generalization, or stable learning, without consid- ering the influence of deep model architectures on OoD gen- eralization, which may lead to sub-optimal performance. Neural Architecture Search (NAS) methods search for ar- chitecture based on its performance on the training data, which may result in poor generalization for OoD tasks. In this work, we propose robust Neural Architecture Search for OoD generalization (NAS-OoD), which optimizes the archi- tecture with respect to its performance on generated OoD data by gradient descent. Specifically, a data generator is learned to synthesize OoD data by maximizing losses com- puted by different neural architectures, while the goal for architecture search is to find the optimal architecture pa- rameters that minimize the synthetic OoD data losses. The data generator and the neural architecture are jointly opti- mized in an end-to-end manner, and the minimax training process effectively discovers robust architectures that gen- eralize well for different distribution shifts. Extensive ex- perimental results show that NAS-OoD achieves superior performance on various OoD generalization benchmarks with deep models having a much fewer number of param- eters. In addition, on a real industry dataset, the proposed NAS-OoD method reduces the error rate by more than 70% compared with the state-of-the-art method, demonstrating the proposed method's practicality for real applications.",
        "authors": [
            "Haoyue Bai",
            "Fengwei Zhou",
            "Lanqing Hong",
            "Nanyang Ye",
            "Zhenguo Li"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/NASOoDNeuralArchitectureSearchforOutofDistributionGeneralization.pdf"
    },
    {
        "internal_id": 530,
        "title": "DTMNet: A Discrete Tchebichef Moments-based Deep Neural Network for Multi-focus Image Fusion",
        "abstract": "Compared with traditional methods, the deep learning- based multi-focus image fusion methods can effectively im- prove the performance of image fusion tasks. However, the existing deep learning-based methods encounter a com- mon issue of a large number of parameters, which leads to the deep learning models with high time complexity and low fusion efficiency. To address this issue, we propose a novel discrete Tchebichef moment-based Deep neural net- work, termed as DTMNet, for multi-focus image fusion. The proposed DTMNet is an end-to-end deep neural network with only one convolutional layer and three fully connected layers. The convolutional layer is fixed with DTM co- efficients (DTMConv) to extract high/low-frequency infor- mation without learning parameters effectively. The three fully connected layers have learnable parameters for fea- ture classification. Therefore, the proposed DTMNet for multi-focus image fusion has a small number of parameters (0.01M paras vs. 4.93M paras of regular CNN) and high computational efficiency (0.32s vs. 79.09s by regular CNN to fuse an image). In addition, a large-scale multi-focus image dataset is synthesized for training and verifying the deep learning model. Experimental results on three pub- lic datasets demonstrate that the proposed method is com- petitive with or even outperforms the state-of-the-art multi- focus image fusion methods in terms of subjective visual perception and objective evaluation metrics.",
        "authors": [
            "Bin Xiao",
            "Haifeng Wu",
            "Xiuli Bi"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/DTMNetADiscreteTchebichefMomentsBasedDeepNeuralNetworkforMultiFocusImageFusion.pdf"
    },
    {
        "internal_id": 531,
        "title": "C2N: Practical Generative Noise Modeling for Real-World Denoising",
        "abstract": "Learning-based image denoising methods have been bounded to situations where well-aligned noisy and clean images are given, or samples are synthesized from prede- termined noise models, e.g., Gaussian. While recent gener- ative noise modeling methods aim to simulate the unknown distribution of real-world noise, several limitations still ex- ist. In a practical scenario, a noise generator should learn to simulate the general and complex noise distribution with- out using paired noisy and clean images. However, since existing methods are constructed on the unrealistic assump- tion of real-world noise, they tend to generate implausi- ble patterns and cannot express complicated noise maps. Therefore, we introduce a Clean-to-Noisy image genera- tion framework, namely C2N, to imitate complex real-world noise without using any paired examples. We construct the noise generator in C2N accordingly with each component of real-world noise characteristics to express a wide range of noise accurately. Combined with our C2N, conventional denoising CNNs can be trained to outperform existing un- supervised methods on challenging real-world benchmarks by a large margin.",
        "authors": [
            "Geonwoon Jang",
            "Wooseok Lee",
            "Sanghyun Son",
            "Kyoungmu Lee"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/C2NPracticalGenerativeNoiseModelingforRealWorldDenoising.pdf"
    },
    {
        "internal_id": 532,
        "title": "Weak Adaptation Learning: Addressing Cross-domain Data Insufficiency with Weak Annotator",
        "abstract": "Data quantity and quality are crucial factors for data- driven learning methods. In some target problem domains, there are not many data samples available, which could significantly hinder the learning process. While data from similar domains may be leveraged to help through domain adaptation, obtaining high-quality labeled data for those source domains themselves could be difficult or costly. To address such challenges on data insufficiency for classifica- tion problem in a target domain, we propose a weak adap- tation learning (WAL) approach that leverages unlabeled data from a similar source domain, a low-cost weak an- notator that produces labels based on task-specific heuris- tics, labeling rules, or other methods (albeit with inaccu- racy), and a small amount of labeled data in the target do- main. Our approach first conducts a theoretical analysis on the error bound of the trained classifier with respect to the data quantity and the performance of the weak annotator, and then introduces a multi-stage weak adaptation learning method to learn an accurate classifier by lowering the error bound. Our experiments demonstrate the effectiveness of our approach in learning an accurate classifier with limited labeled data in the target domain and unlabeled data in the source domain.",
        "authors": [
            "Shichao Xu",
            "Lixu Wang",
            "Yixuan Wang",
            "Qi Zhu"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/WeakAdaptationLearningAddressingCrossDomainDataInsufficiencyWithWeakAnnotator.pdf"
    },
    {
        "internal_id": 533,
        "title": "Learning to Better Segment Objects from Unseen Classes with Unlabeled Videos",
        "abstract": "The ability to localize and segment objects from unseen classes would open the door to new applications, such as autonomous object learning in active vision. Nonetheless, improving the performance on unseen classes requires addi- tional training data, while manually annotating the objects of the unseen classes can be labor-extensive and expensive. In this paper, we explore the use of unlabeled video se- quences to automatically generate training data for objects of unseen classes. It is in principle possible to apply existing video segmentation methods to unlabeled videos and auto- matically obtain object masks, which can then be used as a training set even for classes with no manual labels avail- able. However, our experiments show that these methods do not perform well enough for this purpose. We therefore introduce a Bayesian method that is specifically designed to automatically create such a training set: Our method starts from a set of object proposals and relies on (non-realistic) analysis-by-synthesis to select the correct ones by perform- ing an efficient optimization over all the frames simulta- neously. Through extensive experiments, we show that our method can generate a high-quality training set which sig- nificantly boosts the performance of segmenting objects of unseen classes. We thus believe that our method could open the door for open-world instance segmentation by exploit- ing abundant Internet videos.",
        "authors": [
            "Yuming Du",
            "Yang Xiao",
            "Ecole des Ponts",
            "Univ Gustave Eiffel"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/LearningToBetterSegmentObjectsFromUnseenClassesWithUnlabeledVideos.pdf"
    },
    {
        "internal_id": 534,
        "title": "Contrastive Learning of Image Representations with Cross-Video Cycle-Consistency",
        "abstract": "Recent works have advanced the performance of self- supervised representation learning by a large margin. The core among these methods is intra-image invariance learn- ing. Two different transformations of one image instance are considered as a positive sample pair, where various tasks are designed to learn invariant representations by comparing the pair. Analogically, for video data, rep- resentations of frames from the same video are trained to be closer than frames from other videos, i.e. intra- video invariance. However, cross-video relation has barely been explored for visual representation learning. Un- like intra-video invariance, ground-truth labels of cross- video relation is usually unavailable without human labors. In this paper, we propose a novel contrastive learning method which explores the cross-video relation by using cycle-consistency for general image representation learn- ing. This allows to collect positive sample pairs across dif- ferent video instances, which we hypothesize will lead to higher-level semantics. We validate our method by trans- ferring our image representation to multiple downstream tasks including visual object tracking, image classification, and action recognition. We show significant improvement over state-of-the-art contrastive learning methods. Project page is available at https://happywu.github.io/ cycle_contrast_video .",
        "authors": [
            "Haiping Wu",
            "Xiaolong Wang"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/ContrastiveLearningofImageRepresentationsWithCrossVideoCycleConsistency.pdf"
    },
    {
        "internal_id": 535,
        "title": "Modulated Periodic Activations for Generalizable Local Functional Representations",
        "abstract": "Multi-Layer Perceptrons (MLPs) make powerful func- tional representations for sampling and reconstruction prob- lems involving low-dimensional signals like images, shapes and light fields. Recent works have significantly improved their ability to represent high-frequency content by using periodic activations or positional encodings. This often came at the expense of generalization: modern methods are typically optimized for a single signal. We present a new representation that generalizes to multiple instances and achieves state-of-the-art fidelity. We use a dual-MLP archi- tecture to encode the signals. A synthesis network creates a functional mapping from a low-dimensional input (e.g. pixel-position) to the output domain (e.g. RGB color). A modulation network maps a latent code corresponding to the target signal to parameters that modulate the periodic activations of the synthesis network. We also propose a local- functional representation which enables generalization. The signal's domain is partitioned into a regular grid, with each tile represented by a latent code. At test time, the signal is en- coded with high-fidelity by inferring (or directly optimizing) the latent code-book. Our approach produces generalizable functional representations of images, videos and shapes, and achieves higher reconstruction quality than prior works that are optimized for a single signal.",
        "authors": [
            "Ishit Mehta",
            "Micha el Gharbi",
            "Connelly Barnes",
            "Eli Shechtman",
            "Ravi Ramamoorthi"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/ModulatedPeriodicActivationsforGeneralizableLocalFunctionalRepresentations.pdf"
    },
    {
        "internal_id": 536,
        "title": "End-to-end robust joint unsupervised image alignment and clustering",
        "abstract": "Computing dense pixel-to-pixel image correspondences is a fundamental task of computer vision. Often, the objec- tive is to align image pairs from the same semantic cate- gory for manipulation or segmentation purposes. Despite achieving superior performance, existing deep learning alignment methods cannot cluster images; consequently, clustering and pairing images needed to be a separate la- borious and expensive step. Given a dataset with diverse semantic categories, we propose a multi-task model, Jim-Net, that can directly learn to cluster and align images without any pixel-level or image-level annotations. We design a pair-matching alignment unsupervised training algorithm that selectively matches and aligns image pairs from the clustering branch. Our unsupervised Jim-Net achieves comparable accuracy with state-of-the-art supervised methods on benchmark 2D image alignment dataset PF-PASCAL. Specifically, we ap- ply Jim-Net to cryo-electron tomography, a revolutionary 3D microscopy imaging technique of native subcellular structures. After extensive evaluation on seven datasets, we demonstrate that Jim-Net enables systematic discovery and recovery of representative macromolecular structures in situ, which is essential for revealing molecular mechanisms underlying cellular functions. To our knowledge, Jim-Net is the first end-to-end model that can simultaneously align and cluster images, which significantly improves the perfor- mance as compared to performing each task alone.",
        "authors": [
            "Xiangrui Zeng",
            "Computational Biology",
            "PA ",
            "Gregory Howe",
            "PA ",
            "Min Xu",
            "Computational Biology",
            "PA "
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/EndtoEndRobustJointUnsupervisedImageAlignmentandClustering.pdf"
    },
    {
        "internal_id": 537,
        "title": "Neural Articulated Radiance Field",
        "abstract": "We present Neural Articulated Radiance Field (NARF), a novel deformable 3D representation for articulated ob- jects learned from images. While recent advances in 3D im- plicit representation have made it possible to learn models of complex objects, learning pose-controllable representa- tions of articulated objects remains a challenge, as current methods require 3D shape supervision and are unable to render appearance. In formulating an implicit representa- tion of 3D articulated objects, our method considers only the rigid transformation of the most relevant object part in solving for the radiance field at each 3D location. In this way, the proposed method represents pose-dependent changes without significantly increasing the computational complexity. NARF is fully differentiable and can be trained from images with pose annotations. Moreover, through the use of an autoencoder, it can learn appearance variations over multiple instances of an object class. Experiments show that the proposed method is efficient and can general- ize well to novel poses. The code is available for research purposes at https://github.com/nogu-atsu/NARF.",
        "authors": [],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/NeuralArticulatedRadianceField.pdf"
    },
    {
        "internal_id": 538,
        "title": "Improving Generalization of Batch Whitening by Convolutional Unit Optimization",
        "abstract": "Batch Whitening is a technique that accelerates and sta- bilizes training by transforming input features to have a zero mean (Centering) and a unit variance (Scaling), and by removing linear correlation between channels (Decor- relation). In commonly used structures, which are empir- ically optimized with Batch Normalization, the normaliza- tion layer appears between convolution and activation func- tion. Following Batch Whitening studies have employed the same structure without further analysis; even Batch Whiten- ing was analyzed on the premise that the input of a lin- ear layer is whitened. To bridge the gap, we propose a new Convolutional Unit that in line with the theory, and our method generally improves the performance of Batch Whitening. Moreover, we show the inefficacy of the original Convolutional Unit by investigating rank and correlation of features. As our method is employable off-the-shelf whiten- ing modules, we use Iterative Normalization (IterNorm), the state-of-the-art whitening module, and obtain significantly improved performance on five image classification datasets: CIFAR-10, CIFAR-100, CUB-200-2011, Stanford Dogs, and ImageNet. Notably, we verify that our method improves stability and performance of whitening when using large learning rate, group size, and iteration number. Code is available at https://github.com/YooshinCho/ pytorch_ConvUnitOptimization.",
        "authors": [
            "Yooshin Cho",
            "Hanbyel Cho",
            "Youngsoo Kim",
            "Junmo Kim"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/ImprovingGeneralizationofBatchWhiteningbyConvolutionalUnitOptimization.pdf"
    },
    {
        "internal_id": 539,
        "title": "CPF: Learning a Contact Potential Field to Model the Hand-Object Interaction",
        "abstract": "Modeling the hand-object (HO) interaction not only re- quires estimation of the HO pose, but also pays attention to the contact due to their interaction. Significant progress has been made in estimating hand and object separately with deep learning methods, simultaneous HO pose estimation and contact modeling has not yet been fully explored. In this paper, we present an explicit contact representation namely Contact Potential Field (CPF), and a learning-fitting hy- brid framework namely MIHO to Modeling the Interaction of Hand and Object. In CPF, we treat each contacting HO vertex pair as a spring-mass system. Hence the whole sys- tem forms a potential field with minimal elastic energy at the grasp position. Extensive experiments on the two com- monly used benchmarks have demonstrated that our method can achieve state-of-the-art in several reconstruction met- rics, and allow us to produce more physically plausible HO pose even when the ground-truth exhibits severe in- terpenetration or disjointedness. Our code is available at https://github.com/lixiny/CPF.",
        "authors": [
            "Lixin Yang"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/CPFLearningaContactPotentialFieldToModeltheHandObjectInteraction.pdf"
    },
    {
        "internal_id": 540,
        "title": "Explaining Local, Global, And Higher-Order Interactions In Deep Learning",
        "abstract": "We present a simple yet highly generalizable method for explaining interacting parts within a neural network's rea- soning process. First, we design an algorithm based on cross derivatives for computing statistical interaction ef- fects between individual features, which is generalized to both 2-way and higher-order (3-way or more) interactions. We present results side by side with a weight-based attri- bution technique, corroborating that cross derivatives are a superior metric for both 2-way and higher-order interaction detection. Moreover, we extend the use of cross derivatives as an explanatory device in neural networks to the computer vision setting by expanding Grad-CAM, a popular gradient- based explanatory tool for CNNs, to the higher order. While Grad-CAM can only explain the importance of individual objects in images, our method, which we call Taylor-CAM, can explain a neural network's relational reasoning across multiple objects. We show the success of our explanations both qualitatively and quantitatively, including with a user study. We will release all code as a tool package to facilitate explainable deep learning.",
        "authors": [
            "Samuel Lerman",
            "Charles Venuto",
            "Henry Kautz",
            "Chenliang Xu"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/ExplainingLocalGlobalandHigherOrderInteractionsinDeepLearning.pdf"
    },
    {
        "internal_id": 541,
        "title": "H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction",
        "abstract": "Recent learning approaches that implicitly represent sur- face geometry using coordinate-based neural representa- tions have shown impressive results in the problem of multi- view 3D reconstruction. The effectiveness of these tech- niques is, however, subject to the availability of a large number (several tens) of input views of the scene, and com- putationally demanding optimizations. In this paper, we tackle these limitations for the specific problem of few-shot full 3D head reconstruction, by endowing coordinate-based representations with a probabilistic shape prior that en- ables faster convergence and better generalization when using few input images (down to three). First, we learn a shape model of 3D heads from thousands of incomplete raw scans using implicit representations. At test time, we jointly overfit two coordinate-based neural networks to the scene, one modelling the geometry and another estimat- ing the surface radiance, using implicit differentiable ren- dering. We devise a two-stage optimization strategy in which the learned prior is used to initialize and constrain the geometry during an initial optimization phase. Then, the prior is unfrozen and fine-tuned to the scene. By do- ing this, we achieve high-fidelity head reconstructions, in- cluding hair and shoulders, and with a high level of detail that consistently outperforms both state-of-the-art 3D Mor- phable Models methods in the few-shot scenario, and non- parametric methods when large sets of views are available.",
        "authors": [
            "Eduard Ramon",
            "Gil Triginer",
            "Janna Escur",
            "Albert Pumarola",
            "Jaime Garcia",
            "Francesc Moreno Noguer",
            "Crisalix SA",
            "CSIC UPC",
            "bpc "
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/H3DNetFewShotHighFidelity3DHeadReconstruction.pdf"
    },
    {
        "internal_id": 542,
        "title": "MGSampler: An Explainable Sampling Strategy for Video Action Recognition",
        "abstract": "Frame sampling is a fundamental problem in video ac- tion recognition due to the essential redundancy in time and limited computation resources. The existing sampling strategy often employs a fixed frame selection and lacks the flexibility to deal with complex variations in videos. In this paper, we present a simple, sparse, and explain- able frame sampler, termed as Motion-Guided Sampler (MGSampler). Our basic motivation is that motion is an important and universal signal that can drive us to adap- tively select frames from videos. Accordingly, we propose two important properties in our MGSampler design: mo- tion sensitive and motion uniform. First, we present two different motion representations to enable us to efficiently distinguish the motion-salient frames from the background. Then, we devise a motion-uniform sampling strategy based on the cumulative motion distribution to ensure the sam- pled frames evenly cover all the important segments with high motion salience. Our MGSampler yields a new prin- cipled and holistic sampling scheme, that could be in- corporated into any existing video architecture. Experi- ments on five benchmarks demonstrate the effectiveness of our MGSampler over the previous fixed sampling strate- gies, and its generalization power across different back- bones, video models, and datasets. The code is available at https://github.com/MCG-NJU/MGSampler.",
        "authors": [
            "Yuan Zhi",
            "Zhan Tong",
            "Limin Wang",
            "Gangshan Wu"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/MGSamplerAnExplainableSamplingStrategyforVideoActionRecognition.pdf"
    },
    {
        "internal_id": 543,
        "title": "Multi-Echo LiDAR for 3D Object Detection",
        "abstract": "LiDAR sensors can be used to obtain a wide range of measurement signals other than a simple 3D point cloud, and those signals can be leveraged to improve perception tasks like 3D object detection. A single laser pulse can be partially reflected by multiple objects along its path, result- ing in multiple measurements called echoes. Multi-echo measurement can provide information about object con- tours and semi-transparent surfaces which can be used to better identify and locate objects. LiDAR can also measure surface reflectance (intensity of laser pulse return), as well as ambient light of the scene (sunlight reflected by objects). These signals are already available in commercial LiDAR devices but have not been used in most LiDAR-based de- tection models. We present a 3D object detection model which leverages the full spectrum of measurement signals provided by LiDAR. First, we propose a multi-signal fusion (MSF) module to combine (1) the reflectance and ambient features extracted with a 2D CNN, and (2) point cloud fea- tures extracted using a 3D graph neural network (GNN). Second, we propose a multi-echo aggregation (MEA) mod- ule to combine the information encoded in different sets of echo points. Compared with traditional single echo point cloud methods, our proposed Multi-Signal LiDAR Detector (MSLiD) extracts richer context information from a wider range of sensing measurements and achieves more accurate 3D object detection. Experiments show that by incorporat- ing the multi-modality of LiDAR, our method outperforms the state-of-the-art by up to relatively 9.1%.",
        "authors": [
            "Yunze Man",
            "Xinshuo Weng",
            "Prasanna Kumar Sivakumar",
            "Matthew O Toole",
            "Kris Kitani"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/MultiEchoLiDARfor3DObjectDetection.pdf"
    },
    {
        "internal_id": 544,
        "title": "Rethinking preventing class-collapsing in metric learning with margin-based losses",
        "abstract": "Metric learning seeks perceptual embeddings where vi- sually similar instances are close and dissimilar instances are apart, but learned representations can be sub-optimal when the distribution of intra-class samples is diverse and distinct sub-clusters are present. Although theoretically with optimal assumptions, margin-based losses such as the triplet loss and margin loss have a diverse family of so- lutions. We theoretically prove and empirically show that under reasonable noise assumptions, margin-based losses tend to project all samples of a class with various modes onto a single point in the embedding space, resulting in class collapse that usually renders the space ill-sorted for classification or retrieval. To address this problem, we pro- pose a simple modification to the embedding losses such that each sample selects its nearest same-class counterpart in a batch as the positive element in the tuple. This allows for the presence of multiple sub-clusters within each class. The adaptation can be integrated into a wide range of met- ric learning losses. Our method demonstrates clear bene- fits on various fine-grained image retrieval datasets over a variety of existing losses; qualitative retrieval results show that samples with similar visual patterns are indeed closer in the embedding space.",
        "authors": [
            "Elad Levi",
            "Tete Xiao",
            "Xiaolong Wang ",
            "Trevor Darrell"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/RethinkingPreventingClassCollapsinginMetricLearningWithMarginBasedLosses.pdf"
    },
    {
        "internal_id": 545,
        "title": "SGPA: Structure-Guided Prior Adaptation for Category-Level 6D Object Pose Estimation",
        "abstract": "Category-level 6D object pose estimation aims to predict the position and orientation for unseen objects, which plays a pillar role in many scenarios such as robotics and aug- mented reality. The significant intra-class variation is the bottleneck challenge in this task yet remains unsolved so far. In this paper, we take advantage of category prior to over- come this problem by innovating a structure-guided prior adaptation scheme to accurately estimate 6D pose for in- dividual objects. Different from existing prior based meth- ods, given one object and its corresponding category prior, we propose to leverage their structure similarity to dynam- ically adapt the prior to the observed object. The prior adaptation intrinsically associates the adopted prior with different objects, from which we can accurately reconstruct the 3D canonical model of the specific object for pose es- timation. To further enhance the structure characteristic of objects, we extract low-rank structure points from the dense object point cloud, therefore more efficiently incorporating sparse structural information during prior adaptation. Ex- tensive experiments on CAMERA25 and REAL275 bench- marks demonstrate significant performance improvement. Project homepage: https://www.cse.cuhk.edu. hk/\u02dckaichen/projects/sgpa/sgpa.html.",
        "authors": [],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/SGPAStructureGuidedPriorAdaptationforCategoryLevel6DObjectPoseEstimation.pdf"
    },
    {
        "internal_id": 546,
        "title": "SLIDE: Single Image 3D Photography with Soft Layering and Depth-aware Inpainting",
        "abstract": "Single image 3D photography enables viewers to view a still image from novel viewpoints. Recent approaches combine monocular depth networks with inpainting net- works to achieve compelling results. A drawback of these techniques is the use of hard depth layering, making them unable to model intricate appearance details such as thin hair-like structures. We present SLIDE, a modular and unified system for single image 3D photography that uses a simple yet effective soft layering strategy to better pre- serve appearance details in novel views. In addition, we propose a novel depth-aware training strategy for our in- painting module, better suited for the 3D photography task. The resulting SLIDE approach is modular, enabling the use of other components such as segmentation and mat- ting for improved layering. At the same time, SLIDE uses an efficient layered depth formulation that only re- quires a single forward pass through the component net- works to produce high quality 3D photos. Extensive ex- perimental analysis on three view-synthesis datasets, in combination with user studies on in-the-wild image col- lections, demonstrate superior performance of our tech- nique in comparison to existing strong baselines while be- ing conceptually much simpler. Project page: https: //varunjampani.github.io/slide",
        "authors": [
            "Varun Jampani",
            "Huiwen Chang",
            "Kyle Sargent",
            "Abhishek Kar",
            "Richard Tucker",
            "Michael Krainin",
            "Dominik Kaeser",
            "William T Freeman",
            "David Salesin",
            "Brian Curless"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/SLIDESingleImage3DPhotographyWithSoftLayeringandDepthAwareInpainting.pdf"
    },
    {
        "internal_id": 547,
        "title": "Fooling LiDAR Perception via Adversarial Trajectory Perturbation",
        "abstract": "LiDAR point clouds collected from a moving vehicle are functions of its trajectories, because the sensor motion needs to be compensated to avoid distortions. When au- tonomous vehicles are sending LiDAR point clouds to deep networks for perception and planning, could the motion compensation consequently become a wide-open backdoor in those networks, due to both the adversarial vulnerability of deep learning and GPS-based vehicle trajectory estima- tion that is susceptible to wireless spoofing? We demon- strate such possibilities for the first time: instead of directly attacking point cloud coordinates which requires tampering with the raw LiDAR readings, only adversarial spoofing of a self-driving car's trajectory with small perturbations is enough to make safety-critical objects undetectable or de- tected with incorrect positions. Moreover, polynomial tra- jectory perturbation is developed to achieve a temporally- smooth and highly-imperceptible attack. Extensive experi- ments on 3D object detection have shown that such attacks not only lower the performance of the state-of-the-art de- tectors effectively, but also transfer to other detectors, rais- ing a red flag for the community. The code is available on https://ai4ce.github.io/FLAT/.",
        "authors": [
            "Yiming Li",
            "Congcong Wen",
            "Felix Juefei Xu",
            "Chen Feng",
            "Alibaba Group"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/FoolingLiDARPerceptionviaAdversarialTrajectoryPerturbation.pdf"
    },
    {
        "internal_id": 548,
        "title": "Prediction by Anticipation: An Action-Conditional Prediction Method based on Interaction Learning",
        "abstract": "In autonomous driving (AD), accurately predicting changes in the environment can effectively improve safety and comfort. Due to complex interactions among traffic participants, however, it is very hard to achieve accurate prediction for a long horizon. To address this challenge, we propose prediction by anticipation, which views interaction in terms of a latent probabilistic generative process wherein some vehicles move partly in response to the anticipated motion of other vehicles. Under this view, consecutive data frames can be factorized into sequential samples from an action-conditional distribution that effectively generalizes to a wider range of actions and driving situations. Our pro- posed prediction model, variational Bayesian in nature, is trained to maximize the evidence lower bound (ELBO) of the log-likelihood of this conditional distribution. Evaluations of our approach with prominent AD datasets NGSIM I-80 and Argoverse show significant improvement over current state-of-the-art in both accuracy and generalization.",
        "authors": [
            "Ershad Banijamali",
            "Mohsen Rohani",
            "Elmira Amirloo",
            "Jun Luo",
            "Pascal Poupart"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/PredictionbyAnticipationAnActionConditionalPredictionMethodBasedonInteractionLearning.pdf"
    },
    {
        "internal_id": 549,
        "title": "Towards Vivid and Diverse Image Colorization with Generative Color Prior",
        "abstract": "Colorization has attracted increasing interest in recent years. Classic reference-based methods usually rely on external color images for plausible results. A large im- age database or online search engine is inevitably required for retrieving such exemplars. Recent deep-learning-based methods could automatically colorize images at a low cost. However, unsatisfactory artifacts and incoherent colors are always accompanied. In this work, we aim at recovering vivid colors by leveraging the rich and diverse color priors encapsulated in a pretrained Generative Adversarial Net- works (GAN). Specifically, we first \"retrieve\" matched fea- tures (similar to exemplars) via a GAN encoder and then in- corporate these features into the colorization process with",
        "authors": [
            "Yanze Wu",
            "Xintao Wang",
            "Yu Li",
            "Honglun Zhang",
            "Xun Zhao",
            "Ying Shan",
            "Tencent PCG",
            "bpc ",
            "bpc ",
            "bpc "
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/TowardsVividandDiverseImageColorizationWithGenerativeColorPrior.pdf"
    },
    {
        "internal_id": 550,
        "title": "End-to-End Unsupervised Document Image Blind Denoising",
        "abstract": "Removing noise from scanned pages is a vital step before their submission to optical character recognition (OCR) system. Most available image denoising methods are su- pervised where the pairs of noisy/clean pages are required. However, this assumption is rarely met in real settings. Be- sides, there is no single model that can remove various noise types from documents. Here, we propose a unified end-to- end unsupervised deep learning model, for the first time, that can effectively remove multiple types of noise, includ- ing salt & pepper noise, blurred and/or faded text, as well as watermarks from documents at various levels of inten- sity. We demonstrate that the proposed model significantly improves the quality of scanned images and the OCR of the pages on several test datasets.",
        "authors": [
            "Mehrdad J Gangeh",
            "Marcin Plata",
            "Nigel P Duffy"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/EndtoEndUnsupervisedDocumentImageBlindDenoising.pdf"
    },
    {
        "internal_id": 551,
        "title": "Detecting Human-Object Relationships in Videos",
        "abstract": "We study a crucial problem in video analysis: human- object relationship detection. The majority of previous ap- proaches are developed only for the static image scenario, without incorporating the temporal dynamics so vital to contextualizing human-object relationships. We propose a model with Intra- and Inter-Transformers, enabling joint spatial and temporal reasoning on multiple visual concepts of objects, relationships, and human poses. We find that applying attention mechanisms among features distributed spatio-temporally greatly improves our understanding of human-object relationships. Our method is validated on two datasets, Action Genome and CAD-120-EVAR, and achieves state-of-the-art performance on both of them.",
        "authors": [
            "Jingwei Ji",
            "Rishi Desai",
            "Juan Carlos Niebles"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/DetectingHumanObjectRelationshipsinVideos.pdf"
    },
    {
        "internal_id": 552,
        "title": "Learning to Stylize Novel Views",
        "abstract": "We tackle a 3D scene stylization problem \u2014 generating stylized images of a scene from arbitrary novel views given a set of images of the same scene and a reference image of the desired style as inputs. Direct solution of combining novel view synthesis and stylization approaches lead to re- sults that are blurry or not consistent across different views. We propose a point cloud-based method for consistent 3D scene stylization. First, we construct the point cloud by back-projecting the image features to the 3D space. Sec- ond, we develop point cloud aggregation modules to gather the style information of the 3D scene, and then modulate the features in the point cloud with a linear transformation matrix. Finally, we project the transformed features to 2D space to obtain the novel views. Experimental results on two diverse datasets of real-world scenes validate that our method generates consistent stylized novel view synthesis results against other alternative approaches.",
        "authors": [
            "Hsin Ping Huang",
            "Hung Yu Tseng",
            "Saurabh Saini",
            "Maneesh Singh",
            "Ming Hsuan Yang",
            "UC Merced",
            "Verisk Analytics",
            "bpc ",
            "bpc ",
            "bpc "
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/LearningToStylizeNovelViews.pdf"
    },
    {
        "internal_id": 553,
        "title": "Learning to Match Features with Seeded Graph Matching Network",
        "abstract": "Matching local features across images is a fundamen- tal problem in computer vision. Targeting towards high ac- curacy and efficiency, we propose Seeded Graph Matching Network, a graph neural network with sparse structure to reduce redundant connectivity and learn compact represen- tation. The network consists of 1) Seeding Module, which initializes the matching by generating a small set of reli- able matches as seeds. 2) Seeded Graph Neural Network, which utilizes seed matches to pass messages within/across images and predicts assignment costs. Three novel opera- tions are proposed as basic elements for message passing: 1) Attentional Pooling, which aggregates keypoint features within the image to seed matches. 2) Seed Filtering, which enhances seed features and exchanges messages across im- ages. 3) Attentional Unpooling, which propagates seed fea- tures back to original keypoints. Experiments show that our method reduces computational and memory complexity sig- nificantly compared with typical attention-based networks while competitive or higher performance is achieved.",
        "authors": [],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/LearningToMatchFeaturesWithSeededGraphMatchingNetwork.pdf"
    },
    {
        "internal_id": 554,
        "title": "Joint Visual Semantic Reasoning: Multi-Stage Decoder for Text Recognition",
        "abstract": "Although text recognition has significantly evolved over the years, state-of the-art (SOTA) models still struggle in the wild scenarios due to complex backgrounds, varying fonts, uncontrolled illuminations, distortions and other artifacts. This is because such models solely depend on visual infor- mation for text recognition, thus lacking semantic reasoning capabilities. In this paper, we argue that semantic informa- tion offers a complementary role in addition to visual only. More specifically, we additionally utilize semantic informa- tion by proposing a multi-stage multi-scale attentional de- coder that performs joint visual-semantic reasoning. Our novelty lies in the intuition that for text recognition, predic- tion should be refined in a stage-wise manner. Therefore our key contribution is in designing a stage-wise unrolling attentional decoder where non-differentiability, invoked by discretely predicted character labels, needs to be bypassed for end-to-end training. While the first stage predicts using visual features, subsequent stages refine on-top of it using joint visual-semantic information. Additionally, we intro- duce multi-scale 2D attention along with dense and resid- ual connections between different stages to deal with vary- ing scales of character sizes, for better performance and faster convergence during training. Experimental results show our approach to outperform existing SOTA methods by a considerable margin.",
        "authors": [
            "Ayan Kumar Bhunia",
            "Aneeshan Sain",
            "Pinaki Nath Chowdhury"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/JointVisualSemanticReasoningMultiStageDecoderforTextRecognition.pdf"
    },
    {
        "internal_id": 555,
        "title": "Wanderlust: Online Continual Object Detection in the Real World",
        "abstract": "Online continual learning from data streams in dynamic environments is a critical direction in the computer vision field. However, realistic benchmarks and fundamental stud- ies in this line are still missing. To bridge the gap, we present a new online continual object detection benchmark with an egocentric video dataset, Objects Around Krishna (OAK). OAK adopts the KrishnaCAM videos, an ego-centric video stream collected over nine months by a graduate stu- dent. OAK provides exhaustive bounding box annotations of 80 video snippets (\u223c17.5 hours) for 105 object categories in outdoor scenes. The emergence of new object categories in our benchmark follows a pattern similar to what a sin- gle person might see in their day-to-day life. The dataset also captures the natural distribution shifts as the person travels to different places. These egocentric long running videos provide a realistic playground for continual learn- ing algorithms, especially in online embodied settings. We also introduce new evaluation metrics to evaluate the model performance and catastrophic forgetting and provide base- line studies for online continual object detection. We believe this benchmark will pose new exciting challenges for learn- ing from non-stationary data in continual learning. The OAK dataset and the associated benchmark are released at https://oakdata.github.io/.",
        "authors": [
            "Jianren Wang",
            "Xin Wang",
            "Yue Shang Guan",
            "Abhinav Gupta",
            "bpc "
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/WanderlustOnlineContinualObjectDetectionintheRealWorld.pdf"
    },
    {
        "internal_id": 556,
        "title": "Hierarchical Kinematic Probability Distributions for 3D Human Shape and Pose Estimation from Images in the Wild",
        "abstract": "This paper addresses the problem of 3D human body shape and pose estimation from an RGB image. This is of- ten an ill-posed problem, since multiple plausible 3D bodies may match the visual evidence present in the input - partic- ularly when the subject is occluded. Thus, it is desirable to estimate a distribution over 3D body shape and pose conditioned on the input image instead of a single 3D re- construction. We train a deep neural network to estimate a hierarchical matrix-Fisher distribution over relative 3D joint rotation matrices (i.e. body pose), which exploits the human body's kinematic tree structure, as well as a Gaus- sian distribution over SMPL body shape parameters. To fur- ther ensure that the predicted shape and pose distributions match the visual evidence in the input image, we implement a differentiable rejection sampler to impose a reprojection loss between ground-truth 2D joint coordinates and sam- ples from the predicted distributions, projected onto the im- age plane. We show that our method is competitive with the state-of-the-art in terms of 3D shape and pose metrics on the SSP-3D and 3DPW datasets, while also yielding a structured probability distribution over 3D body shape and pose, with which we can meaningfully quantify prediction uncertainty and sample multiple plausible 3D reconstruc- tions to explain a given input image.",
        "authors": [
            "Akash Sengupta",
            "Ignas Budvytis",
            "Roberto Cipolla"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/HierarchicalKinematicProbabilityDistributionsfor3DHumanShapeandPoseEstimationFromImagesintheWild.pdf"
    },
    {
        "internal_id": 557,
        "title": "AdaFit: Rethinking Learning-based Normal Estimation on Point Clouds",
        "abstract": "This paper presents a neural network for robust nor- mal estimation on point clouds, named AdaFit, that can deal with point clouds with noise and density variations. Existing works use a network to learn point-wise weights for weighted least squares surface fitting to estimate the normals, which has difficulty in finding accurate normals in complex regions or containing noisy points. By ana- lyzing the step of weighted least squares surface fitting, we find that it is hard to determine the polynomial order of the fitting surface and the fitting surface is sensitive to outliers. To address these problems, we propose a simple yet effective solution that adds an additional offset predic- tion to improve the quality of normal estimation. Further- more, in order to take advantage of points from different neighborhood sizes, a novel Cascaded Scale Aggregation layer is proposed to help the network predict more accu- rate point-wise offsets and weights. Extensive experiments demonstrate that AdaFit achieves state-of-the-art perfor- mance on both the synthetic PCPNet dataset and the real- word SceneNN dataset. The code is publicly available at https://github.com/Runsong123/AdaFit.",
        "authors": [
            "Runsong Zhu",
            "Yuan Liu",
            "Zhen Dong",
            "Yuan Wang",
            "Tengping Jiang",
            "Wenping Wang",
            "Bisheng Yang"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/AdaFitRethinkingLearningBasedNormalEstimationonPointClouds.pdf"
    },
    {
        "internal_id": 558,
        "title": "Deep Matching Prior: Test-Time Optimization for Dense Correspondence",
        "abstract": "Conventional techniques to establish dense correspon- dences across visually or semantically similar images fo- cused on designing a task-specific matching prior, which is difficult to model in general. To overcome this, recent learning-based methods have attempted to learn a good matching prior within a model itself on large training data. The performance improvement was apparent, but the need for sufficient training data and intensive learning hinders their applicability. Moreover, using the fixed model at test time does not account for the fact that a pair of images may require their own prior, thus providing limited performance and poor generalization to unseen images. In this paper, we show that an image pair-specific prior can be captured by solely optimizing the untrained match- ing networks on an input pair of images. Tailored for such test-time optimization for dense correspondence, we present a residual matching network and a confidence-aware con- trastive loss to guarantee a meaningful convergence. Ex- periments demonstrate that our framework, dubbed Deep Matching Prior (DMP), is competitive, or even outper- forms, against the latest learning-based methods on several benchmarks for geometric matching and semantic match- ing, even though it requires neither large training data nor intensive learning. With the networks pre-trained, DMP at- tains state-of-the-art performance on all benchmarks.",
        "authors": [
            "Sunghwan Hong",
            "Seungryong Kim"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/DeepMatchingPriorTestTimeOptimizationforDenseCorrespondence.pdf"
    },
    {
        "internal_id": 559,
        "title": "Adaptive Label Noise Cleaning with Meta-Supervision for Deep Face Recognition",
        "abstract": "The training of a deep face recognition system usually faces the interference of label noise in the training data. However, it is difficult to obtain a high-precision cleaning model to remove these noises. In this paper, we propose an adaptive label noise cleaning algorithm based on meta- learning for face recognition datasets, which can learn the distribution of the data to be cleaned and make automatic adjustments based on class differences. It first learns re- liable cleaning knowledge from well-labeled noisy data, then gradually transfers it to the target data with meta- supervision to improve performance. A threshold adapter module is also proposed to address the drift problem in transfer learning methods. Extensive experiments clean two noisy in-the-wild face recognition datasets and show the ef- fectiveness of the proposed method to reach state-of-the-art performance on the IJB-C face recognition benchmark.",
        "authors": [
            "Yaobin Zhang",
            "Weihong Deng",
            "Yaoyao Zhong",
            "Jiani Hu",
            "Xian Li",
            "Dongyue Zhao",
            "Dongchao Wen"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/AdaptiveLabelNoiseCleaningWithMetaSupervisionforDeepFaceRecognition.pdf"
    },
    {
        "internal_id": 560,
        "title": "STR-GQN: Scene Representation and Rendering for Unknown Cameras Based on Spatial Transformation Routing",
        "abstract": "Geometry-aware modules are widely applied in recent deep learning architectures for scene representation and rendering. However, these modules require intrinsic cam- era information that might not be obtained accurately. In this paper, we propose a Spatial Transformation Routing (STR) mechanism to model the spatial properties without applying any geometric prior. The STR mechanism treats the spatial transformation as the message passing process, and the relation between the view poses and the routing weights is modeled by an end-to-end trainable neural net- work. Besides, an Occupancy Concept Mapping (OCM) framework is proposed to provide explainable rationals for scene-fusion processes. We conducted experiments on sev- eral datasets and show that the proposed STR mechanism improves the performance of the Generative Query Network (GQN). The visualization results reveal that the routing pro- cess can pass the observed information from one location of some view to the associated location in the other view, which demonstrates the advantage of the proposed model in terms of spatial cognition.",
        "authors": [
            "Wen Cheng Chen",
            "Min Chun Hu",
            "Chu Song Chen"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/STRGQNSceneRepresentationandRenderingforUnknownCamerasBasedonSpatialTransformationRouting.pdf"
    },
    {
        "internal_id": 561,
        "title": "Towards Flexible Blind JPEG Artifacts Removal",
        "abstract": "Training a single deep blind model to handle differ- ent quality factors for JPEG image artifacts removal has been attracting considerable attention due to its conve- nience for practical usage. However, existing deep blind methods usually directly reconstruct the image without pre- dicting the quality factor, thus lacking the flexibility to con- trol the output as the non-blind methods. To remedy this problem, in this paper, we propose a flexible blind convo- lutional neural network, namely FBCNN, that can predict the adjustable quality factor to control the trade-off be- tween artifacts removal and details preservation. Specifi- cally, FBCNN decouples the quality factor from the JPEG image via a decoupler module and then embeds the pre- dicted quality factor into the subsequent reconstructor mod- ule through a quality factor attention block for flexible con- trol. Besides, We find existing methods are prone to fail on non-aligned double JPEG images even with only one pixel shift, and we thus propose a double JPEG degrada- tion model to augment the training data. Extensive experi- ments on single JPEG images, more general double JPEG images and real-world JPEG images demonstrate that our proposed FBCNN achieves favorable performance against state-of-the-art methods in terms of both quantitative met- rics and visual quality.",
        "authors": [
            "Jiaxi Jiang",
            "Kai Zhang",
            "Radu Timofte"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/TowardsFlexibleBlindJPEGArtifactsRemoval.pdf"
    },
    {
        "internal_id": 562,
        "title": "PIRenderer: Controllable Portrait Image Generation via Semantic Neural Rendering",
        "abstract": "Generating portrait images by controlling the motions of existing faces is an important task of great consequence to social media industries. For easy use and intuitive con- trol, semantically meaningful and fully disentangled pa- rameters should be used as modifications. However, many existing techniques do not provide such fine-grained con- trols or use indirect editing methods i.e. mimic motions of other individuals. In this paper, a Portrait Image Neural Renderer (PIRenderer) is proposed to control the face mo- tions with the parameters of three-dimensional morphable face models (3DMMs). The proposed model can generate photo-realistic portrait images with accurate movements according to intuitive modifications. Experiments on both direct and indirect editing tasks demonstrate the superior- ity of this model. Meanwhile, we further extend this model to tackle the audio-driven facial reenactment task by ex- tracting sequential motions from audio inputs. We show that our model can generate coherent videos with convinc- ing movements from only a single reference image and a driving audio stream. Our source code is available at https://github.com/RenYurui/PIRender.",
        "authors": [
            " Shan Liu",
            "Tencent America"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/PIRendererControllablePortraitImageGenerationviaSemanticNeuralRendering.pdf"
    },
    {
        "internal_id": 563,
        "title": "Learning Conditional Knowledge Distillation for Degraded-Reference Image Quality Assessment",
        "abstract": "An important scenario for image quality assessment (IQA) is to evaluate image restoration (IR) algorithms. The state-of-the-art approaches adopt a full-reference paradigm that compares restored images with their corresponding pristine-quality images. However, pristine-quality images are usually unavailable in blind image restoration tasks and real-world scenarios. In this paper, we propose a prac- tical solution named degraded-reference IQA (DR-IQA), which exploits the inputs of IR models, degraded images, as references. Specifically, we extract reference informa- tion from degraded images by distilling knowledge from pristine-quality images. The distillation is achieved through learning a reference space, where various degraded images are encouraged to share the same feature statistics with pristine-quality images. And the reference space is opti- mized to capture deep image priors that are useful for qual- ity assessment. Note that pristine-quality images are only used during training. Our work provides a powerful and dif- ferentiable metric for blind IRs, especially for GAN-based methods. Extensive experiments show that our results can even be close to the performance of full-reference settings.",
        "authors": [
            "Heliang Zheng",
            "Huan Yang",
            "Jianlong Fu",
            "Zheng Jun Zha",
            "Jiebo Luo"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/LearningConditionalKnowledgeDistillationforDegradedReferenceImageQualityAssessment.pdf"
    },
    {
        "internal_id": 564,
        "title": "Transforms based Tensor Robust PCA: Corrupted Low-Rank Tensors Recovery via Convex Optimization",
        "abstract": "This work studies the Tensor Robust Principal Compo- nent Analysis (TRPCA) problem, which aims to exactly re- cover the low-rank and sparse components from their sum. Our model is motivated by the recently proposed linear transforms based tensor-tensor product and tensor SVD. We define a new transforms depended tensor rank and the cor- responding tensor nuclear norm. Then we solve the TR- PCA problem by convex optimization whose objective is a weighted combination of the new tensor nuclear norm and \u21131-norm. In theory, we prove that under some incoherence conditions, the convex program exactly recovers the under- lying low-rank and sparse components with high probabil- ity. Our new TRPCA is much more general since it allows to use any invertible linear transforms. Thus, we have more choices in practice for different tasks and different type of data. Numerical experiments verify our results and the ap- plication on image recovery demonstrates the superiority of our method.",
        "authors": [
            "Canyi Lu"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/TransformsBasedTensorRobustPCACorruptedLowRankTensorsRecoveryviaConvexOptimization.pdf"
    },
    {
        "internal_id": 565,
        "title": "Discovering Human Interactions with Large-Vocabulary Objects via Query and Multi-Scale Detection",
        "abstract": "In this work, we study the problem of human-object in- teraction (HOI) detection with large vocabulary object cat- egories. Previous HOI studies are mainly conducted in the regime of limit object categories (e.g., 80 categories). Their solutions may face new difficulties in both object detection and interaction classification due to the increasing diver- sity of objects (e.g., 1000 categories). Different from pre- vious methods, we formulate the HOI detection as a query problem. We propose a unified model to jointly discover the target objects and predict the corresponding interac- tions based on the human queries, thereby eliminating the need of using generic object detectors, extra steps to as- sociate human-object instances, and multi-stream interac- tion recognition. This is achieved by a repurposed Trans- former unit and a novel cascade detection over multi-scale feature maps. We observe that such a highly-coupled solu- tion brings benefits for both object detection and interac- tion classification in a large vocabulary setting. To study the new challenges of the large vocabulary HOI detection, we assemble two datasets from the publicly available SWiG and 100 Days of Hands datasets. Experiments on these datasets validate that our proposed method can achieve a notable mAP improvement on HOI detection with a faster inference speed than existing one-stage HOI detectors. Our code is available at https://github.com/scwangdyd/ large_vocabulary_hoi_detection.",
        "authors": [
            "Suchen Wang",
            "Kim Hui Yap",
            "Henghui Ding",
            "Jiyan Wu",
            "Junsong Yuan",
            "Yap Peng Tan"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/DiscoveringHumanInteractionsWithLargeVocabularyObjectsviaQueryandMultiScaleDetection.pdf"
    },
    {
        "internal_id": 566,
        "title": "MDETR - Modulated Detection for End-to-End Multi-Modal Understanding",
        "abstract": "Multi-modal reasoning systems rely on a pre-trained ob- ject detector to extract regions of interest from the im- age. However, this crucial module is typically used as a black box, trained independently of the downstream task and on a fixed vocabulary of objects and attributes. This makes it challenging for such systems to capture the long tail of visual concepts expressed in free form text. In this paper we propose MDETR, an end-to-end modulated de- tector that detects objects in an image conditioned on a raw text query, like a caption or a question. We use a transformer-based architecture to reason jointly over text and image by fusing the two modalities at an early stage of the model. We pre-train the network on 1.3M text-image pairs, mined from pre-existing multi-modal datasets hav- ing explicit alignment between phrases in text and objects in the image. We then fine-tune on several downstream tasks such as phrase grounding, referring expression com- prehension and segmentation, achieving state-of-the-art re- sults on popular benchmarks. We also investigate the util- ity of our model as an object detector on a given label set when fine-tuned in a few-shot setting. We show that our pre-training approach provides a way to handle the long tail of object categories which have very few labelled in- stances. Our approach can be easily extended for visual question answering, achieving competitive performance on GQA and CLEVR. The code and models are available at https://github.com/ashkamath/mdetr.",
        "authors": [
            "Nicolas Carion"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/MDETRModulatedDetectionforEndtoEndMultiModalUnderstanding.pdf"
    },
    {
        "internal_id": 567,
        "title": "Accelerating Atmospheric Turbulence Simulation via Learned Phase-to-Space Transform",
        "abstract": "Fast and accurate simulation of imaging through atmo- spheric turbulence is essential for developing turbulence mitigation algorithms. Recognizing the limitations of pre- vious approaches, we introduce a new concept known as the phase-to-space (P2S) transform to significantly speed up the simulation. P2S is built upon three ideas: (1) re- formulating the spatially varying convolution as a set of in- variant convolutions with basis functions, (2) learning the basis function via the known turbulence statistics models, (3) implementing the P2S transform via a light-weight net- work that directly converts the phase representation to spa- tial representation. The new simulator offers 300\u00d7 \u2013 1000\u00d7 speed up compared to the mainstream split-step simulators while preserving the essential turbulence statistics.",
        "authors": [
            "Zhiyuan Mao",
            "Nicholas Chimitt",
            "Stanley H Chan",
            "West Lafayette"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/AcceleratingAtmosphericTurbulenceSimulationviaLearnedPhasetoSpaceTransform.pdf"
    },
    {
        "internal_id": 569,
        "title": "VSAC: Efficient and Accurate Estimator for H and F",
        "abstract": "We present VSAC, a RANSAC-type robust estimator with a number of novelties. It benefits from the introduction of the concept of independent inliers that improves signifi- cantly the efficacy of the dominant plane handling and, also, allows near error-free rejection of incorrect models, without false positives. The local optimization process and its ap- plication is improved so that it is run on average only once. Further technical improvements include adaptive sequen- tial hypothesis verification and efficient model estimation via Gaussian elimination. Experiments on four standard datasets show that VSAC is significantly faster than all its predecessors and runs on average in 1-2 ms, on a CPU. It is two orders of magnitude faster and yet as precise as MAGSAC++, the currently most accurate estimator of two- view geometry. In the repeated runs on EVD, HPatches, PhotoTourism, and Kusvod2 datasets, it never failed.",
        "authors": [
            "Maksym Ivashechkin",
            "Daniel Barath",
            "ETH Z urich"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/VSACEfficientandAccurateEstimatorforHandF.pdf"
    },
    {
        "internal_id": 570,
        "title": "Seminar Learning for Click-Level Weakly Supervised Semantic Segmentation",
        "abstract": "Annotation burden has become one of the biggest barri- ers to semantic segmentation. Approaches based on click- level annotations have therefore attracted increasing atten- tion due to their superior trade-off between supervision and annotation cost. In this paper, we propose seminar learn- ing, a new learning paradigm for semantic segmentation with click-level supervision. The fundamental rationale of seminar learning is to leverage the knowledge from differ- ent networks to compensate for insufficient information pro- vided in click-level annotations. Mimicking a seminar, our seminar learning involves a teacher-student and a student- student module, where a student can learn from both skill- ful teachers and other students. The teacher-student mod- ule uses a teacher network based on the exponential mov- ing average to guide the training of the student network. In the student-student module, heterogeneous pseudo-labels are proposed to bridge the transfer of knowledge among students to enhance each other's performance. Experimen- tal results demonstrate the effectiveness of seminar learn- ing, which achieves the new state-of-the-art performance of 72.51% (mIOU), surpassing previous methods by a large margin of up to 16.88% on the Pascal VOC 2012 dataset.",
        "authors": [
            "Hongjun Chen",
            "Jinbao Wang",
            "Hong Cai Chen",
            "Xiantong Zhen",
            "Feng Zheng",
            "Rongrong Ji",
            "Ling Shao"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/SeminarLearningforClickLevelWeaklySupervisedSemanticSegmentation.pdf"
    },
    {
        "internal_id": 571,
        "title": "Exploring Long Tail Visual Relationship Recognition with Large Vocabulary",
        "abstract": "Several approaches have been proposed in recent liter- ature to alleviate the long-tail problem, mainly in object classification tasks. In this paper, we make the first large- scale study concerning the task of Long-Tail Visual Rela- tionship Recognition (LTVRR). LTVRR aims at improving the learning of structured visual relationships that come from the long-tail (e.g., \"rabbit grazing on grass\"). In this setup, the subject, relation, and object classes each follow a long-tail distribution. To begin our study and make a future benchmark for the community, we introduce two LTVRR- related benchmarks, dubbed VG8K-LT and GQA-LT, built upon the widely used Visual Genome and GQA datasets. We use these benchmarks to study the performance of sev- eral state-of-the-art long-tail models on the LTVRR setup. Lastly, we propose a visiolinguistic hubless (VilHub) loss and a Mixup augmentation technique adapted to LTVRR setup, dubbed as RelMix. Both VilHub and RelMix can be easily integrated on top of existing models and despite be- ing simple, our results show that they can remarkably im- prove the performance, especially on tail classes. Bench- marks, code, and models have been made available at: https://github.com/Vision-CAIR/LTVRR.",
        "authors": [
            "Sherif Abdelkarim",
            "Aniket Agarwal",
            "Panos Achlioptas",
            "Jun Chen",
            "Jiaji Huang",
            "Boyang Li",
            "Kenneth Church",
            "Mohamed Elhoseiny",
            "IIT Roorkee"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/ExploringLongTailVisualRelationshipRecognitionWithLargeVocabulary.pdf"
    },
    {
        "internal_id": 572,
        "title": "Few-Shot Semantic Segmentation with Cyclic Memory Network",
        "abstract": "Few-shot semantic segmentation (FSS) is an important task for novel (unseen) object segmentation under the data- scarcity scenario. However, most FSS methods rely on uni- directional feature aggregation, e.g., from support proto- types to get the query prediction, and from high-resolution features to guide the low-resolution ones. This usually fails to fully capture the cross-resolution feature relationships and thus leads to inaccurate estimates of the query objects. To resolve the above dilemma, we propose a cyclic memory network (CMN) to directly learn to read abundant support information from all resolution features in a cyclic manner. Specifically, we first generate N pairs (key and value) of multi-resolution query features guided by the support fea- ture and its mask. Next, we circularly take one pair of these features as the query to be segmented, and the rest N-1 pairs are written into an external memory accordingly, i.e., this leave-one-out process is conducted for N times. In each cy- cle, the query feature is updated by collaboratively match- ing its key and value with the memory, which can elegantly cover all the spatial locations from different resolutions. Furthermore, we incorporate the query feature re-adding and the query feature recursive updating mechanisms into the memory reading operation. CMN, equipped with these merits, can thus capture cross-resolution relationships and better handle the object appearance and scale variations in FSS. Experiments on PASCAL-5i and COCO-20i well vali- date the effectiveness of our model for FSS.",
        "authors": [
            "Guo Sen Xie",
            "Huan Xiong",
            "Jie Liu",
            "Yazhou Yao",
            "Ling Shao"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/FewShotSemanticSegmentationWithCyclicMemoryNetwork.pdf"
    },
    {
        "internal_id": 573,
        "title": "Synthesis of Compositional Animations from Textual Descriptions",
        "abstract": "\"How can we animate 3D-characters from a movie script or move robots by simply telling them what we would like them to do?\" \"How unstructured and complex can we make a sentence and still generate plausible movements from it?\" These are questions that need to be answered in the long-run, as the field is still in its infancy. Inspired by these problems, we present a new technique for gener- ating compositional actions, which handles complex input sentences. Our output is a 3D pose sequence depicting the actions in the input sentence. We propose a hierarchical two-stream sequential model to explore a finer joint-level mapping between natural language sentences and 3D pose sequences corresponding to the given motion. We learn two manifold representations of the motion, one each for the up- per body and the lower body movements. Our model can generate plausible pose sequences for short sentences de- scribing single actions as well as long complex sentences describing multiple sequential and compositional actions. We evaluate our proposed model on the publicly avail- able KIT Motion-Language Dataset containing 3D pose data with human-annotated sentences. Experimental results show that our model advances the state-of-the-art on text- based motion synthesis in objective evaluations by a margin of 50%. Qualitative evaluations based on a user study in- dicate that our synthesized motions are perceived to be the closest to the ground-truth motion captures for both short and compositional sentences.",
        "authors": [
            "Anindita Ghosh ",
            "Noshaba Cheema",
            "Cennet Oguz",
            "Christian Theobalt"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/SynthesisofCompositionalAnimationsFromTextualDescriptions.pdf"
    },
    {
        "internal_id": 574,
        "title": "Uncertainty-Guided Transformer Reasoning for Camouflaged Object Detection",
        "abstract": "Spotting objects that are visually adapted to their sur- roundings is challenging for both humans and AI. Con- ventional generic / salient object detection techniques are suboptimal for this task because they tend to only discover easy and clear objects, while overlooking the difficult-to- detect ones with inherent uncertainties derived from indis- tinguishable textures. In this work, we contribute a novel approach using a probabilistic representational model in combination with transformers to explicitly reason under uncertainties, namely uncertainty-guided transformer rea- soning (UGTR), for camouflaged object detection. The core idea is to first learn a conditional distribution over the backbone's output to obtain initial estimates and as- sociated uncertainties, and then reason over these uncer- tain regions with attention mechanism to produce final predictions. Our approach combines the benefits of both Bayesian learning and Transformer-based reasoning, al- lowing the model to handle camouflaged object detection by leveraging both deterministic and probabilistic informa- tion. We empirically demonstrate that our proposed ap- proach can achieve higher accuracy than existing state- of-the-art models on CHAMELEON, CAMO and COD10K datasets. Code is available at https://github.com/ fanyang587/UGTR.",
        "authors": [
            "Fan Yang ",
            "Qiang Zhai",
            " AIQ",
            " UESTC",
            " Megvii",
            " IIAI",
            "Equal contributions"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/UncertaintyGuidedTransformerReasoningforCamouflagedObjectDetection.pdf"
    },
    {
        "internal_id": 575,
        "title": "Motion Adaptive Pose Estimation from Compressed Videos",
        "abstract": "Human pose estimation from videos has many real- world applications. Existing methods focus on applying models with a uniform computation profile on fully de- coded frames, ignoring the freely-available motion signals and motion-compensation residuals from the compressed stream. A novel model, called Motion Adaptive Pose Net is proposed to exploit the compressed streams to efficiently de- code pose sequences from videos. The model incorporates a Motion Compensated ConvLSTM to propagate the spatially aligned features, along with an adaptive gate to dynami- cally determine if the computationally expensive features should be extracted from fully decoded frames to compen- sate the motion-warped features, solely based on the resid- ual errors. Leveraging the informative yet readily available signals from compressed streams, we propagate the latent features through our Motion Adaptive Pose Net efficiently Our model outperforms the state-of-the-art models in pose- estimation accuracy on two widely used datasets with only around half of the computation complexity.",
        "authors": [
            "Zhipeng Fan",
            "Jun Liu",
            "Yao Wang",
            "Brooklyn NY"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/MotionAdaptivePoseEstimationFromCompressedVideos.pdf"
    },
    {
        "internal_id": 576,
        "title": "Audio-Visual Floorplan Reconstruction",
        "abstract": "Given only a few glimpses of an environment, how much can we infer about its entire floorplan? Existing methods can map only what is visible or immediately apparent from context, and thus require substantial movements through a space to fully map it. We explore how both audio and visual sensing together can provide rapid floorplan reconstruction from limited viewpoints. Audio not only helps sense geometry outside the camera's field of view, but it also reveals the existence of distant freespace (e.g., a dog barking in another room) and suggests the presence of rooms not visible to the camera (e.g., a dishwasher humming in what must be the kitchen to the left). We introduce AV-Map, a novel multi- modal encoder-decoder framework that reasons jointly about audio and vision to reconstruct a floorplan from a short input video sequence. We train our model to predict both the interior structure of the environment and the associated rooms' semantic labels. Our results on 85 large real-world environments show the impact: with just a few glimpses spanning 26% of an area, we can estimate the whole area with 66% accuracy\u2014substantially better than the state of the art approach for extrapolating visual maps.",
        "authors": [
            "Senthil Purushwalkam ",
            "Vamsi Krishna Ithapu",
            "Carl Schissler",
            "Philip Robinson",
            "Abhinav Gupta",
            "Kristen Grauman"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/AudioVisualFloorplanReconstruction.pdf"
    },
    {
        "internal_id": 577,
        "title": "TransVG: End-to-End Visual Grounding with Transformers",
        "abstract": "In this paper, we present a neat yet effective transformer- based framework for visual grounding, namely TransVG, to address the task of grounding a language query to the corresponding region onto an image. The state-of-the-art methods, including two-stage or one-stage ones, rely on a complex module with manually-designed mechanisms to perform the query reasoning and multi-modal fusion. How- ever, the involvement of certain mechanisms in fusion mod- ule design, such as query decomposition and image scene graph, makes the models easily overfit to datasets with spe- cific scenarios, and limits the plenitudinous interaction be- tween the visual-linguistic context. To avoid this caveat, we propose to establish the multi-modal correspondence by leveraging transformers, and empirically show that the complex fusion modules (e.g., modular attention network, dynamic graph, and multi-modal tree) can be replaced by a simple stack of transformer encoder layers with higher per- formance. Moreover, we re-formulate the visual grounding as a direct coordinates regression problem and avoid mak- ing predictions out of a set of candidates (i.e., region pro- posals or anchor boxes). Extensive experiments are con- ducted on five widely used datasets, and a series of state- of-the-art records are set by our TransVG. We build the benchmark of transformer-based visual grounding frame- work and make the code available at https://github. com/djiajunustc/TransVG.",
        "authors": [
            "Jiajun Deng",
            "Zhengyuan Yang",
            "Tianlang Chen",
            "Wengang Zhou",
            "Houqiang Li"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/TransVGEndtoEndVisualGroundingWithTransformers.pdf"
    },
    {
        "internal_id": 578,
        "title": "Joint Topology-preserving and Feature-refinement Network for Curvilinear Structure Segmentation",
        "abstract": "Curvilinear structure segmentation (CSS) is under se- mantic segmentation, whose applications include crack de- tection, aerial road extraction, and biomedical image seg- mentation. In general, geometric topology and pixel-wise features are two critical aspects of CSS. However, most se- mantic segmentation methods only focus on enhancing fea- ture representations while existing CSS techniques empha- size preserving topology alone. In this paper, we present a Joint Topology-preserving and Feature-refinement Net- work (JTFN) that jointly models global topology and refined features based on an iterative feedback learning strategy. Specifically, we explore the structure of objects to help pre- serve corresponding topologies of predicted masks, thus de- sign a reciprocative two-stream module for CSS and bound- ary detection. In addition, we introduce such topology- aware predictions as feedback guidance that refines atten- tive features by supplementing and enhancing saliencies. To the best of our knowledge, this is the first work that jointly addresses topology preserving and feature refinement for CSS. We evaluate JTFN on four datasets of diverse appli- cations: Crack500, CrackTree200, Roads, and DRIVE. Re- sults show that JTFN performs best in comparison with al- ternative methods. Code is available.1",
        "authors": [
            "Mingfei Cheng",
            "Kaili Zhao",
            "Xuhong Guo",
            "Yajing Xu",
            "Jun Guo"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/JointTopologyPreservingandFeatureRefinementNetworkforCurvilinearStructureSegmentation.pdf"
    },
    {
        "internal_id": 582,
        "title": "SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit Shapes",
        "abstract": "Neural implicit surface representations have emerged as a promising paradigm to capture 3D shapes in a continu- ous and resolution-independent manner. However, adapt- ing them to articulated shapes is non-trivial. Existing ap- proaches learn a backward warp field that maps deformed to canonical points. However, this is problematic since the backward warp field is pose dependent and thus requires large amounts of data to learn. To address this, we in- troduce SNARF, which combines the advantages of linear blend skinning (LBS) for polygonal meshes with those of neural implicit surfaces by learning a forward deforma- tion field without direct supervision. This deformation field is defined in canonical, pose-independent, space, enabling generalization to unseen poses. Learning the deformation field from posed meshes alone is challenging since the cor- respondences of deformed points are defined implicitly and may not be unique under changes of topology. We propose a forward skinning model that finds all canonical correspon- dences of any deformed point using iterative root finding. We derive analytical gradients via implicit differentiation, enabling end-to-end training from 3D meshes with bone transformations. Compared to state-of-the-art neural im- plicit representations, our approach generalizes better to unseen poses while preserving accuracy. We demonstrate our method in challenging scenarios on (clothed) 3D hu- mans in diverse and unseen poses.",
        "authors": [
            "Xu Chen",
            "Yufeng Zheng",
            "Michael J Black",
            "Otmar Hilliges",
            "ETH Z urich",
            "T ubingen"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/SNARFDifferentiableForwardSkinningforAnimatingNonRigidNeuralImplicitShapes.pdf"
    },
    {
        "internal_id": 583,
        "title": "CANet: A Context-Aware Network for Shadow Removal",
        "abstract": "In this paper, we propose a novel two-stage context- aware network named CANet for shadow removal, in which the contextual information from non-shadow regions is transferred to shadow regions at the embedded feature spaces. At Stage-I, we propose a contextual patch match- ing (CPM) module to generate a set of potential match- ing pairs of shadow and non-shadow patches. Combined with the potential contextual relationships between shadow and non-shadow regions, our well-designed contextual fea- ture transfer (CFT) mechanism can transfer contextual in- formation from non-shadow to shadow regions at differ- ent scales. With the reconstructed feature maps, we re- move shadows at L and A/B channels separately. At Stage- II, we use an encoder-decoder to refine current results and generate the final shadow removal results. We eval- uate our proposed CANet on two benchmark datasets and some real-world shadow images with complex scenes. Ex- tensive experimental results strongly demonstrate the effi- cacy of our proposed CANet and exhibit superior perfor- mance to state-of-the-arts. Our source code is available at https://github.com/Zipei-Chen/CANet.",
        "authors": [
            "Zipei Chen",
            "Chengjiang Long",
            "Ling Zhang",
            "Chunxia Xiao",
            "Mountain View"
        ],
        "created_time": "",
        "conference": "ICCV",
        "filepath": "/Users/adit/papers/ICCV/CANetAContextAwareNetworkforShadowRemoval.pdf"
    },
    {
        "internal_id": 584,
        "title": "BAG OF TRICKS FOR ADVERSARIAL TRAINING",
        "abstract": "Adversarial training (AT) is one of the most effective strategies for promoting model robustness. However, recent benchmarks show that most of the proposed improvements on AT are less effective than simply early stopping the training pro- cedure. This counter-intuitive fact motivates us to investigate the implementation details of tens of AT methods. Surprisingly, we find that the basic settings (e.g., weight decay, training schedule, etc.) used in these methods are highly inconsistent. In this work, we provide comprehensive evaluations on CIFAR-10, focusing on the effects of mostly overlooked training tricks and hyperparameters for adversarially trained models. Our empirical observations suggest that adversarial robustness is much more sensitive to some basic training settings than we thought. For example, a slightly different value of weight decay can reduce the model robust accuracy by more than 7%, which is probable to override the potential promotion induced by the proposed methods. We conclude a baseline training setting and re-implement previous defenses to achieve new state-of-the-art results1. These facts also appeal to more concerns on the overlooked confounders when benchmarking defenses.",
        "authors": [
            "Tianyu Pang",
            "Xiao Yang",
            "Yinpeng Dong",
            "Hang Su",
            "Jun Zhu",
            "BNRist Center"
        ],
        "created_time": "24 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/BagofTricksforAdversarialTraining.pdf"
    },
    {
        "internal_id": 585,
        "title": "MOPRO: WEBLY SUPERVISED LEARNING WITH MOMENTUM PROTOTYPES",
        "abstract": "We propose a webly-supervised representation learning method that does not suf- fer from the annotation unscalability of supervised learning, nor the computa- tion unscalability of self-supervised learning. Most existing works on webly- supervised representation learning adopt a vanilla supervised learning method without accounting for the prevalent noise in the training data, whereas most prior methods in learning with label noise are less effective for real-world large-scale noisy data. We propose momentum prototypes (MoPro), a simple contrastive learning method that achieves online label noise correction, out-of-distribution sample removal, and representation learning. MoPro achieves state-of-the-art per- formance on WebVision, a weakly-labeled noisy dataset. MoPro also shows su- perior performance when the pretrained model is transferred to down-stream im- age classification and detection tasks. It outperforms the ImageNet supervised pretrained model by +10.5 on 1-shot classification on VOC, and outperforms the best self-supervised pretrained model by +17.3 when finetuned on 1% of ImageNet labeled samples. Furthermore, MoPro is more robust to distribution shifts. Code and pretrained models are available at https://github.com/ salesforce/MoPro.",
        "authors": [
            "Junnan Li",
            "Caiming Xiong"
        ],
        "created_time": "09 February 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/MoProWeblySupervisedLearningwithMomentumPrototypes.pdf"
    },
    {
        "internal_id": 586,
        "title": "DISCRETE GRAPH STRUCTURE LEARNING FOR FORE- CASTING MULTIPLE TIME SERIES",
        "abstract": "Time series forecasting is an extensively studied subject in statistics, economics, and computer science. Exploration of the correlation and causation among the variables in a multivariate time series shows promise in enhancing the perfor- mance of a time series model. When using deep neural networks as forecasting models, we hypothesize that exploiting the pairwise information among multiple (multivariate) time series also improves their forecast. If an explicit graph struc- ture is known, graph neural networks (GNNs) have been demonstrated as powerful tools to exploit the structure. In this work, we propose learning the structure si- multaneously with the GNN if the graph is unknown. We cast the problem as learning a probabilistic graph model through optimizing the mean performance over the graph distribution. The distribution is parameterized by a neural network so that discrete graphs can be sampled differentiably through reparameterization. Empirical evaluations show that our method is simpler, more efficient, and better performing than a recently proposed bilevel learning approach for graph structure learning, as well as a broad array of forecasting models, either deep or non-deep learning based, and graph or non-graph based.",
        "authors": [
            "Chao Shang",
            "Jie Chen",
            "Jinbo Bi"
        ],
        "created_time": "20 April 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/DiscreteGraphStructureLearningforForecastingMultipleTimeSeries.pdf"
    },
    {
        "internal_id": 587,
        "title": "LEARNING VALUE FUNCTIONS IN DEEP POLICY GRA- DIENTS USING RESIDUAL VARIANCE",
        "abstract": "Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action- value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights.",
        "authors": [
            "Yannis Flet Berliac",
            "Univ Lille",
            "Reda Ouhamma",
            "Univ Lille",
            "Odalric Ambrym Maillard",
            "Philippe Preux",
            "Univ Lille"
        ],
        "created_time": "15 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/LearningValueFunctionsinDeepPolicyGradientsusingResidualVariance.pdf"
    },
    {
        "internal_id": 588,
        "title": "REPRESENTATION LEARNING FOR IMPROVED INTER- PRETABILITY AND CLASSIFICATION ACCURACY OF CLINICAL FACTORS FROM EEG",
        "abstract": "Despite extensive standardization, diagnostic interviews for mental health disorders encompass substantial subjective judgment. Previous studies have demonstrated that EEG-based neural measures can function as reliable objective correlates of depression, or even predictors of depression and its course. However, their clinical utility has not been fully realized because of 1) the lack of automated ways to deal with the inherent noise associated with EEG data at scale, and 2) the lack of knowledge of which aspects of the EEG signal may be markers of a clinical disorder. Here we adapt an unsupervised pipeline from the recent deep representation learning literature to address these problems by 1) learning a disentangled representation using \u03b2-VAE to denoise the signal, and 2) extracting interpretable features associated with a sparse set of clinical labels using a Symbol\u2013Concept Association Network (SCAN). We demonstrate that our method is able to outperform the canonical baseline classification method on a number of factors, including participant age and depression diagnosis. Furthermore, our method recovers a representation that can be used to automatically extract denoised Event Related Potentials (ERPs) from novel, single EEG trajectories, and supports fast supervised re-mapping to various clinical labels, allowing clinicians to re-use a single EEG representation regardless of updates to the standardized diagnostic system. Finally, single factors of the learned disentangled representations often correspond to meaningful markers of clinical factors, as automatically detected by SCAN, allowing for human interpretability and post-hoc expert analysis of the recommendations made by the model.",
        "authors": [
            "Garrett Honke",
            "the Moonshot Factory",
            "Mountain View",
            "Irina Higgins",
            "Nina Thigpen",
            "the Moonshot Factory",
            "Mountain View",
            "Vladimir Miskovic",
            "the Moonshot Factory",
            "Mountain View"
        ],
        "created_time": "16 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/RepresentationlearningforimprovedinterpretabilityandclassificationaccuracyofclinicalfactorsfromEEG.pdf"
    },
    {
        "internal_id": 589,
        "title": "GSHARD: SCALING GIANT MODELS WITH CONDI- TIONAL COMPUTATION AND AUTOMATIC SHARDING",
        "abstract": "Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. In this paper we demonstrate conditional computation as a remedy to the above mentioned impediments, and demonstrate its efficacy and utility. We make extensive use of GShard, a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler to enable large scale models with up to trillions of parameters. GShard and conditional computation enable us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of- Experts. We demonstrate that such a giant model with 600 billion parameters can efficiently be trained on 2048 TPU v3 cores in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art.",
        "authors": [
            "Dmitry Lepikhin",
            "Yuanzhong Xu",
            "Dehao Chen",
            "Orhan Firat",
            "Yanping Huang",
            "Maxim Krikun",
            "Noam Shazeer",
            "Zhifeng Chen"
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/GShardScalingGiantModelswithConditionalComputationandAutomaticSharding.pdf"
    },
    {
        "internal_id": 590,
        "title": "BRECQ: PUSHING THE LIMIT OF POST-TRAINING QUANTIZATION BY BLOCK RECONSTRUCTION",
        "abstract": "We study the challenging task of neural network quantization without end-to- end retraining, called Post-training Quantization (PTQ). PTQ usually requires a small subset of training data but produces less powerful quantized models than Quantization-Aware Training (QAT). In this work, we propose a novel PTQ frame- work, dubbed BRECQ, which pushes the limits of bitwidth in PTQ down to INT2 for the first time. BRECQ leverages the basic building blocks in neural networks and reconstructs them one-by-one. In a comprehensive theoretical study of the second-order error, we show that BRECQ achieves a good balance between cross- layer dependency and generalization error. To further employ the power of quan- tization, the mixed precision technique is incorporated in our framework by ap- proximating the inter-layer and intra-layer sensitivity. Extensive experiments on various handcrafted and searched neural architectures are conducted for both im- age classification and object detection tasks. And for the first time we prove that, without bells and whistles, PTQ can attain 4-bit ResNet and MobileNetV2 com- parable with QAT and enjoy 240\u00d7 faster production of quantized models. Codes are available at https://github.com/yhhhli/BRECQ.",
        "authors": [
            "Yuhang Li",
            "Ruihao Gong",
            "Xu Tan",
            "Yang Yang",
            "Peng Hu",
            "Qi Zhang",
            "Fengwei Yu",
            "Wei Wang",
            "Shi Gu"
        ],
        "created_time": "28 February 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/BRECQPushingtheLimitofPostTrainingQuantizationbyBlockReconstruction.pdf"
    },
    {
        "internal_id": 591,
        "title": "PSEUDOSEG: DESIGNING PSEUDO LABELS FOR SEMANTIC SEGMENTATION",
        "abstract": "Recent advances in semi-supervised learning (SSL) demonstrate that a combi- nation of consistency regularization and pseudo-labeling can effectively improve image classification accuracy in the low-data regime. Compared to classifica- tion, semantic segmentation tasks require much more intensive labeling costs. Thus, these tasks greatly benefit from data-efficient training methods. However, structured outputs in segmentation render particular difficulties (e.g., designing pseudo-labeling and augmentation) to apply existing SSL strategies. To address this problem, we present a simple and novel re-design of pseudo-labeling to gener- ate well-calibrated structured pseudo labels for training with unlabeled or weakly- labeled data. Our proposed pseudo-labeling strategy is network structure agnos- tic to apply in a one-stage consistency training framework. We demonstrate the effectiveness of the proposed pseudo-labeling strategy in both low-data and high- data regimes. Extensive experiments have validated that pseudo labels generated from wisely fusing diverse sources and strong data augmentation are crucial to consistency training for semantic segmentation. The source code is available at https://github.com/googleinterns/wss.",
        "authors": [
            "Han Zhang",
            "Chun Liang Li",
            "Xiao Bian",
            "Jia Bin Huang",
            "Tomas P ster"
        ],
        "created_time": "15 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/PseudoSegDesigningPseudoLabelsforSemanticSegmentation.pdf"
    },
    {
        "internal_id": 592,
        "title": "HETEROFL: COMPUTATION AND COMMUNICATION EFFICIENT FEDERATED LEARNING FOR HETEROGE- NEOUS CLIENTS",
        "abstract": "Federated Learning (FL) is a method of training machine learning models on pri- vate data distributed over a large number of possibly heterogeneous clients such as mobile phones and IoT devices. In this work, we propose a new federated learning framework named HeteroFL to address heterogeneous clients equipped with very different computation and communication capabilities. Our solution can enable the training of heterogeneous local models with varying computation com- plexities and still produce a single global inference model. For the first time, our method challenges the underlying assumption of existing work that local models have to share the same architecture as the global model. We demonstrate several strategies to enhance FL training and conduct extensive empirical evaluations, in- cluding five computation complexity levels of three model architecture on three datasets. We show that adaptively distributing subnetworks according to clients' capabilities is both computation and communication efficient.",
        "authors": [
            "Enmao Diao",
            "NC ",
            "Jie Ding",
            "MN ",
            "Vahid Tarokh",
            "NC "
        ],
        "created_time": "19 February 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/HeteroFLComputationandCommunicationEfficientFederatedLearningforHeterogeneousClients.pdf"
    },
    {
        "internal_id": 593,
        "title": "VTNET: VISUAL TRANSFORMER NETWORK FOR OBJECT GOAL NAVIGATION",
        "abstract": "Object goal navigation aims to steer an agent towards a target object based on observations of the agent. It is of pivotal importance to design effective visual representations of the observed scene in determining navigation actions. In this paper, we introduce a Visual Transformer Network (VTNet) for learning infor- mative visual representation in navigation. VTNet is a highly effective structure that embodies two key properties for visual representations: First, the relation- ships among all the object instances in a scene are exploited; Second, the spatial locations of objects and image regions are emphasized so that directional naviga- tion signals can be learned. Furthermore, we also develop a pre-training scheme to associate the visual representations with navigation signals, and thus facilitate navigation policy learning. In a nutshell, VTNet embeds object and region fea- tures with their location cues as spatial-aware descriptors and then incorporates all the encoded descriptors through attention operations to achieve informative representation for navigation. Given such visual representations, agents are able to explore the correlations between visual observations and navigation actions. For example, an agent would prioritize \"turning right\" over \"turning left\" when the visual representation emphasizes on the right side of activation map. Experi- ments in the artificial environment AI2-Thor demonstrate that VTNet significantly outperforms state-of-the-art methods in unseen testing environments.",
        "authors": [
            "Heming Du",
            "CSIRO DATA"
        ],
        "created_time": "16 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/VTNetVisualTransformerNetworkforObjectGoalNavigation.pdf"
    },
    {
        "internal_id": 594,
        "title": "SLICED KERNELIZED STEIN DISCREPANCY",
        "abstract": "Kernelized Stein discrepancy (KSD), though being extensively used in goodness-of- fit tests and model learning, suffers from the curse-of-dimensionality. We address this issue by proposing the sliced Stein discrepancy and its scalable and kernel- ized variants, which employ kernel-based test functions defined on the optimal one-dimensional projections. When applied to goodness-of-fit tests, extensive experiments show the proposed discrepancy significantly outperforms KSD and various baselines in high dimensions. For model learning, we show its advan- tages over existing Stein discrepancy baselines by training independent component analysis models with different discrepancies. We further propose a novel particle inference method called sliced Stein variational gradient descent (S-SVGD) which alleviates the mode-collapse issue of SVGD in training variational autoencoders.",
        "authors": [
            "Wenbo Gong",
            "Yingzhen Li"
        ],
        "created_time": "",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/SlicedKernelizedSteinDiscrepancy.pdf"
    },
    {
        "internal_id": 595,
        "title": "SHAPLEY EXPLANATION NETWORKS",
        "abstract": "Shapley values have become one of the most popular feature attribution explanation methods. However, most prior work has focused on post-hoc Shapley explanations, which can be computationally demanding due to its exponential time complexity and preclude model regularization based on Shapley explanations during training. Thus, we propose to incorporate Shapley values themselves as latent representa- tions in deep models\u2014thereby making Shapley explanations first-class citizens in the modeling paradigm. This intrinsic explanation approach enables layer-wise explanations, explanation regularization of the model during training, and fast expla- nation computation at test time. We define the Shapley transform that transforms the input into a Shapley representation given a specific function. We operationalize the Shapley transform as a neural network module and construct both shallow and deep networks, called SHAPNETs, by composing Shapley modules. We prove that our Shallow SHAPNETs compute the exact Shapley values and our Deep SHAPNETs maintain the missingness and accuracy properties of Shapley values. We demon- strate on synthetic and real-world datasets that our SHAPNETs enable layer-wise Shapley explanations, novel Shapley regularizations during training, and fast com- putation while maintaining reasonable performance. Code is available at https: //github.com/inouye-lab/ShapleyExplanationNetworks.",
        "authors": [
            "Rui Wang",
            "Xiaoqian Wang",
            "David I Inouye",
            "West Lafayette",
            "IN "
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/ShapleyExplanationNetworks.pdf"
    },
    {
        "internal_id": 596,
        "title": "DEBIASING CONCEPT-BASED EXPLANATIONS WITH CAUSAL ANALYSIS",
        "abstract": "Concept-based explanation approach is a popular model interpertability tool be- cause it expresses the reasons for a model's predictions in terms of concepts that are meaningful for the domain experts. In this work, we study the problem of the concepts being correlated with confounding information in the features. We propose a new causal prior graph for modeling the impacts of unobserved variables and a method to remove the impact of confounding information and noise using a two-stage regression technique borrowed from the instrumental variable literature. We also model the completeness of the concepts set and show that our debiasing method works when the concepts are not complete. Our synthetic and real-world experiments demonstrate the success of our method in removing biases and im- proving the ranking of the concepts in terms of their contribution to the explanation of the predictions.",
        "authors": [
            "Mohammad Taha Bahadori",
            "David E Heckerman"
        ],
        "created_time": "04 February 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/DebiasingConceptbasedExplanationswithCausalAnalysis.pdf"
    },
    {
        "internal_id": 597,
        "title": "UNIVERSAL WEAKLY SUPERVISED SEGMENTATION BY PIXEL-TO-SEGMENT CONTRASTIVE LEARNING",
        "abstract": "Weakly supervised segmentation requires assigning a label to every pixel based on training instances with partial annotations such as image-level tags, object bound- ing boxes, labeled points and scribbles. This task is challenging, as coarse an- notations (tags, boxes) lack precise pixel localization whereas sparse annotations (points, scribbles) lack broad region coverage. Existing methods tackle these two types of weak supervision differently: Class activation maps are used to localize coarse labels and iteratively refine the segmentation model, whereas conditional random fields are used to propagate sparse labels to the entire image. We formulate weakly supervised segmentation as a semi-supervised metric learn- ing problem, where pixels of the same (different) semantics need to be mapped to the same (distinctive) features. We propose 4 types of contrastive relationships between pixels and segments in the feature space, capturing low-level image simi- larity, semantic annotation, co-occurrence, and feature affinity. They act as priors; the pixel-wise feature can be learned from training images with any partial anno- tations in a data-driven fashion. In particular, unlabeled pixels in training images participate not only in data-driven grouping within each image, but also in discrim- inative feature learning within and across images. We deliver a universal weakly supervised segmenter with significant gains on Pascal VOC and DensePose. Our code is publicly available at https://github.com/twke18/SPML.",
        "authors": [
            "Tsung Wei Ke",
            "Jyh Jing Hwang",
            "Stella X Yu"
        ],
        "created_time": "10 May 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/UniversalWeaklySupervisedSegmentationbyPixeltoSegmentContrastiveLearning.pdf"
    },
    {
        "internal_id": 598,
        "title": "GRAPH INFORMATION BOTTLENECK FOR SUBGRAPH RECOGNITION",
        "abstract": "Given the input graph and its label/property, several key problems of graph learn- ing, such as finding interpretable subgraphs, graph denoising and graph compres- sion, can be attributed to the fundamental problem of recognizing a subgraph of the original one. This subgraph shall be as informative as possible, yet contains less redundant and noisy structure. This problem setting is closely related to the well-known information bottleneck (IB) principle, which, however, has less been studied for the irregular graph data and graph neural networks (GNNs). In this paper, we propose a framework of Graph Information Bottleneck (GIB) for the subgraph recognition problem in deep graph learning. Under this framework, one can recognize the maximally informative yet compressive subgraph, named IB- subgraph. However, the GIB objective is notoriously hard to optimize, mostly due to the intractability of the mutual information of irregular graph data and the unsta- ble optimization process. In order to tackle these challenges, we propose: i) a GIB objective based-on a mutual information estimator for the irregular graph data; ii) a bi-level optimization scheme to maximize the GIB objective; iii) a connectiv- ity loss to stabilize the optimization process. We evaluate the properties of the IB-subgraph in three application scenarios: improvement of graph classification, graph interpretation and graph denoising. Extensive experiments demonstrate that the information-theoretic IB-subgraph enjoys superior graph properties.",
        "authors": [
            "Junchi Yu",
            "Tingyang Xu",
            "Yu Rong",
            "Yatao Bian",
            "Junzhou Huang",
            "Ran He",
            "NLPR CRIPAC"
        ],
        "created_time": "16 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/GraphInformationBottleneckforSubgraphRecognition.pdf"
    },
    {
        "internal_id": 599,
        "title": "SELF-SUPERVISED REPRESENTATION LEARNING WITH RELATIVE PREDICTIVE CODING",
        "abstract": "This paper introduces Relative Predictive Coding (RPC), a new contrastive repre- sentation learning objective that maintains a good balance among training stabil- ity, minibatch size sensitivity, and downstream task performance. The key to the success of RPC is two-fold. First, RPC introduces the relative parameters to reg- ularize the objective for boundedness and low variance. Second, RPC contains no logarithm and exponential score functions, which are the main cause of training instability in prior contrastive objectives. We empirically verify the effectiveness of RPC on benchmark vision and speech self-supervised learning tasks. Lastly, we relate RPC with mutual information (MI) estimation, showing RPC can be used to estimate MI with low variance 1.",
        "authors": [
            "Martin Q Ma",
            "Muqiao Yang",
            "Han Zhao",
            "Louis Philippe Morency",
            "Ruslan Salakhutdinov"
        ],
        "created_time": "12 April 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/SelfsupervisedRepresentationLearningwithRelativePredictiveCoding.pdf"
    },
    {
        "internal_id": 600,
        "title": "GRAPH EDIT NETWORKS",
        "abstract": "While graph neural networks have made impressive progress in classification and regression, few approaches to date perform time series prediction on graphs, and those that do are mostly limited to edge changes. We suggest that graph edits are a more natural interface for graph-to-graph learning. In particular, graph edits are general enough to describe any graph-to-graph change, not only edge changes; they are sparse, making them easier to understand for humans and more efficient computationally; and they are local, avoiding the need for pooling layers in graph neural networks. In this paper, we propose a novel output layer - the graph edit network - which takes node embeddings as input and generates a sequence of graph edits that transform the input graph to the output graph. We prove that a mapping between the node sets of two graphs is sufficient to construct training data for a graph edit network and that an optimal mapping yields edit scripts that are almost as short as the graph edit distance between the graphs. We further provide a proof-of-concept empirical evaluation on several graph dynamical systems, which are difficult to learn for baselines from the literature.",
        "authors": [
            "Benjamin Paassen",
            "Daniele Grattarola",
            "Daniele Zambon",
            "Cesare Alippi",
            "Politecnico di Milano",
            "Barbara Hammer"
        ],
        "created_time": "14 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/GraphEditNetworks.pdf"
    },
    {
        "internal_id": 601,
        "title": "ENJOY YOUR EDITING: CONTROLLABLE GANS FOR IMAGE EDITING VIA LATENT SPACE NAVIGATION",
        "abstract": "Controllable semantic image editing enables a user to change entire image at- tributes with a few clicks, e.g., gradually making a summer scene look like it was taken in winter. Classic approaches for this task use a Generative Adversarial Net (GAN) to learn a latent space and suitable latent-space transformations. However, current approaches often suffer from attribute edits that are entangled, global im- age identity changes, and diminished photo-realism. To address these concerns, we learn multiple attribute transformations simultaneously, integrate attribute re- gression into the training of transformation functions, and apply a content loss and an adversarial loss that encourages the maintenance of image identity and photo-realism. We propose quantitative evaluation strategies for measuring con- trollable editing performance, unlike prior work, which primarily focuses on qual- itative evaluation. Our model permits better control for both single- and multiple- attribute editing while preserving image identity and realism during transforma- tion. We provide empirical results for both natural and synthetic images, high- lighting that our model achieves state-of-the-art performance for targeted image manipulation.",
        "authors": [
            "Peiye Zhuang",
            "Oluwasanmi Koyejo"
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/EnjoyYourEditingControllableGANsforImageEditingviaLatentSpaceNavigation.pdf"
    },
    {
        "internal_id": 603,
        "title": "MULTIPLICATIVE FILTER NETWORKS",
        "abstract": "Although deep networks are typically used to approximate functions over high dimensional inputs, recent work has increased interest in neural networks as func- tion approximators for low-dimensional-but-complex functions, such as represent- ing images as a function of pixel coordinates, solving differential equations, or representing signed distance functions or neural radiance fields. Key to these re- cent successes has been the use of new elements such as sinusoidal nonlineari- ties or Fourier features in positional encodings, which vastly outperform simple ReLU networks. In this paper, we propose and empirically demonstrate that an arguably simpler class of function approximators can work just as well for such problems: multiplicative filter networks. In these networks, we avoid traditional compositional depth altogether, and simply multiply together (linear functions of) sinusoidal or Gabor wavelet functions applied to the input. This representation has the notable advantage that the entire function can simply be viewed as a linear function approximator over an exponential number of Fourier or Gabor basis func- tions, respectively. Despite this simplicity, when compared to recent approaches that use Fourier features with ReLU networks or sinusoidal activation networks, we show that these multiplicative filter networks largely outperform or match the performance of these approaches on the domains highlighted in these past works.",
        "authors": [
            "Rizal Fathony",
            "Anit Kumar Sahu",
            "Devin Willmott",
            "J Zico Kolter"
        ],
        "created_time": "18 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/MultiplicativeFilterNetworks.pdf"
    },
    {
        "internal_id": 604,
        "title": "DICE: DIVERSITY IN DEEP ENSEMBLES VIA CONDI- TIONAL REDUNDANCY ADVERSARIAL ESTIMATION",
        "abstract": "Deep ensembles perform better than a single network thanks to the diversity among their members. Recent approaches regularize predictions to increase diver- sity; however, they also drastically decrease individual members' performances. In this paper, we argue that learning strategies for deep ensembles need to tackle the trade-off between ensemble diversity and individual accuracies. Motivated by arguments from information theory and leveraging recent advances in neural esti- mation of conditional mutual information, we introduce a novel training criterion called DICE: it increases diversity by reducing spurious correlations among fea- tures. The main idea is that features extracted from pairs of members should only share information useful for target class prediction without being conditionally redundant. Therefore, besides the classification loss with information bottleneck, we adversarially prevent features from being conditionally predictable from each other. We manage to reduce simultaneous errors while protecting class informa- tion. We obtain state-of-the-art accuracy results on CIFAR-10/100: for example, an ensemble of 5 networks trained with DICE matches an ensemble of 7 networks trained independently. We further analyze the consequences on calibration, uncer- tainty estimation, out-of-distribution detection and online co-distillation.",
        "authors": [
            "Sorbonne Universit e",
            "Matthieu Cord"
        ],
        "created_time": "03 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/DICEDiversityinDeepEnsemblesviaConditionalRedundancyAdversarialEstimation.pdf"
    },
    {
        "internal_id": 605,
        "title": "NEURAL ATTENTION DISTILLATION: ERASING BACK- DOOR TRIGGERS FROM DEEP NEURAL NETWORKS",
        "abstract": "Deep neural networks (DNNs) are known vulnerable to backdoor attacks, a train- ing time attack that injects a trigger pattern into a small proportion of training data so as to control the model's prediction at the test time. Backdoor attacks are no- tably dangerous since they do not affect the model's performance on clean exam- ples, yet can fool the model to make incorrect prediction whenever the trigger pat- tern appears during testing. In this paper, we propose a novel defense framework Neural Attention Distillation (NAD) to erase backdoor triggers from backdoored DNNs. NAD utilizes a teacher network to guide the finetuning of the backdoored student network on a small clean subset of data such that the intermediate-layer at- tention of the student network aligns with that of the teacher network. The teacher network can be obtained by an independent finetuning process on the same clean subset. We empirically show, against 6 state-of-the-art backdoor attacks, NAD can effectively erase the backdoor triggers using only 5% clean training data without causing obvious performance degradation on clean examples. Our code is avail- able at https://github.com/bboylyg/NAD.",
        "authors": [
            "Yige Li",
            "Lingjuan Lyu",
            "Bo Li",
            "Xingjun Ma",
            "Ant Group"
        ],
        "created_time": "19 February 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/NeuralAttentionDistillationErasingBackdoorTriggersfromDeepNeuralNetworks.pdf"
    },
    {
        "internal_id": 606,
        "title": "PROPERTY CONTROLLABLE VARIATIONAL AUTOEN- CODER VIA INVERTIBLE MUTUAL DEPENDENCE",
        "abstract": "Deep generative models have made important progress towards modeling com- plex, high dimensional data. Their usefulness is nevertheless often limited by a lack of control over the generative process or a poor understanding of the latent representation. To overcome these issues, attention is now focused on discovering latent variables correlated to the data properties and manipulating these proper- ties. This paper presents the Property-controllable VAE (PCVAE), where a new Bayesian model is proposed to inductively bias the latent representation using explicit data properties via novel group-wise and property-wise disentanglement terms. Each data property corresponds seamlessly to a latent variable, by enforc- ing invertible mutual dependence between them. This allows us to move along the learned latent dimensions to control specific properties of the generated data with great precision. Quantitative and qualitative evaluations confirm that the PCVAE outperforms the existing models by up to 28% in capturing and 65% in manip- ulating the desired properties. The code for the proposed PCVAE is available at:https://github.com/xguo7/PCVAE.",
        "authors": [
            "Xiaojie Guo",
            "VA ",
            "Yuanqi Du",
            "VA ",
            "Liang Zhao",
            "GA "
        ],
        "created_time": "18 February 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/PropertyControllableVariationalAutoencoderviaInvertibleMutualDependence.pdf"
    },
    {
        "internal_id": 608,
        "title": "SIMPLE SPECTRAL GRAPH CONVOLUTION",
        "abstract": "Graph Convolutional Networks (GCNs) are leading methods for learning graph representations. However, without specially designed architectures, the perfor- mance of GCNs degrades quickly with increased depth. As the aggregated neigh- borhood size and neural network depth are two completely orthogonal aspects of graph representation, several methods focus on summarizing the neighborhood by aggregating K-hop neighborhoods of nodes while using shallow neural networks. However, these methods still encounter oversmoothing, and suffer from high com- putation and storage costs. In this paper, we use a modified Markov Diffusion Ker- nel to derive a variant of GCN called Simple Spectral Graph Convolution (S2GC). Our spectral analysis shows that our simple spectral graph convolution used in S2GC is a trade-off of low- and high-pass filter bands which capture the global and local contexts of each node. We provide two theoretical claims which demon- strate that we can aggregate over a sequence of increasingly larger neighborhoods compared to competitors while limiting severe oversmoothing. Our experimen- tal evaluations show that S2GC with a linear learner is competitive in text and node classification tasks. Moreover, S2GC is comparable to other state-of-the-art methods for node clustering and community prediction tasks.",
        "authors": [
            "Hao Zhu",
            "Piotr Koniusz",
            "Data CSIRO"
        ],
        "created_time": "27 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/SimpleSpectralGraphConvolution.pdf"
    },
    {
        "internal_id": 609,
        "title": "QPLEX: DUPLEX DUELING MULTI-AGENT Q-LEARNING",
        "abstract": "We explore value-based multi-agent reinforcement learning (MARL) in the popular paradigm of centralized training with decentralized execution (CTDE). CTDE has an important concept, Individual-Global-Max (IGM) principle, which requires the consistency between joint and local action selections to support efficient local decision-making. However, in order to achieve scalability, existing MARL methods either limit representation expressiveness of their value function classes or relax the IGM consistency, which may suffer from instability risk or may not perform well in complex domains. This paper presents a novel MARL approach, called duPLEX dueling multi-agent Q-learning (QPLEX), which takes a duplex dueling network architecture to factorize the joint value function. This duplex dueling structure encodes the IGM principle into the neural network architecture and thus enables efficient value function learning. Theoretical analysis shows that QPLEX achieves a complete IGM function class. Empirical experiments on StarCraft II micromanagement tasks demonstrate that QPLEX significantly outperforms state- of-the-art baselines in both online and offline data collection settings, and also reveal that QPLEX achieves high sample efficiency and can benefit from offline datasets without additional online exploration1.",
        "authors": [
            "Jianhao Wang ",
            "Zhizhou Ren ",
            "Terry Liu",
            "Yang Yu",
            "Chongjie Zhang"
        ],
        "created_time": "21 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/QPLEXDuplexDuelingMultiAgentQLearning.pdf"
    },
    {
        "internal_id": 610,
        "title": "CPR: CLASSIFIER-PROJECTION REGULARIZATION FOR CONTINUAL LEARNING",
        "abstract": "We propose a general, yet simple patch that can be applied to existing regularization- based continual learning methods called classifier-projection regularization (CPR). Inspired by both recent results on neural networks with wide local minima and information theory, CPR adds an additional regularization term that maximizes the entropy of a classifier's output probability. We demonstrate that this additional term can be interpreted as a projection of the conditional probability given by a classifier's output to the uniform distribution. By applying the Pythagorean theorem for KL divergence, we then prove that this projection may (in theory) improve the performance of continual learning methods. In our extensive experimental results, we apply CPR to several state-of-the-art regularization-based continual learning methods and benchmark performance on popular image recognition datasets. Our results demonstrate that CPR indeed promotes a wide local minima and signifi- cantly improves both accuracy and plasticity while simultaneously mitigating the catastrophic forgetting of baseline continual learning methods. The codes and scripts for this work are available at https://github.com/csm9493/CPR_CL.",
        "authors": [
            "Sungmin Cha",
            "Hsiang Hsu",
            "Taebaek Hwang",
            "Flavio P Calmon"
        ],
        "created_time": "21 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/CPRClassifierProjectionRegularizationforContinualLearning.pdf"
    },
    {
        "internal_id": 611,
        "title": "REDUCING THE COMPUTATIONAL COST OF DEEP GENERATIVE MODELS WITH BINARY NEURAL NETWORKS",
        "abstract": "Deep generative models provide a powerful set of tools to understand real-world data. But as these models improve, they increase in size and complexity, so their computational cost in memory and execution time grows. Using binary weights in neural networks is one method which has shown promise in reducing this cost. However, whether binary neural networks can be used in generative models is an open problem. In this work we show, for the first time, that we can successfully train generative models which utilize binary neural networks. This reduces the computational cost of the models massively. We develop a new class of binary weight normalization, and provide insights for architecture designs of these binarized generative models. We demonstrate that two state-of-the-art deep generative models, the ResNet VAE and Flow++ models, can be binarized effectively using these techniques. We train binary models that achieve loss values close to those of the regular models but are 90%-94% smaller in size, and also allow significant speed-ups in execution time.",
        "authors": [
            "Thomas Bird"
        ],
        "created_time": "",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/ReducingtheComputationalCostofDeepGenerativeModelswithBinaryNeuralNetworks.pdf"
    },
    {
        "internal_id": 612,
        "title": "HIERARCHICAL REINFORCEMENT LEARNING BY DISCOVERING INTRINSIC OPTIONS",
        "abstract": "We propose a hierarchical reinforcement learning method, HIDIO, that can learn task-agnostic options in a self-supervised manner while jointly learning to utilize them to solve sparse-reward tasks. Unlike current hierarchical RL approaches that tend to formulate goal-reaching low-level tasks or pre-define ad hoc lower- level policies, HIDIO encourages lower-level option learning that is independent of the task at hand, requiring few assumptions or little knowledge about the task structure. These options are learned through an intrinsic entropy minimization ob- jective conditioned on the option sub-trajectories. The learned options are diverse and task-agnostic. In experiments on sparse-reward robotic manipulation and nav- igation tasks, HIDIO achieves higher success rates with greater sample efficiency than regular RL baselines and two state-of-the-art hierarchical RL methods. Code available at https://www.github.com/jesbu1/hidio.",
        "authors": [
            "Jesse Zhang ",
            "Haonan Yu ",
            "Wei Xu",
            "Horizon Robotics"
        ],
        "created_time": "11 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/HierarchicalReinforcementLearningbyDiscoveringIntrinsicOptions.pdf"
    },
    {
        "internal_id": 614,
        "title": "FAIR MIXUP: FAIRNESS VIA INTERPOLATION",
        "abstract": "Training classifiers under fairness constraints such as group fairness, regularizes the disparities of predictions between the groups. Nevertheless, even though the constraints are satisfied during training, they might not generalize at evaluation time. To improve the generalizability of fair classifiers, we propose fair mixup, a new data augmentation strategy for imposing the fairness constraint. In partic- ular, we show that fairness can be achieved by regularizing the models on paths of interpolated samples between the groups. We use mixup, a powerful data aug- mentation strategy to generate these interpolates. We analyze fair mixup and em- pirically show that it ensures a better generalization for both accuracy and fairness measurement in tabular, vision, and language benchmarks. The code is available at https://github.com/chingyaoc/fair-mixup.",
        "authors": [
            "Ching Yao Chuang",
            "Youssef Mroueh"
        ],
        "created_time": "10 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/FairMixupFairnessviaInterpolation.pdf"
    },
    {
        "internal_id": 615,
        "title": "MODEL-BASED MICRO-DATA REINFORCEMENT LEARN- ING: WHAT ARE THE CRUCIAL MODEL PROPERTIES AND WHICH MODEL TO CHOOSE?",
        "abstract": "We contribute to micro-data model-based reinforcement learning (MBRL) by rig- orously comparing popular generative models using a fixed (random shooting) control agent. We find that on an environment that requires multimodal posterior predictives, mixture density nets outperform all other models by a large margin. When multimodality is not required, our surprising finding is that we do not need probabilistic posterior predictives: deterministic models are on par, in fact they consistently (although non-significantly) outperform their probabilistic counter- parts. We also found that heteroscedasticity at training time, perhaps acting as a regularizer, improves predictions at longer horizons. At the methodological side, we design metrics and an experimental protocol which can be used to evaluate the various models, predicting their asymptotic performance when using them on the control problem. Using this framework, we improve the state-of-the-art sample complexity of MBRL on Acrobot by two to four folds, using an aggressive training schedule which is outside of the hyperparameter interval usually considered.",
        "authors": [
            "Gabriel Hurtado",
            "Albert Thomas"
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/Modelbasedmicrodatareinforcementlearningwhatarethecrucialmodelpropertiesandwhichmodeltochoose.pdf"
    },
    {
        "internal_id": 616,
        "title": "COLLECTIVE ROBUSTNESS CERTIFICATES: EXPLOITING INTERDEPENDENCE IN GRAPH NEURAL NETWORKS",
        "abstract": "In tasks like node classification, image segmentation, and named-entity recogni- tion we have a classifier that simultaneously outputs multiple predictions (a vector of labels) based on a single input, i.e. a single graph, image, or document respec- tively. Existing adversarial robustness certificates consider each prediction inde- pendently and are thus overly pessimistic for such tasks. They implicitly assume that an adversary can use different perturbed inputs to attack different predictions, ignoring the fact that we have a single shared input. We propose the first col- lective robustness certificate which computes the number of predictions that are simultaneously guaranteed to remain stable under perturbation, i.e. cannot be at- tacked. We focus on Graph Neural Networks and leverage their locality property \u2013 perturbations only affect the predictions in a close neighborhood \u2013 to fuse mul- tiple single-node certificates into a drastically stronger collective certificate. For example, on the Citeseer dataset our collective certificate for node classification increases the average number of certifiable feature perturbations from 7 to 351.",
        "authors": [
            "Jan Schuchardt"
        ],
        "created_time": "18 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/CollectiveRobustnessCertificatesExploitingInterdependenceinGraphNeuralNetworks.pdf"
    },
    {
        "internal_id": 617,
        "title": "A DIFFUSION THEORY FOR DEEP LEARNING DYNAM- ICS: STOCHASTIC GRADIENT DESCENT EXPONEN- TIALLY FAVORS FLAT MINIMA",
        "abstract": "Stochastic Gradient Descent (SGD) and its variants are mainstream methods for training deep networks in practice. SGD is known to find a flat minimum that often generalizes well. However, it is mathematically unclear how deep learning can select a flat minimum among so many minima. To answer the question quantitatively, we develop a density diffusion theory to reveal how minima selection quantitatively depends on the minima sharpness and the hyperparameters. To the best of our knowledge, we are the first to theoretically and empirically prove that, benefited from the Hessian-dependent covariance of stochastic gradient noise, SGD favors flat minima exponentially more than sharp minima, while Gradient Descent (GD) with injected white noise favors flat minima only polynomially more than sharp minima. We also reveal that either a small learning rate or large-batch training requires exponentially many iterations to escape from minima in terms of the ratio of the batch size and learning rate. Thus, large-batch training cannot search flat minima efficiently in a realistic computational time.",
        "authors": [
            "Zeke Xie",
            "Issei Sato "
        ],
        "created_time": "20 April 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/ADiffusionTheoryForDeepLearningDynamicsStochasticGradientDescentExponentiallyFavorsFlatMinima.pdf"
    },
    {
        "internal_id": 618,
        "title": "TRAINING BATCHNORM AND ONLY BATCHNORM: ON THE EXPRESSIVE POWER OF RANDOM FEATURES IN CNNS",
        "abstract": "A wide variety of deep learning techniques from style transfer to multitask learning rely on training affine transformations of features. Most prominent among these is the popular feature normalization technique BatchNorm, which normalizes activations and then subsequently applies a learned affine transform. In this paper, we aim to understand the role and expressive power of affine parameters used to transform features in this way. To isolate the contribution of these parameters from that of the learned features they transform, we investigate the performance achieved when training only these parameters in BatchNorm and freezing all weights at their random initializations. Doing so leads to surprisingly high performance considering the significant limitations that this style of training imposes. For example, sufficiently deep ResNets reach 82% (CIFAR-10) and 32% (ImageNet, top-5) accuracy in this configuration, far higher than when training an equivalent number of randomly chosen parameters elsewhere in the network. BatchNorm achieves this performance in part by naturally learning to disable around a third of the random features. Not only do these results highlight the expressive power of affine parameters in deep learning, but\u2014in a broader sense\u2014they characterize the expressive power of neural networks constructed simply by shifting and rescaling random features.",
        "authors": [
            "Jonathan Frankle",
            "MIT CSAIL",
            "David J Schwab",
            "CUNY Graduate Center",
            "Ari S Morcos"
        ],
        "created_time": "08 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/TrainingBatchNormandOnlyBatchNormOntheExpressivePowerofRandomFeaturesinCNNs.pdf"
    },
    {
        "internal_id": 619,
        "title": "STOCHASTIC SECURITY: ADVERSARIAL DEFENSE USING LONG-RUN DYNAMICS OF ENERGY-BASED MODELS",
        "abstract": "The vulnerability of deep networks to adversarial attacks is a central problem for deep learning from the perspective of both cognition and security. The current most successful defense method is to train a classifier using adversarial images created during learning. Another defense approach involves transformation or purification of the original input to remove adversarial signals before the image is classified. We focus on defending naturally-trained classifiers using Markov Chain Monte Carlo (MCMC) sampling with an Energy-Based Model (EBM) for adversarial purification. In contrast to adversarial training, our approach is intended to secure highly vulnerable pre-existing classifiers. To our knowledge, no prior defensive transformation is capable of securing naturally-trained classifiers, and our method is the first to validate a post-training defense approach that is distinct from current successful defenses which modify classifier training. The memoryless behavior of long-run MCMC sampling will eventually remove adversarial signals, while metastable behavior preserves consistent appearance of MCMC samples after many steps to allow accurate long-run prediction. Bal- ancing these factors can lead to effective purification and robust classification. We evaluate adversarial defense with an EBM using the strongest known at- tacks against purification. Our contributions are 1) an improved method for training EBM's with realistic long-run MCMC samples for effective purifica- tion, 2) an Expectation-Over-Transformation (EOT) defense that resolves ambi- guities for evaluating stochastic defenses and from which the EOT attack nat- urally follows, and 3) state-of-the-art adversarial defense for naturally-trained classifiers and competitive defense compared to adversarial training on CIFAR- 10, SVHN, and CIFAR-100. Our code and pre-trained models are available at https://github.com/point0bar1/ebm-defense.",
        "authors": [
            "Mitch Hill"
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/StochasticSecurityAdversarialDefenseUsingLongRunDynamicsofEnergyBasedModels.pdf"
    },
    {
        "internal_id": 620,
        "title": "DIRECTED ACYCLIC GRAPH NEURAL NETWORKS",
        "abstract": "Graph-structured data ubiquitously appears in science and engineering. Graph neural networks (GNNs) are designed to exploit the relational inductive bias ex- hibited in graphs; they have been shown to outperform other forms of neural networks in scenarios where structure information supplements node features. The most common GNN architecture aggregates information from neighborhoods based on message passing. Its generality has made it broadly applicable. In this paper, we focus on a special, yet widely used, type of graphs\u2014DAGs\u2014and inject a stronger inductive bias\u2014partial ordering\u2014into the neural network design. We propose the directed acyclic graph neural network, DAGNN, an architecture that processes information according to the flow defined by the partial order. DAGNN can be considered a framework that entails earlier works as special cases (e.g., models for trees and models updating node representations recurrently), but we identify several crucial components that prior architectures lack. We perform comprehensive experiments, including ablation studies, on representative DAG datasets (i.e., source code, neural architectures, and probabilistic graphical mod- els) and demonstrate the superiority of DAGNN over simpler DAG architectures as well as general graph architectures.",
        "authors": [],
        "created_time": "15 February 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/DirectedAcyclicGraphNeuralNetworks.pdf"
    },
    {
        "internal_id": 621,
        "title": "LEARNING AND EVALUATING REPRESENTATIONS FOR DEEP ONE-CLASS CLASSIFICATION",
        "abstract": "We present a two-stage framework for deep one-class classification. We first learn self-supervised representations from one-class data, and then build one-class clas- sifiers on learned representations. The framework not only allows to learn bet- ter representations, but also permits building one-class classifiers that are faithful to the target task. We argue that classifiers inspired by the statistical perspec- tive in generative or discriminative models are more effective than existing ap- proaches, such as a normality score from a surrogate classifier. We thoroughly evaluate different self-supervised representation learning algorithms under the proposed framework for one-class classification. Moreover, we present a novel distribution-augmented contrastive learning that extends training distributions via data augmentation to obstruct the uniformity of contrastive representations. In experiments, we demonstrate state-of-the-art performance on visual domain one- class classification benchmarks, including novelty and anomaly detection. Finally, we present visual explanations, confirming that the decision-making process of deep one-class classifiers is intuitive to humans. The code is available at https: //github.com/google-research/deep_representation_one_class.",
        "authors": [
            "Chun Liang Li",
            "Jinsung Yoon",
            "Google Cloud AI"
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/LearningandEvaluatingRepresentationsforDeepOneClassClassification.pdf"
    },
    {
        "internal_id": 622,
        "title": "UNDERSTANDING AND IMPROVING LEXICAL CHOICE IN NON-AUTOREGRESSIVE TRANSLATION",
        "abstract": "Knowledge distillation (KD) is essential for training non-autoregressive translation (NAT) models by reducing the complexity of the raw data with an autoregressive teacher model. In this study, we empirically show that as a side effect of this training, the lexical choice errors on low-frequency words are propagated to the NAT model from the teacher model. To alleviate this problem, we propose to expose the raw data to NAT models to restore the useful information of low-frequency words, which are missed in the distilled data. To this end, we introduce an extra Kullback-Leibler divergence term derived by comparing the lexical choice of NAT model and that embedded in the raw data. Experimental results across language pairs and model architectures demonstrate the effectiveness and universality of the proposed approach. Extensive analyses confirm our claim that our approach improves performance by reducing the lexical choice errors on low-frequency words. Encouragingly, our approach pushes the SOTA NAT performance on the WMT14 English-German and WMT16 Romanian-English datasets up to 27.8 and 33.8 BLEU points, respectively.",
        "authors": [
            "Liang Ding",
            "Longyue Wang",
            "Xuebo Liu",
            "Derek F Wong"
        ],
        "created_time": "18 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/UnderstandingandImprovingLexicalChoiceinNonAutoregressiveTranslation.pdf"
    },
    {
        "internal_id": 623,
        "title": "COCO: CONTROLLABLE COUNTERFACTUALS FOR EVALUATING DIALOGUE STATE TRACKERS",
        "abstract": "Dialogue state trackers have made significant progress on benchmark datasets, but their generalization capability to novel and realistic scenarios beyond the held- out conversations is less understood. We propose controllable counterfactuals (COCO) to bridge this gap and evaluate dialogue state tracking (DST) models on novel scenarios, i.e., would the system successfully tackle the request if the user responded differently but still consistently with the dialogue flow? COCO leverages turn-level belief states as counterfactual conditionals to produce novel conversation scenarios in two steps: (i) counterfactual goal generation at turn- level by dropping and adding slots followed by replacing slot values, (ii) coun- terfactual conversation generation that is conditioned on (i) and consistent with the dialogue flow. Evaluating state-of-the-art DST models on MultiWOZ dataset with COCO-generated counterfactuals results in a significant performance drop of up to 30.8% (from 49.4% to 18.6%) in absolute joint goal accuracy. In compari- son, widely used techniques like paraphrasing only affect the accuracy by at most 2%. Human evaluations show that COCO-generated conversations perfectly re- flect the underlying user goal with more than 95% accuracy and are as human-like as the original conversations, further strengthening its reliability and promise to be adopted as part of the robustness evaluation of DST models. 1",
        "authors": [
            "Shiyang Li",
            "Semih Yavuz",
            "Kazuma Hashimoto",
            "Jia Li",
            "Tong Niu",
            "Nazneen Rajani",
            "Xifeng Yan",
            "Yingbo Zhou",
            "Caiming Xiong",
            "Santa Barbara"
        ],
        "created_time": "10 February 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/CoCoControllableCounterfactualsforEvaluatingDialogueStateTrackers.pdf"
    },
    {
        "internal_id": 624,
        "title": "RANDOMIZED ENSEMBLED DOUBLE Q-LEARNING: LEARNING FAST WITHOUT A MODEL",
        "abstract": "Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model- free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model- based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio \u226b 1; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio \u226b 1.",
        "authors": [
            "Xinyue Chen",
            "Che Wang",
            "Zijian Zhou",
            "Keith Ross"
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/RandomizedEnsembledDoubleQLearningLearningFastWithoutaModel.pdf"
    },
    {
        "internal_id": 625,
        "title": "GENERATIVE LANGUAGE-GROUNDED POLICY IN VISION-AND-LANGUAGE NAVIGATION WITH BAYES' RULE",
        "abstract": "Vision-and-language navigation (VLN) is a task in which an agent is embodied in a realistic 3D environment and follows an instruction to reach the goal node. While most of the previous studies have built and investigated a discriminative approach, we notice that there are in fact two possible approaches to building such a VLN agent: discriminative and generative. In this paper, we design and investigate a generative language-grounded policy which uses a language model to compute the distribution over all possible instructions i.e. all possible sequences of vocabulary tokens given action and the transition history. In experiments, we show that the proposed generative approach outperforms the discriminative approach in the Room-2-Room (R2R) and Room-4-Room (R4R) datasets, especially in the unseen environments. We further show that the combination of the generative and discriminative policies achieves close to the state-of-the art results in the R2R dataset, demonstrating that the generative and discriminative policies capture the different aspects of VLN.",
        "authors": [
            "Shuhei Kurita",
            "Kyunghyun Cho",
            "CIFAR Fellow"
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/GenerativeLanguageGroundedPolicyinVisionandLanguageNavigationwithBayesRule.pdf"
    },
    {
        "internal_id": 626,
        "title": "LEARNING WITH INSTANCE-DEPENDENT LABEL NOISE: A SAMPLE SIEVE APPROACH",
        "abstract": "Human-annotated labels are often prone to noise, and the presence of such noise will degrade the performance of the resulting deep neural network (DNN) mod- els. Much of the literature (with several recent exceptions) of learning with noisy labels focuses on the case when the label noise is independent of features. Practi- cally, annotations errors tend to be instance-dependent and often depend on the difficulty levels of recognizing a certain task. Applying existing results from instance-independent settings would require a significant amount of estimation of noise rates. Therefore, providing theoretically rigorous solutions for learning with instance-dependent label noise remains a challenge. In this paper, we propose CORES2 (COnfidence REgularized Sample Sieve), which progressively sieves out corrupted examples. The implementation of CORES2 does not require specifying noise rates and yet we are able to provide theoretical guarantees of CORES2 in filtering out the corrupted examples. This high-quality sample sieve allows us to treat clean examples and the corrupted ones separately in training a DNN solu- tion, and such a separation is shown to be advantageous in the instance-dependent noise setting. We demonstrate the performance of CORES2 on CIFAR10 and CI- FAR100 datasets with synthetic instance-dependent label noise and Clothing1M with real-world human noise. As of independent interests, our sample sieve pro- vides a generic machinery for anatomizing noisy datasets and provides a flexi- ble interface for various robust training techniques to further improve the perfor- mance. Code is available at https://github.com/UCSC-REAL/cores.",
        "authors": [
            "Hao Cheng",
            "Zhaowei Zhu",
            "Xingyu Li",
            "Yifei Gong",
            "Xing Sun",
            "Santa Cruz"
        ],
        "created_time": "18 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/LearningwithInstanceDependentLabelNoiseASampleSieveApproach.pdf"
    },
    {
        "internal_id": 628,
        "title": "EFFICIENT CERTIFIED DEFENSES AGAINST PATCH AT- TACKS ON IMAGE CLASSIFIERS",
        "abstract": "Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated driving should thus contain a fail-safe fallback component that combines certifiable robustness against patches with effi- cient inference while maintaining high performance on clean inputs. We propose BAGCERT, a novel combination of model architecture and certification procedure that allows efficient certification. We derive a loss that enables end-to-end op- timization of certified robustness against patches of different sizes and locations. On CIFAR10, BAGCERT certifies 10.000 examples in 43 seconds on a single GPU and obtains 86% clean and 60% certified accuracy against 5 \u00d7 5 patches.",
        "authors": [
            "Robert Bosch GmbH",
            " Renningen"
        ],
        "created_time": "12 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/EfficientCertifiedDefensesAgainstPatchAttacksonImageClassifiers.pdf"
    },
    {
        "internal_id": 629,
        "title": "Drop-Bottleneck: LEARNING DISCRETE COMPRESSED REPRESENTATION FOR NOISE-ROBUST EXPLORATION",
        "abstract": "We propose a novel information bottleneck (IB) method named Drop-Bottleneck, which discretely drops features that are irrelevant to the target variable. Drop- Bottleneck not only enjoys a simple and tractable compression objective but also additionally provides a deterministic compressed representation of the input vari- able, which is useful for inference tasks that require consistent representation. Moreover, it can jointly learn a feature extractor and select features considering each feature dimension's relevance to the target task, which is unattainable by most neural network-based IB methods. We propose an exploration method based on Drop-Bottleneck for reinforcement learning tasks. In a multitude of noisy and reward sparse maze navigation tasks in VizDoom (Kempka et al., 2016) and DM- Lab (Beattie et al., 2016), our exploration method achieves state-of-the-art per- formance. As a new IB framework, we demonstrate that Drop-Bottleneck outper- forms Variational Information Bottleneck (VIB) (Alemi et al., 2017) in multiple aspects including adversarial robustness and dimensionality reduction.",
        "authors": [
            "Jaekyeom Kim",
            "Minjung Kim"
        ],
        "created_time": "18 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/DropBottleneckLearningDiscreteCompressedRepresentationforNoiseRobustExploration.pdf"
    },
    {
        "internal_id": 630,
        "title": "DEEP PARTITION AGGREGATION: PROVABLE DEFENSES AGAINST GENERAL POISONING ATTACKS",
        "abstract": "Adversarial poisoning attacks distort training data in order to corrupt the test-time behavior of a classifier. A provable defense provides a certificate for each test sample, which is a lower bound on the magnitude of any adversarial distortion of the training set that can corrupt the test sample's classification. We propose two novel provable defenses against poisoning attacks: (i) Deep Partition Aggregation (DPA), a certified defense against a general poisoning threat model, defined as the insertion or deletion of a bounded number of samples to the training set \u2014 by implication, this threat model also includes arbitrary distortions to a bounded number of images and/or labels; and (ii) Semi-Supervised DPA (SS-DPA), a certi- fied defense against label-flipping poisoning attacks. DPA is an ensemble method where base models are trained on partitions of the training set determined by a hash function. DPA is related to both subset aggregation, a well-studied ensem- ble method in classical machine learning, as well as to randomized smoothing, a popular provable defense against evasion (inference) attacks. Our defense against label-flipping poison attacks, SS-DPA, uses a semi-supervised learning algorithm as its base classifier model: each base classifier is trained using the entire unla- beled training set in addition to the labels for a partition. SS-DPA significantly outperforms the existing certified defense for label-flipping attacks (Rosenfeld et al., 2020) on both MNIST and CIFAR-10: provably tolerating, for at least half of test images, over 600 label flips (vs. < 200 label flips) on MNIST and over 300 label flips (vs. 175 label flips) on CIFAR-10. Against general poi- soning attacks where no prior certified defenses exists, DPA can certify \u2265 50% of test images against over 500 poison image insertions on MNIST, and nine insertions on CIFAR-10. These results establish new state-of-the-art provable defenses against general and label-flipping poison attacks. Code is available at https://github.com/alevine0/DPA.",
        "authors": [
            "MD "
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/DeepPartitionAggregationProvableDefensesagainstGeneralPoisoningAttacks.pdf"
    },
    {
        "internal_id": 631,
        "title": "DENOISING DIFFUSION IMPLICIT MODELS",
        "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high qual- ity image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sam- pling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10\u00d7 to 50\u00d7 faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error.",
        "authors": [
            "Jiaming Song"
        ],
        "created_time": "03 May 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/DenoisingDiffusionImplicitModels.pdf"
    },
    {
        "internal_id": 632,
        "title": "ON POSITION EMBEDDINGS IN BERT",
        "abstract": "Various Position Embeddings (PEs) have been proposed in Transformer based ar- chitectures (e.g. BERT) to model word order. These are empirically-driven and perform well, but no formal framework exists to systematically study them. To address this, we present three properties of PEs that capture word distance in vec- tor space: translation invariance, monotonicity, and symmetry. These properties formally capture the behaviour of PEs and allow us to reinterpret sinusoidal PEs in a principled way. Moreover, we propose a new probing test (called 'identical word probing') and mathematical indicators to quantitatively detect the general attention patterns with respect to the above properties. An empirical evaluation of seven PEs (and their combinations) for classification (GLUE) and span predic- tion (SQuAD) shows that: (1) both classification and span prediction benefit from translation invariance and local monotonicity, while symmetry slightly decreases performance; (2) The fully-learnable absolute PE performs better in classification, while relative PEs perform better in span prediction. We contribute the first formal and quantitative analysis of desiderata for PEs, and a principled discussion about their correlation to the performance of typical downstream tasks.",
        "authors": [
            "Benyou Wang",
            "Lifeng Shang",
            "Christina Lioma",
            "Xin Jiang",
            "Hao Yang",
            "Qun Liu",
            "Jakob Grue Simonsen"
        ],
        "created_time": "02 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/OnPositionEmbeddingsinBERT.pdf"
    },
    {
        "internal_id": 633,
        "title": "SSD: A UNIFIED FRAMEWORK FOR SELF- SUPERVISED OUTLIER DETECTION",
        "abstract": "We ask the following question: what training information is required to design an effective outlier/out-of-distribution (OOD) detector, i.e., detecting samples that lie far away from the training distribution? Since unlabeled data is easily accessible for many applications, the most compelling approach is to develop detectors based on only unlabeled in-distribution data. However, we observe that most existing detectors based on unlabeled data perform poorly, often equivalent to a random prediction. In contrast, existing state-of-the-art OOD detectors achieve impressive performance but require access to fine-grained data labels for supervised training. We propose SSD, an outlier detector based on only unlabeled in-distribution data. We use self-supervised representation learning followed by a Mahalanobis distance based detection in the feature space. We demonstrate that SSD outperforms most existing detectors based on unlabeled data by a large margin. Additionally, SSD even achieves performance on par, and sometimes even better, with supervised training based detectors. Finally, we expand our detection framework with two key extensions. First, we formulate few-shot OOD detection, in which the detector has access to only one to five samples from each class of the targeted OOD dataset. Second, we extend our framework to incorporate training data labels, if available. We find that our novel detection framework based on SSD displays enhanced performance with these extensions, and achieves state-of-the-art performance1.",
        "authors": [
            "Vikash Sehwag",
            "Mung Chiang",
            "Prateek Mittal"
        ],
        "created_time": "18 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/SSDAUnifiedFrameworkforSelfSupervisedOutlierDetection.pdf"
    },
    {
        "internal_id": 634,
        "title": "HOPPER: MULTI-HOP TRANSFORMER FOR SPATIOTEMPORAL REASONING",
        "abstract": "This paper considers the problem of spatiotemporal object-centric reasoning in videos. Central to our approach is the notion of object permanence, i.e., the ability to reason about the location of objects as they move through the video while being occluded, contained or carried by other objects. Existing deep learning based approaches often suffer from spatiotemporal biases when applied to video reasoning problems. We propose Hopper, which uses a Multi-hop Transformer for reasoning object permanence in videos. Given a video and a localization query, Hopper reasons over image and object tracks to automatically hop over critical frames in an iterative fashion to predict the final position of the object of interest. We demonstrate the effectiveness of using a contrastive loss to reduce spatiotemporal biases. We evaluate over CATER dataset and find that Hopper achieves 73.2% Top-1 accuracy using just 1 FPS by hopping through just a few critical frames. We also demonstrate Hopper can perform long-term reasoning by building a CATER-h dataset1 that requires multi-step reasoning to localize objects of interest correctly.",
        "authors": [
            "Honglu Zhou",
            "Asim Kadav",
            "Farley Lai",
            "Martin Renqiang Min",
            "Mubbasir Kapadia",
            "Hans Peter Graf",
            "San Jose"
        ],
        "created_time": "22 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/HopperMultihopTransformerforSpatiotemporalReasoning.pdf"
    },
    {
        "internal_id": 635,
        "title": "INTRACLASS CLUSTERING: AN IMPLICIT LEARNING ABILITY THAT REGULARIZES DNNS",
        "abstract": "Several works have shown that the regularization mechanisms underlying deep neural networks' generalization performances are still poorly understood (Neyshabur et al., 2015; Zhang et al., 2017). In this paper, we hypothesize that deep neural networks are regularized through their ability to extract meaningful clusters among the samples of a class. This constitutes an implicit form of regu- larization, as no explicit training mechanisms or supervision target such behaviour. To support our hypothesis, we design four different measures of intraclass cluster- ing, based on the neuron- and layer-level representations of the training data. We then show that these measures constitute accurate predictors of generalization per- formance across variations of a large set of hyperparameters (learning rate, batch size, optimizer, weight decay, dropout rate, data augmentation, network depth and width).",
        "authors": [
            "Simon Carbonnelle",
            "Christophe De Vleeschouwer",
            "Louvain La Neuve"
        ],
        "created_time": "24 February 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/IntraclassclusteringanimplicitlearningabilitythatregularizesDNNs.pdf"
    },
    {
        "internal_id": 636,
        "title": "FAIRBATCH: BATCH SELECTION FOR MODEL FAIRNESS",
        "abstract": "Training a fair machine learning model is essential to prevent demographic disparity. Existing techniques for improving model fairness require broad changes in either data preprocessing or model training, rendering themselves difficult-to-adopt for potentially already complex machine learning systems. We address this problem via the lens of bilevel optimization. While keeping the standard training algorithm as an inner optimizer, we incorporate an outer optimizer so as to equip the inner problem with an additional functionality: Adaptively selecting minibatch sizes for the purpose of improving model fairness. Our batch selection algorithm, which we call FairBatch, implements this optimization and supports prominent fairness measures: equal opportunity, equalized odds, and demographic parity. FairBatch comes with a significant implementation benefit \u2013 it does not require any modification to data preprocessing or model training. For instance, a single-line change of PyTorch code for replacing batch selection part of model training suffices to employ FairBatch. Our experiments conducted both on synthetic and benchmark real data demonstrate that FairBatch can provide such functionalities while achieving comparable (or even greater) performances against the state of the arts. Furthermore, FairBatch can readily improve fairness of any pre-trained model simply via fine-tuning. It is also compatible with existing batch selection techniques intended for different purposes, such as faster convergence, thus gracefully achieving multiple purposes.",
        "authors": [
            "Yuji Roh",
            "Kangwook Lee",
            "Changho Suh"
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/FairBatchBatchSelectionforModelFairness.pdf"
    },
    {
        "internal_id": 637,
        "title": "ON INSTAHIDE, PHASE RETRIEVAL, AND SPARSE MA- TRIX FACTORIZATION",
        "abstract": "In this work, we examine the security of InstaHide, a scheme recently proposed by Huang et al. (2020b) for preserving the security of private datasets in the context of distributed learning. To generate a synthetic training example to be shared among the distributed learners, InstaHide takes a convex combination of private feature vectors and randomly flips the sign of each entry of the resulting vector with probability 1/2. A salient question is whether this scheme is secure in any provable sense, perhaps under a plausible complexity-theoretic assumption. The answer to this turns out to be quite subtle and closely related to the average- case complexity of a multi-task, missing-data version of the classic problem of phase retrieval that is interesting in its own right. Motivated by this connection, under the standard distributional assumption that the public/private feature vectors are isotropic Gaussian, we design an algorithm that can actually recover a private vector using only the public vectors and a sequence of synthetic vectors generated by InstaHide.",
        "authors": [
            "Sitan Chen",
            "Xiaoxiao Li",
            "Zhao Song",
            "Danyang Zhuo"
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/OnInstaHidePhaseRetrievalandSparseMatrixFactorization.pdf"
    },
    {
        "internal_id": 638,
        "title": "EVALUATION OF SIMILARITY-BASED EXPLANATIONS",
        "abstract": "Explaining the predictions made by complex machine learning models helps users to understand and accept the predicted outputs with confidence. One promising way is to use similarity-based explanation that provides similar instances as evidence to support model predictions. Several relevance metrics are used for this purpose. In this study, we investigated relevance metrics that can provide reasonable ex- planations to users. Specifically, we adopted three tests to evaluate whether the relevance metrics satisfy the minimal requirements for similarity-based explanation. Our experiments revealed that the cosine similarity of the gradients of the loss performs best, which would be a recommended choice in practice. In addition, we showed that some metrics perform poorly in our tests and analyzed the reasons of their failure. We expect our insights to help practitioners in selecting appropriate relevance metrics and also aid further researches for designing better relevance metrics for explanations.",
        "authors": [
            "Kazuaki Hanawa",
            "Sho Yokoi",
            "Satoshi Hara",
            "Kentaro Inui"
        ],
        "created_time": "18 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/EvaluationofSimilaritybasedExplanations.pdf"
    },
    {
        "internal_id": 639,
        "title": "PROXIMAL GRADIENT DESCENT-ASCENT: VARIABLE CONVERGENCE UNDER K\u0141 GEOMETRY",
        "abstract": "The gradient descent-ascent (GDA) algorithm has been widely applied to solve minimax optimization problems. In order to achieve convergent policy parameters for minimax optimization, it is important that GDA generates convergent variable sequences rather than convergent sequences of function values or gradient norms. However, the variable convergence of GDA has been proved only under convexity geometries, and there lacks understanding for general nonconvex minimax opti- mization. This paper fills such a gap by studying the convergence of a more general proximal-GDA for regularized nonconvex-strongly-concave minimax optimiza- tion. Specifically, we show that proximal-GDA admits a novel Lyapunov function, which monotonically decreases in the minimax optimization process and drives the variable sequence to a critical point. By leveraging this Lyapunov function and the K\u0141 geometry that parameterizes the local geometries of general nonconvex functions, we formally establish the variable convergence of proximal-GDA to a critical point x\u2217, i.e., xt \u2192 x\u2217, yt \u2192 y\u2217(x\u2217). Furthermore, over the full spectrum of the K\u0141-parameterized geometry, we show that proximal-GDA achieves different types of convergence rates ranging from sublinear convergence up to finite-step convergence, depending on the geometry associated with the K\u0141 parameter. This is the first theoretical result on the variable convergence for nonconvex minimax optimization.",
        "authors": [
            "Ziyi Chen",
            "Yi Zhou",
            "Salt Lake City",
            "UT ",
            "Tengyu Xu",
            "Yingbin Liang",
            "OH "
        ],
        "created_time": "17 February 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/ProximalGradientDescentAscentVariableConvergenceunderKGeometry.pdf"
    },
    {
        "internal_id": 640,
        "title": "EMERGENT ROAD RULES IN MULTI-AGENT DRIVING ENVIRONMENTS",
        "abstract": "For autonomous vehicles to safely share the road with human drivers, autonomous vehicles must abide by specific \"road rules\" that human drivers have agreed to follow. \"Road rules\" include rules that drivers are required to follow by law \u2013 such as the requirement that vehicles stop at red lights \u2013 as well as more subtle social rules \u2013 such as the implicit designation of fast lanes on the highway. In this paper, we provide empirical evidence that suggests that \u2013 instead of hard-coding road rules into self-driving algorithms \u2013 a scalable alternative may be to design multi-agent environments in which road rules emerge as optimal solutions to the problem of maximizing traffic flow. We analyze what ingredients in driving environments cause the emergence of these road rules and find that two crucial factors are noisy perception and agents' spatial density. We provide qualitative and quantitative evidence of the emergence of seven social driving behaviors, ranging from obeying traffic signals to following lanes, all of which emerge from training agents to drive quickly to destinations without colliding. Our results add empirical support for the social road rules that countries worldwide have agreed on for safe, efficient driving.",
        "authors": [
            "Avik Pal",
            "Jonah Philion",
            "Yuan Hong Liao",
            "Sanja Fidler",
            "IIT Kanpur"
        ],
        "created_time": "11 February 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/EmergentRoadRulesInMultiAgentDrivingEnvironments.pdf"
    },
    {
        "internal_id": 641,
        "title": "CAUSALWORLD: A ROBOTIC MANIPULATION BENCHMARK FOR CAUSAL STRUCTURE AND TRANS- FER LEARNING",
        "abstract": "Despite recent successes of reinforcement learning (RL), it remains a challenge for agents to transfer learned skills to related environments. To facilitate research ad- dressing this problem, we propose CausalWorld, a benchmark for causal structure and transfer learning in a robotic manipulation environment. The environment is a simulation of an open-source robotic platform, hence offering the possibility of sim-to-real transfer. Tasks consist of constructing 3D shapes from a set of blocks - inspired by how children learn to build complex structures. The key strength of CausalWorld is that it provides a combinatorial family of such tasks with common causal structure and underlying factors (including, e.g., robot and object masses, colors, sizes). The user (or the agent) may intervene on all causal variables, which allows for fine-grained control over how similar different tasks (or task distri- butions) are. One can thus easily define training and evaluation distributions of a desired difficulty level, targeting a specific form of generalization (e.g., only changes in appearance or object mass). Further, this common parametrization fa- cilitates defining curricula by interpolating between an initial and a target task. While users may define their own task distributions, we present eight meaningful distributions as concrete benchmarks, ranging from simple to very challenging, all of which require long-horizon planning as well as precise low-level motor control. Finally, we provide baseline results for a subset of these tasks on distinct training curricula and corresponding evaluation protocols, verifying the feasibility of the tasks in this benchmark.1",
        "authors": [
            "Ossama Ahmed",
            " Anirudh Goyal",
            "Yoshua Bengio",
            " Stefan Bauer"
        ],
        "created_time": "18 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/CausalWorldARoboticManipulationBenchmarkforCausalStructureandTransferLearning.pdf"
    },
    {
        "internal_id": 643,
        "title": "BSQ: EXPLORING BIT-LEVEL SPARSITY FOR MIXED- PRECISION NEURAL NETWORK QUANTIZATION",
        "abstract": "Mixed-precision quantization can potentially achieve the optimal tradeoff between performance and compression rate of deep neural networks, and thus, have been widely investigated. However, it lacks a systematic method to determine the exact quantization scheme. Previous methods either examine only a small manually- designed search space or utilize a cumbersome neural architecture search to explore the vast search space. These approaches cannot lead to an optimal quantization scheme efficiently. This work proposes bit-level sparsity quantization (BSQ) to tackle the mixed-precision quantization from a new angle of inducing bit-level sparsity. We consider each bit of quantized weights as an independent trainable variable and introduce a differentiable bit-sparsity regularizer. BSQ can induce all-zero bits across a group of weight elements and realize the dynamic precision reduction, leading to a mixed-precision quantization scheme of the original model. Our method enables the exploration of the full mixed-precision space with a single gradient-based optimization process, with only one hyperparameter to tradeoff the performance and compression. BSQ achieves both higher accuracy and higher bit reduction on various model architectures on the CIFAR-10 and ImageNet datasets comparing to previous methods.",
        "authors": [
            "Huanrui Yang",
            "Lin Duan",
            "NC "
        ],
        "created_time": "15 February 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/BSQExploringBitLevelSparsityforMixedPrecisionNeuralNetworkQuantization.pdf"
    },
    {
        "internal_id": 644,
        "title": "MOVIE: REVISITING MODULATED CONVOLUTIONS FOR VISUAL COUNTING AND BEYOND",
        "abstract": "This paper focuses on visual counting, which aims to predict the number of oc- currences given a natural image and a query (e.g. a question or a category). Un- like most prior works that use explicit, symbolic models which can be computa- tionally expensive and limited in generalization, we propose a simple and effec- tive alternative by revisiting modulated convolutions that fuse the query and the image locally. Following the design of residual bottleneck, we call our method MoVie, short for Modulated conVolutional bottlenecks. Notably, MoVie reasons implicitly and holistically and only needs a single forward-pass during inference. Nevertheless, MoVie showcases strong performance for counting: 1) advancing the state-of-the-art on counting-specific VQA tasks while being more efficient; 2) outperforming prior-art on difficult benchmarks like COCO for common ob- ject counting; 3) helped us secure the first place of 2020 VQA challenge when integrated as a module for 'number' related questions in generic VQA models. Finally, we show evidence that modulated convolutions such as MoVie can serve as a general mechanism for reasoning tasks beyond counting.",
        "authors": [
            "Duy Kien Nguyen",
            "Vedanuj Goswami",
            "Xinlei Chen"
        ],
        "created_time": "10 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/MoVieRevisitingModulatedConvolutionsforVisualCountingandBeyond.pdf"
    },
    {
        "internal_id": 645,
        "title": "ACTING IN DELAYED ENVIRONMENTS WITH NON-STATIONARY MARKOV POLICIES",
        "abstract": "The standard Markov Decision Process (MDP) formulation hinges on the assump- tion that an action is executed immediately after it was chosen. However, assuming it is often unrealistic and can lead to catastrophic failures in applications such as robotic manipulation, cloud computing, and finance. We introduce a framework for learning and planning in MDPs where the decision-maker commits actions that are executed with a delay of m steps. The brute-force state augmentation baseline where the state is concatenated to the last m committed actions suffers from an exponential complexity in m, as we show for policy iteration. We then prove that with execution delay, deterministic Markov policies in the original state-space are sufficient for attaining maximal reward, but need to be non-stationary. As for stationary Markov policies, we show they are sub-optimal in general. Consequently, we devise a non-stationary Q-learning style model-based algorithm that solves delayed execution tasks without resorting to state-augmentation. Experiments on tabular, physical, and Atari domains reveal that it converges quickly to high perfor- mance even for substantial delays, while standard approaches that either ignore the delay or rely on state-augmentation struggle or fail due to divergence. The code is available at https://github.com/galdl/rl_delay_basic.git.",
        "authors": [
            "Esther Derman",
            "Gal Dalal",
            "Shie Mannor"
        ],
        "created_time": "18 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/ActinginDelayedEnvironmentswithNonStationaryMarkovPolicies.pdf"
    },
    {
        "internal_id": 646,
        "title": "BYPASSING THE AMBIENT DIMENSION: PRIVATE SGD WITH GRADIENT SUBSPACE IDENTIFICATION",
        "abstract": "Differentially private SGD (DP-SGD) is one of the most popular methods for solv- ing differentially private empirical risk minimization (ERM). Due to its noisy per- turbation on each gradient update, the error rate of DP-SGD scales with the ambi- ent dimension p, the number of parameters in the model. Such dependence can be problematic for over-parameterized models where p \u226b n, the number of training samples. Existing lower bounds on private ERM show that such dependence on p is inevitable in the worst case. In this paper, we circumvent the dependence on the ambient dimension by leveraging a low-dimensional structure of gradient space in deep networks\u2014that is, the stochastic gradients for deep nets usually stay in a low dimensional subspace in the training process. We propose Projected DP-SGD that performs noise reduction by projecting the noisy gradients to a low-dimensional subspace, which is given by the top gradient eigenspace on a small public dataset. We provide a general sample complexity analysis on the public dataset for the gradient subspace identification problem and demonstrate that under certain low- dimensional assumptions the public sample complexity only grows logarithmi- cally in p. Finally, we provide a theoretical analysis and empirical evaluations to show that our method can substantially improve the accuracy of DP-SGD in the high privacy regime (corresponding to low privacy loss \u03f5).",
        "authors": [
            "Yingxue Zhou",
            "Zhiwei Steven Wu",
            "Arindam Banerjee"
        ],
        "created_time": "18 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/BypassingtheAmbientDimensionPrivateSGDwithGradientSubspaceIdentification.pdf"
    },
    {
        "internal_id": 647,
        "title": "DOP: OFF-POLICY MULTI-AGENT DECOMPOSED POLICY GRADIENTS",
        "abstract": "Multi-agent policy gradient (MAPG) methods recently witness vigorous progress. However, there is a significant performance discrepancy between MAPG meth- ods and state-of-the-art multi-agent value-based approaches. In this paper, we investigate causes that hinder the performance of MAPG algorithms and present a multi-agent decomposed policy gradient method (DOP). This method introduces the idea of value function decomposition into the multi-agent actor-critic framework. Based on this idea, DOP supports efficient off-policy learning and addresses the issue of centralized-decentralized mismatch and credit assignment in both discrete and continuous action spaces. We formally show that DOP critics have sufficient representational capability to guarantee convergence. In addition, empirical evalu- ations on the StarCraft II micromanagement benchmark and multi-agent particle environments demonstrate that DOP outperforms both state-of-the-art value-based and policy-based multi-agent reinforcement learning algorithms. Demonstrative videos are available at https://sites.google.com/view/dop-mapg/.",
        "authors": [
            "Yihan Wang",
            "Beining Han",
            "Tonghan Wang",
            "Heng Dong",
            "Chongjie Zhang"
        ],
        "created_time": "18 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/DOPOffPolicyMultiAgentDecomposedPolicyGradients.pdf"
    },
    {
        "internal_id": 648,
        "title": "BENCHMARKS FOR DEEP OFF-POLICY EVALUATION",
        "abstract": "Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both evaluating and selecting complex policies for decision making. The ability to learn offline is particularly important in many real-world domains, such as in healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring on- line interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, com- paring results between papers is difficult because currently there is a lack of a comprehensive and unified benchmark, and measuring algorithmic progress has been challenging due to the lack of difficult evaluation tasks. In order to address this gap, we present a collection of policies that in conjunction with existing offline datasets can be used for benchmarking off-policy evaluation. Our tasks include a range of challenging high-dimensional continuous control problems, with wide selections of datasets and policies for performing policy selection. The goal of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform an evaluation of state-of-the-art algorithms and provide open-source access to our data and code to foster future research in this area\u2020.",
        "authors": [
            "Justin Fu ",
            "Mohammad Norouzi ",
            "George Tucker ",
            "Ziyu Wang",
            "Mengjiao Yang",
            "Michael R Zhang",
            "Yutian Chen",
            "Aviral Kumar",
            "Cosmin Paduraru",
            "Sergey Levine"
        ],
        "created_time": "18 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/BenchmarksforDeepOffPolicyEvaluation.pdf"
    },
    {
        "internal_id": 649,
        "title": "CAN A FRUIT FLY LEARN WORD EMBEDDINGS?",
        "abstract": "The mushroom body of the fruit fly brain is one of the best studied systems in neuroscience. At its core it consists of a population of Kenyon cells, which re- ceive inputs from multiple sensory modalities. These cells are inhibited by the anterior paired lateral neuron, thus creating a sparse high dimensional represen- tation of the inputs. In this work we study a mathematical formalization of this network motif and apply it to learning the correlational structure between words and their context in a corpus of unstructured text, a common natural language pro- cessing (NLP) task. We show that this network can learn semantic representations of words and can generate both static and context-dependent word embeddings. Unlike conventional methods (e.g., BERT, GloVe) that use dense representations for word embedding, our algorithm encodes semantic meaning of words and their context in the form of sparse binary hash codes. The quality of the learned rep- resentations is evaluated on word similarity analysis, word-sense disambiguation, and document classification. It is shown that not only can the fruit fly network motif achieve performance comparable to existing methods in NLP, but, addition- ally, it uses only a fraction of the computational resources (shorter training time and smaller memory footprint).",
        "authors": [
            "Yuchen Liang",
            "Chaitanya K Ryali",
            "Benjamin Hoover",
            "Leopold Grinberg",
            "Saket Navlakha",
            "Mohammed J Zaki",
            "Dmitry Krotov"
        ],
        "created_time": "13 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/CanaFruitFlyLearnWordEmbeddings.pdf"
    },
    {
        "internal_id": 650,
        "title": "CONDITIONAL NEGATIVE SAMPLING FOR CON-",
        "abstract": "Recent methods for learning unsupervised visual representations, dubbed con- trastive learning, optimize the noise-contrastive estimation (NCE) bound on mu- tual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally \u2013 in a \"ring\" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing mod- els (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. More- over, we find continued benefits when transferring features to a variety of new im- age distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.",
        "authors": [
            "Mike Wu",
            "Milan Mosse",
            "Chengxu Zhuang",
            "Daniel Yamins",
            "Noah Goodman"
        ],
        "created_time": "16 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/ConditionalNegativeSamplingforContrastiveLearningofVisualRepresentations.pdf"
    },
    {
        "internal_id": 651,
        "title": "LEARNING BETTER STRUCTURED REPRESENTATIONS USING LOW-RANK ADAPTIVE LABEL SMOOTHING",
        "abstract": "Training with soft targets instead of hard targets has been shown to improve per- formance and calibration of deep neural networks. Label smoothing is a popular way of computing soft targets, where one-hot encoding of a class is smoothed with a uniform distribution. Owing to its simplicity, label smoothing has found wide- spread use for training deep neural networks on a wide variety of tasks, ranging from image and text classification to machine translation and semantic parsing. Complementing recent empirical justification for label smoothing, we obtain PAC- Bayesian generalization bounds for label smoothing and show that the generaliza- tion error depends on the choice of the noise (smoothing) distribution. Then we propose low-rank adaptive label smoothing (LORAS): a simple yet novel method for training with learned soft targets that generalizes label smoothing and adapts to the latent structure of the label space in structured prediction tasks. Specifi- cally, we evaluate our method on semantic parsing tasks and show that training with appropriately smoothed soft targets can significantly improve accuracy and model calibration, especially in low-resource settings. Used in conjunction with pre-trained sequence-to-sequence models, our method achieves state of the art performance on four semantic parsing data sets. LORAS can be used with any model, improves performance and implicit model calibration without increasing the number of model parameters, and can be scaled to problems with large label spaces containing tens of thousands of labels.",
        "authors": [
            "Asish Ghoshal",
            "Xilun Chen",
            "Sonal Gupta"
        ],
        "created_time": "24 February 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/LearningBetterStructuredRepresentationsUsingLowrankAdaptiveLabelSmoothing.pdf"
    },
    {
        "internal_id": 652,
        "title": "PRUNING NEURAL NETWORKS AT INITIALIZATION: WHY ARE WE MISSING THE MARK?",
        "abstract": "Recent work has explored the possibility of pruning neural networks at initializa- tion. We assess proposals for doing so: SNIP (Lee et al., 2019), GraSP (Wang et al., 2020), SynFlow (Tanaka et al., 2020), and magnitude pruning. Although these methods surpass the trivial baseline of random pruning, we find that they remain below the accuracy of magnitude pruning after training. We show that, un- like magnitude pruning after training, randomly shuffling the weights these meth- ods prune within each layer or sampling new initial values preserves or improves accuracy. As such, the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune. This property suggests broader challenges with the underlying pruning heuristics, the desire to prune at initialization, or both.",
        "authors": [
            "Jonathan Frankle",
            "MIT CSAIL",
            "Gintare Karolina Dziugaite",
            "Element AI",
            "Daniel M Roy",
            "Michael Carbin",
            "MIT CSAIL"
        ],
        "created_time": "21 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/PruningNeuralNetworksatInitializationWhyAreWeMissingtheMark.pdf"
    },
    {
        "internal_id": 653,
        "title": "FEDERATED SEMI-SUPERVISED LEARNING WITH INTER-CLIENT CONSISTENCY & DISJOINT LEARNING",
        "abstract": "While existing federated learning approaches mostly require that clients have fully- labeled data to train on, in realistic settings, data obtained at the client-side often comes without any accompanying labels. Such deficiency of labels may result from either high labeling cost, or difficulty of annotation due to the requirement of expert knowledge. Thus the private data at each client may be either partly labeled, or completely unlabeled with labeled data being available only at the server, which leads us to a new practical federated learning problem, namely Federated Semi- Supervised Learning (FSSL). In this work, we study two essential scenarios of FSSL based on the location of the labeled data. The first scenario considers a conventional case where clients have both labeled and unlabeled data (labels-at-client), and the second scenario considers a more challenging case, where the labeled data is only available at the server (labels-at-server). We then propose a novel method to tackle the problems, which we refer to as Federated Matching (FedMatch). FedMatch improves upon naive combinations of federated learning and semi-supervised learning approaches with a new inter-client consistency loss and decomposition of the parameters for disjoint learning on labeled and unlabeled data. Through extensive experimental validation of our method in the two different scenarios, we show that our method outperforms both local semi-supervised learning and baselines which naively combine federated learning with semi-supervised learning. The code is available at https://github.com/wyjeong/FedMatch.",
        "authors": [
            "Wonyong Jeong",
            "Jaehong Yoon",
            "Eunho Yang",
            "AITRICS "
        ],
        "created_time": "06 September 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/FederatedSemiSupervisedLearningwithInterClientConsistencyDisjointLearning.pdf"
    },
    {
        "internal_id": 654,
        "title": "CAPC LEARNING: CONFIDENTIAL AND PRIVATE COLLABORATIVE LEARNING",
        "abstract": "Machine learning benefits from large training datasets, which may not always be possible to collect by any single entity, especially when using privacy-sensitive data. In many contexts, such as healthcare and finance, separate parties may wish to collaborate and learn from each other's data but are prevented from doing so due to privacy regulations. Some regulations prevent explicit sharing of data between parties by joining datasets in a central location (confidentiality). Others also limit implicit sharing of data, e.g., through model predictions (privacy). There is currently no method that enables machine learning in such a setting, where both confidentiality and privacy need to be preserved, to prevent both explicit and implicit sharing of data. Federated learning only provides confidentiality, not privacy, since gradients shared still contain private information. Differentially private learning assumes unreasonably large datasets. Furthermore, both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model. We introduce Confidential and Private Collaborative (CaPC) learning, the first method provably achieving both confidentiality and privacy in a collaborative setting. We leverage secure multi- party computation (MPC), homomorphic encryption (HE), and other techniques in combination with privately aggregated teacher models. We demonstrate how CaPC allows participants to collaborate without having to explicitly join their training sets or train a central model. Each party is able to improve the accuracy and fairness of their model, even in settings where each party has a model that performs well on their own dataset or when datasets are not IID and model architectures are heterogeneous across parties.1",
        "authors": [
            "Natalie Dullerud",
            "Adam Dziedzic",
            "Yunxiang Zhang",
            "Somesh Jha",
            "Nicolas Papernot",
            "Xiao Wang"
        ],
        "created_time": "19 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/CaPCLearningConfidentialandPrivateCollaborativeLearning.pdf"
    },
    {
        "internal_id": 655,
        "title": "\u2207Sim: DIFFERENTIABLE SIMULATION FOR SYSTEM IDENTIFICATION AND VISUOMOTOR CONTROL",
        "abstract": "We consider the problem of estimating an object's physical properties such as mass, friction, and elasticity directly from video sequences. Such a system identification problem is fundamentally ill-posed due to the loss of information during image formation. Current solutions require precise 3D labels which are labor-intensive to gather, and infeasible to create for many systems such as deformable solids or cloth. We present \u2207Sim, a framework that overcomes the dependence on 3D supervision by leveraging differentiable multiphysics simulation and differentiable rendering to jointly model the evolution of scene dynamics and image formation. This novel combination enables backpropagation from pixels in a video sequence through to the underlying physical attributes that generated them. Moreover, our unified computation graph \u2013 spanning from the dynamics and through the rendering process \u2013 enables learning in challenging visuomotor control tasks, without relying on state-based (3D) supervision, while obtaining performance competitive to or better than techniques that rely on precise 3D labels.",
        "authors": [
            "Miles Macklin ",
            "Florian Golemo",
            "Vikram Voleti",
            "Martin Weiss",
            "Kevin Xie",
            "Liam Paull",
            "Florian Shkurti",
            "Derek Nowrouzezahrai",
            "bpc ",
            "deformable solids"
        ],
        "created_time": "18 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/gradSimDifferentiablesimulationforsystemidentificationandvisuomotorcontrol.pdf"
    },
    {
        "internal_id": 656,
        "title": "PLANNING FROM PIXELS USING INVERSE DYNAMICS MODELS",
        "abstract": "Learning task-agnostic dynamics models in high-dimensional observation spaces can be challenging for model-based RL agents. We propose a novel way to learn latent world models by learning to predict sequences of future actions conditioned on task completion. These task-conditioned models adaptively focus modeling capacity on task-relevant dynamics, while simultaneously serving as an effective heuristic for planning with sparse rewards. We evaluate our method on challeng- ing visual goal completion tasks and show a substantial increase in performance compared to prior model-free approaches.",
        "authors": [
            "Keiran Paster"
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/PlanningfromPixelsusingInverseDynamicsModels.pdf"
    },
    {
        "internal_id": 657,
        "title": "NEURAL ODE PROCESSES",
        "abstract": "Neural Ordinary Differential Equations (NODEs) use a neural network to model the instantaneous rate of change in the state of a system. However, despite their apparent suitability for dynamics-governed time-series, NODEs present a few dis- advantages. First, they are unable to adapt to incoming data-points, a fundamental requirement for real-time applications imposed by the natural direction of time. Second, time-series are often composed of a sparse set of measurements that could be explained by many possible underlying dynamics. NODEs do not capture this uncertainty. In contrast, Neural Processes (NPs) are a new class of stochastic processes providing uncertainty estimation and fast data-adaptation, but lack an explicit treatment of the flow of time. To address these problems, we introduce Neural ODE Processes (NDPs), a new class of stochastic processes determined by a distribution over Neural ODEs. By maintaining an adaptive data-dependent dis- tribution over the underlying ODE, we show that our model can successfully cap- ture the dynamics of low-dimensional systems from just a few data-points. At the same time, we demonstrate that NDPs scale up to challenging high-dimensional time-series with unknown latent dynamics such as rotating MNIST digits.",
        "authors": [
            "Cristian Bodnar",
            "Ben Day"
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/NeuralODEProcesses.pdf"
    },
    {
        "internal_id": 658,
        "title": "IS LABEL SMOOTHING TRULY INCOMPATIBLE WITH KNOWLEDGE DISTILLATION: AN EMPIRICAL STUDY",
        "abstract": "This work aims to empirically clarify a recently discovered perspective that label smoothing is incompatible with knowledge distillation (M\u00a8uller et al., 2019). We begin by introducing the motivation behind on how this incompatibility is raised, i.e., label smoothing erases relative information between teacher logits. We provide a novel connection on how label smoothing affects distributions of semantically similar and dissimilar classes. Then we propose a metric to quantitatively measure the degree of erased information in sample's representation. After that, we study its one-sidedness and imperfection of the incompatibility view through massive analyses, visualizations and comprehensive experiments on Image Classification, Binary Networks, and Neural Machine Translation. Finally, we broadly discuss several circumstances wherein label smoothing will indeed lose its effectiveness.1",
        "authors": [
            "Zhiqiang Shen",
            "Zechun Liu",
            "CMU HKUST",
            "Dejia Xu",
            "Zitian Chen",
            "UMass Amherst",
            "Kwang Ting Cheng",
            "Marios Savvides"
        ],
        "created_time": "02 July 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/IsLabelSmoothingTrulyIncompatiblewithKnowledgeDistillationAnEmpiricalStudy.pdf"
    },
    {
        "internal_id": 659,
        "title": "LARGE-WIDTH FUNCTIONAL ASYMPTOTICS FOR DEEP GAUSSIAN NEURAL NETWORKS",
        "abstract": "In this paper, we consider fully-connected feed-forward deep neural networks where weights and biases are independent and identically distributed according to Gaussian distributions. Extending previous results (Matthews et al., 2018a;b; Yang, 2019) we adopt a function-space perspective, i.e. we look at neural networks as infinite-dimensional random elements on the input space RI. Under suitable assumptions on the activation function we show that: i) a network defines a continuous stochastic process on the input space RI; ii) a network with re-scaled weights converges weakly to a continuous Gaussian Process in the large-width limit; iii) the limiting Gaussian Process has almost surely locally \u03b3-H\u00f6lder continuous paths, for 0 < \u03b3 < 1. Our results contribute to recent theoretical studies on the interplay between infinitely-wide deep neural networks and Gaussian Processes by establishing weak convergence in function-space with respect to a stronger metric.",
        "authors": [
            "Daniele Bracale",
            "Stefano Favaro",
            "Stefano Peluchetti"
        ],
        "created_time": "20 February 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/LargewidthfunctionalasymptoticsfordeepGaussianneuralnetworks.pdf"
    },
    {
        "internal_id": 660,
        "title": "DYNATUNE: DYNAMIC TENSOR PROGRAM OPTI- MIZATION IN DEEP NEURAL NETWORK COMPILATION",
        "abstract": "Recently, the DL compiler, together with Learning to Compile has proven to be a powerful technique for optimizing deep learning models. However, existing meth- ods focus on accelerating the convergence speed of the individual tensor operator rather than the convergence speed of the entire model, which results in long opti- mization time to obtain a desired latency. In this paper, we present a new method called DynaTune, which provides significantly faster convergence speed to op- timize a DNN model. In particular, we consider a Multi-Armed Bandit (MAB) model for the tensor program optimization problem. We use UCB to handle the decision-making of time-slot-based optimization, and we devise a Bayesian belief model that allows predicting the potential performance gain of each operator with uncertainty quantification, which guides the optimization process. We evaluate and compare DynaTune with the state-of-the-art DL compiler. The experiment re- sults show that DynaTune is 1.2\u20132.4 times faster to achieve the same optimization quality for a range of models across different hardware architectures.",
        "authors": [
            "Minjia Zhang",
            "Menghao Li",
            "Microsoft Corporation"
        ],
        "created_time": "16 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/DynaTuneDynamicTensorProgramOptimizationinDeepNeuralNetworkCompilation.pdf"
    },
    {
        "internal_id": 661,
        "title": "LEARNING A LATENT SEARCH SPACE FOR ROUTING PROBLEMS USING VARIATIONAL AUTOENCODERS",
        "abstract": "Methods for automatically learning to solve routing problems are rapidly improv- ing in performance. While most of these methods excel at generating solutions quickly, they are unable to effectively utilize longer run times because they lack a sophisticated search component. We present a learning-based optimization ap- proach that allows a guided search in the distribution of high-quality solutions for a problem instance. More precisely, our method uses a conditional variational autoencoder that learns to map points in a continuous (latent) search space to high- quality, instance-specific routing problem solutions. The learned space can then be searched by any unconstrained continuous optimization method. We show that even using a standard differential evolution search strategy our approach is able to outperform existing purely machine learning based approaches.",
        "authors": [
            "Kevin Tierney"
        ],
        "created_time": "16 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/LearningaLatentSearchSpaceforRoutingProblemsusingVariationalAutoencoders.pdf"
    },
    {
        "internal_id": 662,
        "title": "IDENTIFYING PHYSICAL LAW OF HAMILTONIAN SYS- TEMS VIA META-LEARNING",
        "abstract": "Hamiltonian mechanics is an effective tool to represent many physical processes with concise yet well-generalized mathematical expressions. A well-modeled Hamiltonian makes it easy for researchers to analyze and forecast many related phenomena that are governed by the same physical law. However, in general, identifying a functional or shared expression of the Hamiltonian is very difficult. It requires carefully designed experiments and the researcher's insight that comes from years of experience. We propose that meta-learning algorithms can be poten- tially powerful data-driven tools for identifying the physical law governing Hamil- tonian systems without any mathematical assumptions on the representation, but with observations from a set of systems governed by the same physical law. We show that a well meta-trained learner can identify the shared representation of the Hamiltonian by evaluating our method on several types of physical systems with various experimental settings.",
        "authors": [
            "Seungjun Lee",
            "Haesang Yang",
            "Woojae Seong"
        ],
        "created_time": "03 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/IdentifyingPhysicalLawofHamiltonianSystemsviaMetaLearning.pdf"
    },
    {
        "internal_id": 663,
        "title": "CREATIVE SKETCH GENERATION",
        "abstract": "Sketching or doodling is a popular creative activity that people engage in. How- ever, most existing work in automatic sketch understanding or generation has focused on sketches that are quite mundane. In this work, we introduce two datasets of creative sketches \u2013 Creative Birds and Creative Creatures \u2013 contain- ing 10k sketches each along with part annotations. We propose DoodlerGAN \u2013 a part-based Generative Adversarial Network (GAN) \u2013 to generate unseen com- positions of novel part appearances. Quantitative evaluations as well as human studies demonstrate that sketches generated by our approach are more creative and of higher quality than existing approaches. In fact, in Creative Birds, subjects prefer sketches generated by DoodlerGAN over those drawn by humans!",
        "authors": [
            "Songwei Ge",
            "Devi Parikh"
        ],
        "created_time": "23 February 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/CreativeSketchGeneration.pdf"
    },
    {
        "internal_id": 664,
        "title": "EARLY STOPPING IN DEEP NETWORKS: DOUBLE DE- SCENT AND HOW TO ELIMINATE IT",
        "abstract": "Over-parameterized models, such as large deep networks, often exhibit a double descent phenomenon, where as a function of model size, error first decreases, in- creases, and decreases at last. This intriguing double descent behavior also occurs as a function of training epochs and has been conjectured to arise because training epochs control the model complexity. In this paper, we show that such epoch-wise double descent occurs for a different reason: It is caused by a superposition of two or more bias-variance tradeoffs that arise because different parts of the network are learned at different epochs, and mitigating this by proper scaling of stepsizes can significantly improve the early stopping performance. We show this analytically for i) linear regression, where differently scaled features give rise to a superposi- tion of bias-variance tradeoffs, and for ii) a wide two-layer neural network, where the first and second layers govern bias-variance tradeoffs. Inspired by this theory, we study two standard convolutional networks empirically and show that elimi- nating epoch-wise double descent through adjusting stepsizes of different layers improves the early stopping performance.",
        "authors": [
            "Reinhard Heckel",
            "Fatih Furkan Yilmaz"
        ],
        "created_time": "15 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/EarlyStoppinginDeepNetworksDoubleDescentandHowtoEliminateit.pdf"
    },
    {
        "internal_id": 665,
        "title": "ON GRAPH NEURAL NETWORKS VERSUS GRAPH- AUGMENTED MLPS",
        "abstract": "From the perspectives of expressive power and learning, this work compares multi-layer Graph Neural Networks (GNNs) with a simplified alternative that we call Graph-Augmented Multi-Layer Perceptrons (GA-MLPs), which first aug- ments node features with certain multi-hop operators on the graph and then applies learnable node-wise functions. From the perspective of graph isomorphism test- ing, we show both theoretically and numerically that GA-MLPs with suitable op- erators can distinguish almost all non-isomorphic graphs, just like the Weisfeiler- Lehman (WL) test and GNNs. However, by viewing them as node-level functions and examining the equivalence classes they induce on rooted graphs, we prove a separation in expressive power between GA-MLPs and GNNs that grows expo- nentially in depth. In particular, unlike GNNs, GA-MLPs are unable to count the number of attributed walks. We also demonstrate via community detection exper- iments that GA-MLPs can be limited by their choice of operator family, whereas GNNs have higher flexibility in learning.",
        "authors": [
            "Lei Chen",
            "Zhengdao Chen",
            "New York",
            "Joan Bruna",
            "New York"
        ],
        "created_time": "16 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/OnGraphNeuralNetworksversusGraphAugmentedMLPs.pdf"
    },
    {
        "internal_id": 666,
        "title": "TRAINING GANS WITH STRONGER AUGMENTATIONS VIA CONTRASTIVE DISCRIMINATOR",
        "abstract": "Recent works in Generative Adversarial Networks (GANs) are actively revisiting various data augmentation techniques as an effective way to prevent discriminator overfitting. It is still unclear, however, that which augmentations could actually improve GANs, and in particular, how to apply a wider range of augmentations in training. In this paper, we propose a novel way to address these questions by incorporating a recent contrastive representation learning scheme into the GAN discriminator, coined ContraD. This \"fusion\" enables the discriminators to work with much stronger augmentations without increasing their training instability, thereby preventing the discriminator overfitting issue in GANs more effectively. Even better, we observe that the contrastive learning itself also benefits from our GAN training, i.e., by maintaining discriminative features between real and fake samples, suggesting a strong coherence between the two worlds: good contrastive representations are also good for GAN discriminators, and vice versa. Our ex- perimental results show that GANs with ContraD consistently improve FID and IS compared to other recent techniques incorporating data augmentations, still maintaining highly discriminative features in the discriminator in terms of the lin- ear evaluation. Finally, as a byproduct, we also show that our GANs trained in an unsupervised manner (without labels) can induce many conditional generative models via a simple latent sampling, leveraging the learned features of ContraD. Code is available at https://github.com/jh-jeong/ContraD.",
        "authors": [
            "Daejeon "
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/TrainingGANswithStrongerAugmentationsviaContrastiveDiscriminator.pdf"
    },
    {
        "internal_id": 667,
        "title": "GENERALIZED VARIATIONAL CONTINUAL LEARNING",
        "abstract": "Continual learning deals with training models on new tasks and datasets in an on- line fashion. One strand of research has used probabilistic regularization for con- tinual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modifi- cation to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspi- ration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers out- performs or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.",
        "authors": [
            "Noel Loo"
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/GeneralizedVariationalContinualLearning.pdf"
    },
    {
        "internal_id": 668,
        "title": "DEEP NEURAL TANGENT KERNEL AND LAPLACE KERNEL HAVE THE SAME RKHS",
        "abstract": "We prove that the reproducing kernel Hilbert spaces (RKHS) of a deep neural tangent kernel and the Laplace kernel include the same set of functions, when both kernels are restricted to the sphere Sd\u22121. Additionally, we prove that the exponential power kernel with a smaller power (making the kernel less smooth) leads to a larger RKHS, when it is restricted to the sphere Sd\u22121 and when it is defined on the entire Rd.",
        "authors": [
            "Lin Chen",
            "Sheng Xu"
        ],
        "created_time": "18 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/DeepNeuralTangentKernelandLaplaceKernelHavetheSameRKHS.pdf"
    },
    {
        "internal_id": 669,
        "title": "CALIBRATION TESTS BEYOND CLASSIFICATION",
        "abstract": "Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point esti- mates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. It applies to any such model, including classifica- tion and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. In par- ticular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression problems.1",
        "authors": [
            "David Widmann",
            "Fredrik Lindsten",
            "Dave Zachariah"
        ],
        "created_time": "01 January 1980",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/Calibrationtestsbeyondclassification.pdf"
    },
    {
        "internal_id": 670,
        "title": "CROSS-ATTENTIONAL AUDIO-VISUAL FUSION FOR WEAKLY-SUPERVISED ACTION LOCALIZATION",
        "abstract": "Temporally localizing actions in videos is one of the key components for video understanding. Learning from weakly-labeled data is seen as a potential solu- tion towards avoiding expensive frame-level annotations. Different from other works which only depend on visual-modality, we propose to learn richer audio- visual representation for weakly-supervised action localization. First, we propose a multi-stage cross-attention mechanism to collaboratively fuse audio and visual features, which preserves the intra-modal characteristics. Second, to model both foreground and background frames, we construct an open-max classifier which treats the background class as an open-set. Third, for precise action localiza- tion, we design consistency losses to enforce temporal continuity for the action- class prediction, and also help with foreground-prediction reliability. Extensive experiments on two publicly available video-datasets (AVE and ActivityNet1.2) show that the proposed method effectively fuses audio and visual modalities, and achieves the state-of-the-art results for weakly-supervised action localization.",
        "authors": [
            "Juntae Lee",
            "Mihir Jain"
        ],
        "created_time": "10 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/CrossAttentionalAudioVisualFusionforWeaklySupervisedActionLocalization.pdf"
    },
    {
        "internal_id": 671,
        "title": "REMEMBERING FOR THE RIGHT REASONS: EXPLANATIONS REDUCE CATASTROPHIC FORGETTING",
        "abstract": "The goal of continual learning (CL) is to learn a sequence of tasks without suf- fering from the phenomenon of catastrophic forgetting. Previous work has shown that leveraging memory in the form of a replay buffer can reduce performance degradation on prior tasks. We hypothesize that forgetting can be further reduced when the model is encouraged to remember the evidence for previously made decisions. As a first step towards exploring this hypothesis, we propose a sim- ple novel training paradigm, called Remembering for the Right Reasons (RRR), that additionally stores visual model explanations for each example in the buffer and ensures the model has \"the right reasons\" for its predictions by encourag- ing its explanations to remain consistent with those used to make decisions at training time. Without this constraint, there is a drift in explanations and in- crease in forgetting as conventional continual learning algorithms learn new tasks. We demonstrate how RRR can be easily added to any memory or regularization- based approach and results in reduced forgetting, and more importantly, improved model explanations. We have evaluated our approach in the standard and few-shot settings and observed a consistent improvement across various CL approaches using different architectures and techniques to generate model explanations and demonstrated our approach showing a promising connection between explainabil- ity and continual learning. Our code is available at https://github.com/ SaynaEbrahimi/Remembering-for-the-Right-Reasons.",
        "authors": [
            "Sayna Ebrahimi",
            "Suzanne Petryk",
            "Akash Gokul",
            "William Gan",
            "Joseph E Gonzalez",
            "Marcus Rohrbach",
            "Trevor Darrell"
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/RememberingfortheRightReasonsExplanationsReduceCatastrophicForgetting.pdf"
    },
    {
        "internal_id": 672,
        "title": "HYPERDYNAMICS: META-LEARNING OBJECT AND AGENT DYNAMICS WITH HYPERNETWORKS",
        "abstract": "We propose HyperDynamics, a dynamics meta-learning framework that condi- tions on an agent's interactions with the environment and optionally its visual observations, and generates the parameters of neural dynamics models based on inferred properties of the dynamical system. Physical and visual properties of the environment that are not part of the low-dimensional state yet affect its temporal dynamics are inferred from the interaction history and visual observations, and are implicitly captured in the generated parameters. We test HyperDynamics on a set of object pushing and locomotion tasks. It outperforms existing dynamics models in the literature that adapt to environment variations by learning dynamics over high dimensional visual observations, capturing the interactions of the agent in re- current state representations, or using gradient-based meta-optimization. We also show our method matches the performance of an ensemble of separately trained experts, while also being able to generalize well to unseen environment variations at test time. We attribute its good performance to the multiplicative interactions between the inferred system properties\u2014captured in the generated parameters\u2014 and the low-dimensional state representation of the dynamical system.",
        "authors": [
            "Zhou Xian",
            "Shamit Lal",
            "PA "
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/HyperDynamicsMetaLearningObjectandAgentDynamicswithHypernetworks.pdf"
    },
    {
        "internal_id": 673,
        "title": "SELF-SUPERVISED ADVERSARIAL ROBUSTNESS FOR THE LOW-LABEL, HIGH-DATA REGIME",
        "abstract": "Recent work discovered that training models to be invariant to adversarial per- turbations requires substantially larger datasets than those required for standard classification. Perhaps more surprisingly, these larger datasets can be \"mostly\" unlabeled. Pseudo-labeling, a technique simultaneously pioneered by four separate and simultaneous works in 2019, has been proposed as a competitive alternative to labeled data for training adversarially robust models. However, when the amount of labeled data decreases, the performance of pseudo-labeling catastrophically drops, thus questioning the theoretical insights put forward by Uesato et al. (2019), which suggest that the sample complexity for learning an adversarially robust model from unlabeled data should match the fully supervised case. We introduce Bootstrap Your Own Robust Latents (BYORL), a self-supervised learning technique based on BYOL for training adversarially robust models. Our method enables us to train robust representations without any labels (reconciling practice with theory). Most notably, this robust representation can be leveraged by a linear classifier to train adversarially robust models, even when the linear classifier is not trained adversari- ally. We evaluate BYORL and pseudo-labeling on CIFAR-10 and IMAGENET and demonstrate that BYORL achieves significantly higher robustness in the low-label regime (i.e., models resulting from BYORL are up to two times more accurate). Experiments on CIFAR-10 against \u21132 and \u2113\u221e norm-bounded perturbations demon- strate that BYORL achieves near state-of-the-art robustness with as little as 500 labeled examples. We also note that against \u21132 norm-bounded perturbations of size \u03f5 = 128/255, BYORL surpasses the known state-of-the-art with an accuracy under attack of 77.61% (against 72.91% for the prior art).",
        "authors": [
            "Sven Gowal",
            "Po Sen Huang"
        ],
        "created_time": "15 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/SelfsupervisedAdversarialRobustnessfortheLowlabelHighdataRegime.pdf"
    },
    {
        "internal_id": 674,
        "title": "MODELLING HIERARCHICAL STRUCTURE BETWEEN DIALOGUE POLICY AND NATURAL LANGUAGE GEN- ERATOR WITH OPTION FRAMEWORK FOR TASK- ORIENTED DIALOGUE SYSTEM",
        "abstract": "Designing task-oriented dialogue systems is a challenging research topic, since it needs not only to generate utterances fulfilling user requests but also to guaran- tee the comprehensibility. Many previous works trained end-to-end (E2E) models with supervised learning (SL), however, the bias in annotated system utterances remains as a bottleneck. Reinforcement learning (RL) deals with the problem through using non-differentiable evaluation metrics (e.g., the success rate) as re- wards. Nonetheless, existing works with RL showed that the comprehensibility of generated system utterances could be corrupted when improving the performance on fulfilling user requests. In our work, we (1) propose modelling the hierarchical structure between dialogue policy and natural language generator (NLG) with the option framework, called HDNO, where the latent dialogue act is applied to avoid designing specific dialogue act representations; (2) train HDNO via hierarchical reinforcement learning (HRL), as well as suggest the asynchronous updates be- tween dialogue policy and NLG during training to theoretically guarantee their convergence to a local maximizer; and (3) propose using a discriminator mod- elled with language models as an additional reward to further improve the com- prehensibility. We test HDNO on MultiWoz 2.0 and MultiWoz 2.1, the datasets on multi-domain dialogues, in comparison with word-level E2E model trained with RL, LaRL and HDSA, showing improvements on the performance evaluated by automatic evaluation metrics and human evaluation. Finally, we demonstrate the semantic meanings of latent dialogue acts to show the explanability for HDNO.",
        "authors": [
            "Jianhong Wang",
            "Yuan Zhang",
            "Tae Kyun Kim",
            "Yunjie Gu"
        ],
        "created_time": "19 February 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/ModellingHierarchicalStructurebetweenDialoguePolicyandNaturalLanguageGeneratorwithOptionFrameworkforTaskorientedDialogueSystem.pdf"
    },
    {
        "internal_id": 675,
        "title": "C-LEARNING: HORIZON-AWARE CUMULATIVE ACCESSIBILITY ESTIMATION",
        "abstract": "Multi-goal reaching is an important problem in reinforcement learning needed to achieve algorithmic generalization. Despite recent advances in this field, current algorithms suffer from three major challenges: high sample complexity, learning only a single way of reaching the goals, and difficulties in solving complex motion planning tasks. In order to address these limitations, we introduce the concept of cumulative accessibility functions, which measure the reachability of a goal from a given state within a specified horizon. We show that these functions obey a re- currence relation, which enables learning from offline interactions. We also prove that optimal cumulative accessibility functions are monotonic in the planning hori- zon. Additionally, our method can trade off speed and reliability in goal-reaching by suggesting multiple paths to a single goal depending on the provided horizon. We evaluate our approach on a set of multi-goal discrete and continuous con- trol tasks. We show that our method outperforms state-of-the-art goal-reaching algorithms in success rate, sample complexity, and path optimality. Our code is available at https://github.com/layer6ai-labs/CAE, and addi- tional visualizations can be found at https://sites.google.com/view/ learning-cae/.",
        "authors": [
            "Panteha Naderian",
            "Gabriel Loaiza Ganem",
            "Harry J Braviner",
            "Anthony L Caterini",
            "Layer  AI",
            "Animesh Garg"
        ],
        "created_time": "25 January 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/CLearningHorizonAwareCumulativeAccessibilityEstimation.pdf"
    },
    {
        "internal_id": 676,
        "title": "MULTI-RESOLUTION MODELING OF A DISCRETE STOCHASTIC PROCESS IDENTIFIES CAUSES OF CANCER",
        "abstract": "Detection of cancer-causing mutations within the vast and mostly unexplored hu- man genome is a major challenge. Doing so requires modeling the background mutation rate, a highly non-stationary stochastic process, across regions of interest varying in size from one to millions of positions. Here, we present the split- Poisson-Gamma (SPG) distribution, an extension of the classical Poisson-Gamma formulation, to model a discrete stochastic process at multiple resolutions. We demonstrate that the probability model has a closed-form posterior, enabling effi- cient and accurate linear-time prediction over any length scale after the parameters of the model have been inferred a single time. We apply our framework to model mutation rates in tumors and show that model parameters can be accurately in- ferred from high-dimensional epigenetic data using a convolutional neural network, Gaussian process, and maximum-likelihood estimation. Our method is both more accurate and more efficient than existing models over a large range of length scales. We demonstrate the usefulness of multi-resolution modeling by detecting genomic elements that drive tumor emergence and are of vastly differing sizes.",
        "authors": [
            "Adam Yaari",
            "Maxwell Sherman",
            "Oliver Priebe",
            "Po Ru Loh",
            "Boris Katz",
            "Bonnie Berger",
            " MIT CSAIL",
            " MIT CBMM"
        ],
        "created_time": "21 April 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/Multiresolutionmodelingofadiscretestochasticprocessidentifiescausesofcancer.pdf"
    },
    {
        "internal_id": 677,
        "title": "ON THE UNIVERSALITY OF THE DOUBLE DESCENT PEAK IN RIDGELESS REGRESSION",
        "abstract": "We prove a non-asymptotic distribution-independent lower bound for the expected mean squared generalization error caused by label noise in ridgeless linear regres- sion. Our lower bound generalizes a similar known result to the overparameterized (interpolating) regime. In contrast to most previous works, our analysis applies to a broad class of input distributions with almost surely full-rank feature matrices, which allows us to cover various types of deterministic or random feature maps. Our lower bound is asymptotically sharp and implies that in the presence of label noise, ridgeless linear regression does not perform well around the interpolation threshold for any of these feature maps. We analyze the imposed assumptions in detail and provide a theory for analytic (random) feature maps. Using this the- ory, we can show that our assumptions are satisfied for input distributions with a (Lebesgue) density and feature maps given by random deep neural networks with analytic activation functions like sigmoid, tanh, softplus or GELU. As further ex- amples, we show that feature maps from random Fourier features and polynomial kernels also satisfy our assumptions. We complement our theory with further ex- perimental and analytic results.",
        "authors": [
            "David Holzm ller"
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/OntheUniversalityoftheDoubleDescentPeakinRidgelessRegression.pdf"
    },
    {
        "internal_id": 678,
        "title": "NEGATIVE DATA AUGMENTATION",
        "abstract": "Data augmentation is often used to enlarge datasets with synthetic samples gen- erated in accordance with the underlying data distribution. To enable a wider range of augmentations, we explore negative data augmentation strategies (NDA) that intentionally create out-of-distribution samples. We show that such negative out-of-distribution samples provide information on the support of the data distri- bution, and can be leveraged for generative modeling and representation learning. We introduce a new GAN training objective where we use NDA as an additional source of synthetic data for the discriminator. We prove that under suitable con- ditions, optimizing the resulting objective still recovers the true data distribution but can directly bias the generator towards avoiding samples that lack the desired structure. Empirically, models trained with our method achieve improved con- ditional/unconditional image generation along with improved anomaly detection capabilities. Further, we incorporate the same negative data augmentation strategy in a contrastive learning framework for self-supervised representation learning on images and videos, achieving improved performance on downstream image clas- sification, object detection, and action recognition tasks. These results suggest that prior knowledge on what does not constitute valid data is an effective form of weak supervision across a range of unsupervised learning tasks.",
        "authors": [
            "Abhishek Sinha",
            "Kumar Ayush",
            "Jiaming Song",
            "Burak Uzkent",
            "Hongxia Jin",
            "Stefano Ermon"
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/NegativeDataAugmentation.pdf"
    },
    {
        "internal_id": 679,
        "title": "GRAPPA: GRAMMAR-AUGMENTED PRE-TRAINING FOR TABLE SEMANTIC PARSING",
        "abstract": "We present GRAPPA, an effective pre-training approach for table semantic parsing that learns a compositional inductive bias in the joint representations of textual and tabular data. We construct synthetic question-SQL pairs over high-quality tables via a synchronous context-free grammar (SCFG). We pre-train GRAPPA on the synthetic data to inject important structural properties commonly found in table semantic parsing into the pre-training language model. To maintain the model's ability to represent real-world data, we also include masked language modeling (MLM) on several existing table-and-language datasets to regularize our pre-training process. Our proposed pre-training strategy is much data-efficient. When incorporated with strong base semantic parsers, GRAPPA achieves new state-of-the-art results on four popular fully supervised and weakly supervised table semantic parsing tasks. The pre-trained embeddings can be downloaded at https://huggingface.co/Salesforce/grappa_large_jnt.",
        "authors": [
            "Tao Yu",
            "Chien Sheng Wu",
            "Xi Victoria Lin",
            "Bailin Wang",
            "Yi Chern Tan",
            "Xinyi Yang",
            "Dragomir Radev",
            "Richard Socher",
            "Caiming Xiong"
        ],
        "created_time": "18 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/GraPPaGrammarAugmentedPreTrainingforTableSemanticParsing.pdf"
    },
    {
        "internal_id": 680,
        "title": "PARAMETER EFFICIENT MULTIMODAL TRANSFORM- ERS FOR VIDEO REPRESENTATION LEARNING",
        "abstract": "The recent success of Transformers in the language domain has motivated adapting it to a multimodal setting, where a new visual model is trained in tandem with an already pretrained language model. However, due to the excessive memory requirements from Transformers, existing work typically fixes the language model and train only the vision module, which limits its ability to learn cross-modal infor- mation in an end-to-end manner. In this work, we focus on reducing the parameters of multimodal Transformers in the context of audio-visual video representation learning. We alleviate the high memory requirement by sharing the parameters of Transformers across layers and modalities; we decompose the Transformer into modality-specific and modality-shared parts so that the model learns the dynamics of each modality both individually and together, and propose a novel parameter sharing scheme based on low-rank approximation. We show that our approach reduces parameters of the Transformers up to 97%, allowing us to train our model end-to-end from scratch. We also propose a negative sampling approach based on an instance similarity measured on the CNN embedding space that our model learns together with the Transformers. To demonstrate our approach, we pretrain our model on 30-second clips (480 frames) from Kinetics-700 and transfer it to audio-visual classification tasks.",
        "authors": [
            "Sangho Lee",
            "Youngjae Yu",
            "Gunhee Kim",
            "Thomas Breuel",
            "Jan Kautz",
            "Yale Song"
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/ParameterEfficientMultimodalTransformersforVideoRepresentationLearning.pdf"
    },
    {
        "internal_id": 681,
        "title": "ESTIMATING INFORMATIVENESS OF SAMPLES WITH SMOOTH UNIQUE INFORMATION",
        "abstract": "We define a notion of information that an individual sample provides to the train- ing of a neural network, and we specialize it to measure both how much a sam- ple informs the final weights and how much it informs the function computed by the weights. Though related, we show that these quantities have a qualita- tively different behavior. We give efficient approximations of these quantities using a linearized network and demonstrate empirically that the approximation is accurate for real-world architectures, such as pre-trained ResNets. We apply these measures to several problems, such as dataset summarization, analysis of under-sampled classes, comparison of informativeness of different data sources, and detection of adversarial and corrupted examples. Our work generalizes ex- isting frameworks but enjoys better computational properties for heavily over- parametrized models, which makes it possible to apply it to real-world networks.",
        "authors": [
            "Hrayr Harutyunyan",
            "Giovanni Paolini",
            "Orchid Majumder",
            "Rahul Bhotika",
            "Stefano Soatto"
        ],
        "created_time": "17 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/EstimatinginformativenessofsampleswithSmoothUniqueInformation.pdf"
    },
    {
        "internal_id": 682,
        "title": "A TEMPORAL KERNEL APPROACH FOR DEEP LEARN- ING WITH CONTINUOUS-TIME INFORMATION",
        "abstract": "Sequential deep learning models such as RNN, causal CNN and attention mech- anism do not readily consume continuous-time information. Discretizing the temporal data, as we show, causes inconsistency even for simple continuous-time processes. Current approaches often handle time in a heuristic manner to be con- sistent with the existing deep learning architectures and implementations. In this paper, we provide a principled way to characterize continuous-time systems using deep learning tools. Notably, the proposed approach applies to all the major deep learning architectures and requires little modifications to the implementation. The critical insight is to represent the continuous-time system by composing neural networks with a temporal kernel, where we gain our intuition from the recent advancements in understanding deep learning with Gaussian process and neural tangent kernel. To represent the temporal kernel, we introduce the random feature approach and convert the kernel learning problem to spectral density estimation under reparameterization. We further prove the convergence and consistency re- sults even when the temporal kernel is non-stationary, and the spectral density is misspecified. The simulations and real-data experiments demonstrate the empirical effectiveness of our temporal kernel approach in a broad range of settings.",
        "authors": [
            "Da Xu",
            "CA ",
            "Chuanwei Ruan",
            "San Francisco",
            "CA ",
            "Evren Korpeoglu",
            "Sushant Kuamr",
            "Kannan Achan",
            "CA "
        ],
        "created_time": "18 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/ATemporalKernelApproachforDeepLearningwithContinuoustimeInformation.pdf"
    },
    {
        "internal_id": 683,
        "title": "UNSUPERVISED META-LEARNING THROUGH LATENT- SPACE INTERPOLATION IN GENERATIVE MODELS",
        "abstract": "Several recently proposed unsupervised meta-learning approaches rely on synthetic meta-tasks created using techniques such as random selection, clustering and/or augmentation. In this work, we describe a novel approach that generates meta- tasks using generative models. The proposed family of algorithms generate pairs of in-class and out-of-class samples from the latent space in a principled way, allowing us to create synthetic classes forming the training and validation data of a meta-task. We find that the proposed approach, LAtent Space Interpolation Unsupervised Meta-learning (LASIUM), outperforms or is competitive with current unsupervised learning baselines on few-shot classification tasks on the most widely used benchmark datasets.",
        "authors": [
            "Siavash Khodadadeh",
            "Sharare Zehtabian",
            "Saeed Vahidian",
            "Weijia Wang",
            "Bill Lin"
        ],
        "created_time": "16 March 2021",
        "conference": "ICLR",
        "filepath": "/Users/adit/papers/ICLR/UnsupervisedMetaLearningthroughLatentSpaceInterpolationinGenerativeModels.pdf"
    }
]