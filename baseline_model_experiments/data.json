{
    "000feefa7f77907933e5b29fe7e2cd5ff6410418": {
      "paper_id": "000feefa7f77907933e5b29fe7e2cd5ff6410418",
      "abstract": "Modeling the purposeful behavior of imperfect agents from a small number of observations is a challenging task. When restricted to the single-agent decision-theoretic setting, inverse optimal control techniques assume that observed behavior is an approximately optimal solution to an unknown decision problem. These techniques learn a utility function that explains the example behavior and can then be used to accurately predict or imitate future behavior in similar observed or unobserved situations. \nIn this work, we consider similar tasks in competitive and cooperative multi-agent domains. Here, unlike single-agent settings, a player cannot myopically maximize its reward; it must speculate on how the other agents may act to influence the game's outcome. Employing the game-theoretic notion of regret and the principle of maximum entropy, we introduce a technique for predicting and generalizing behavior.",
      "title": "Computational Rationalization: The Inverse Equilibrium Problem"
    },
    "00105a98161f98000cefd1880f39fc005319ec33": {
      "paper_id": "00105a98161f98000cefd1880f39fc005319ec33",
      "abstract": "Abstract : This paper surveys the application of artificial intelligence approaches to the software engineering processes. These approaches can have a major impact on reducing the time to market and improving the quality of software systems in general. Existing survey papers are driven by the AI techniques used, or are focused on specific software engineering processes. This paper relates AI techniques to software engineering processes specified by the IEEE 12207 standard of software engineering. The paper is driven by the activities and tasks specified in the standard for each software engineering process. The paper brings the state of the art of AI techniques closer to the software engineer, and highlights the open research problems for the research community. Keywords: Automated Software Engineering, Artificial Intelligence Techniques. 1. Introduction 2. The software intensive systems we develop these days are becoming much more complex in terms of the number of functional and nonfunctional requirements they need to support. The impact of low quality can also have a catastrophic impact on the mission of these systems in many critical applications. Moreover, the cost of software development dominates the total cost of such systems. Research in applying artificial intelligence techniques to software Engineering have grown tremendously in the last two decades producing a large number of projects and publications. A number of conferences and journals are dedicated to publish the research in this field. The AI techniques are proposed in order to reduce the time to market and enhance the quality of software systems. Yet many of these AI techniques remain largely used by the research community and with little impact on the processes and tools used by the practicing software engineer. The recent survey papers published in this field are mainly targeted to the research community. They are driven by the specific AI techniques used rather than the software engineering activities supported. They are also focused on a specific software engineering process such as software design [28] This survey paper attempts to close the gap between the research and practice of applying AI techniques to the software engineering processes. It also highlights open practical problems to the research community in applying such techniques by surveying the recently proposed work in this area. We use the terminology and the processes defined by the IEEE 12207 standard of software engineering. We then map the current state art of AI art techniques proposed in the literature to specific tasks and activities of some of the software processes define by the 12207 standard. These AI techniques attempt to automate or semi-automate these tasks and produce optimal or semi-optimal solutions in much less time. . The paper is organized as follows. In section 2, we give an overview of the IEEE 12207 standard of software engineering, and describe the most important AI techniques. We survey the current AI techniques proposed for the primary processes of development in sections 3. We highlight the open problems in section 4.",
      "title": "Software Engineering Using Artificial Intelligence Techniques: Current State and Open Problems"
    },
    "00111610254bfb8ec16428501c2ca68dcf817474": {
      "paper_id": "00111610254bfb8ec16428501c2ca68dcf817474",
      "abstract": "We introduce the social study of bullying to the NLP community. Bullying, in both physical and cyber worlds (the latter known as cyberbullying), has been recognized as a serious national health issue among adolescents. However, previous social studies of bullying are handicapped by data scarcity, while the few computational studies narrowly restrict themselves to cyberbullying which accounts for only a small fraction of all bullying episodes. Our main contribution is to present evidence that social media, with appropriate natural language processing techniques, can be a valuable and abundant data source for the study of bullying in both worlds. We identify several key problems in using such data sources and formulate them as NLP tasks, including text classification, role labeling, sentiment analysis, and topic modeling. Since this is an introductory paper, we present baseline results on these tasks using off-the-shelf NLP solutions, and encourage the NLP community to contribute better models in the future.",
      "title": "Learning from Bullying Traces in Social Media"
    },
    "0000fcfd467a19cf0e59169c2f07d730a0f3a8b9": {
      "paper_id": "0000fcfd467a19cf0e59169c2f07d730a0f3a8b9",
      "abstract": "It is always well believed that modeling relationships between objects would be helpful for representing and eventually describing an image. Nevertheless, there has not been evidence in support of the idea on image description generation. In this paper, we introduce a new design to explore the connections between objects for image captioning under the umbrella of attention-based encoder-decoder framework. Specifically, we present Graph Convolutional Networks plus Long Short-Term Memory (dubbed as GCN-LSTM) architecture that novelly integrates both semantic and spatial object relationships into image encoder. Technically, we build graphs over the detected objects in an image based on their spatial and semantic connections. The representations of each region proposed on objects are then refined by leveraging graph structure through GCN. With the learnt region-level features, our GCN-LSTM capitalizes on LSTM-based captioning framework with attention mechanism for sentence generation. Extensive experiments are conducted on COCO image captioning dataset, and superior results are reported when comparing to state-of-the-art approaches. More remarkably, GCN-LSTM increases CIDEr-D performance from 120.1% to 128.7% on COCO testing set.",
      "title": "Exploring Visual Relationship for Image Captioning"
    },
    "0001c638511772a45944ca7cc8bc68a6380f4544": {
      "paper_id": "0001c638511772a45944ca7cc8bc68a6380f4544",
      "abstract": "Operators of online social networks are increasingly sharing potentially sensitive information about users and their relationships with advertisers, application developers, and data-mining researchers. Privacy is typically protected by anonymization, i.e., removing names, addresses, etc.We present a framework for analyzing privacy and anonymity in social networks and develop a new re-identification algorithm targeting anonymized social-network graphs. To demonstrate its effectiveness on real-world networks, we show that a third of the users who can be verified to have accounts on both Twitter, a popular microblogging service, and Flickr, an online photo-sharing site, can be re-identified in the anonymous Twitter graph with only a 12% error rate.Our de-anonymization algorithm is based purely on the network topology, does not require creation of a large number of dummy \"sybil\" nodes, is robust to noise and all existing defenses, and works even when the overlap between the target network and the adversary's auxiliary information is small.",
      "title": "De-anonymizing Social Networks"
    },
    "0003adcd7bfb44cf32f2aa45301ecb5d8a56c487": {
      "paper_id": "0003adcd7bfb44cf32f2aa45301ecb5d8a56c487",
      "abstract": "This paper describes the voltage doubler (VD) behavior of a 2.45-GHz VD-type rectenna. An important feature of the developed VD-type rectenna is the absence of the charge pump capacitor that is present in conventional VDs. Based on the circuit simulation and measurement results, the developed VD-type rectenna shows drastic improvement to the radio frequency-to-direct current (RF\u2013dc) conversion efficiency in comparison with a half-wave rectifier (HWR)-type rectenna at output loads exceeding  $100~\\Omega $ . From the simulation, both the VD- and HWR-type rectennas show VD behavior, even without the charge pump capacitor. This behavior was found to be attributable to the electric charge stored in the junction and package capacitances of the Schottky barrier diode in the rectennas. Additionally, it was found that the improvement of the RF\u2013dc conversion efficiency is caused by the voltage conservation in the series diode. This improvement is dependent on the microwave period and the electric discharge time constant. When the microwave period is comparable with the electric discharge time constant, the VD-type rectenna conserves a dc voltage even when the input microwave voltage is negative. These phenomena were observed by analyzing the voltage and current waveforms of the rectenna equivalent circuit obtained via circuit simulation.",
      "title": "Analysis of Voltage Doubler Behavior of 2.45-GHz Voltage Doubler-Type Rectenna"
    },
    "0007181efc556fd1fcda2642e9bd85dd0f0c32d6": {
      "paper_id": "0007181efc556fd1fcda2642e9bd85dd0f0c32d6",
      "abstract": "Routers must perform packet classification at high speeds to efficiently implement functions such as firewalls and QoS routing. Packet classification requires matching each packet against a database of filters (or rules), and forwarding the packet according to the highest priority filter. Existing filter schemes with fast lookup time do not scale to large filter databases. Other more scalable schemes work for 2-dimensional filters, but their lookup times degrade quickly with each additional dimension. While there exist good hardware solutions, our new schemes are geared towards software implementation.We introduce a generic packet classification algorithm, called Tuple Space Search (TSS). Because real databases typically use only a small number of distinct field lengths, by mapping filters to tuples even a simple linear search of the tuple space can provide significant speedup over naive linear search over the filters. Each tuple is maintained as a hash table that can be searched in one memory access. We then introduce techniques for further refining the search of the tuple space, and demonstrate their effectiveness on some firewall databases. For example, a real database of 278 filters had a tuple space of 41 which our algorithm prunes to 11 tuples. Even as we increased the filter database size from 1K to 100K (using a random two-dimensional filter generation model), the number of tuples grew from 53 to only 186, and the pruned tuples only grew from 1 to 4. Our Pruned Tuple Space search is also the only scheme known to us that allows fast updates and fast search times. We also show a lower bound on the general tuple space search problem, and describe an optimal algorithm, called Rectangle Search, for two-dimensional filters.",
      "title": "Packet classification using tuple space search"
    },
    "0007d959867b6ec42c388ed1e9c47ab724b87c8e": {
      "paper_id": "0007d959867b6ec42c388ed1e9c47ab724b87c8e",
      "abstract": "We investigated the use of context-dependent deep neural network hidden Markov models, or CD-DNN-HMMs, to improve speech recognition performance for a better assessment of children English language learners (ELLs). The ELL data used in the present study was obtained from a large language assessment project administered in schools in a U.S. state. Our DNN-based speech recognition system, built using rectified linear units (ReLU), greatly outperformed recognition accuracy of Gaussian mixture models (GMM)-HMMs, even when the latter models were trained with eight times more data. Large improvement was observed for cases of noisy and/or unclear responses, which are common in ELL children speech. We further explored the use of content and manner-of-speaking features, derived from the speech recognizer output, for estimating spoken English proficiency levels. Experimental results show that the DNN-based recognition approach achieved 31% relative WER reduction when compared to GMM-HMMs. This further improved the quality of the extracted features and final spoken English proficiency scores, and increased overall automatic assessment performance to the human performance level, for various open-ended spoken language tasks.",
      "title": "Using deep neural networks to improve proficiency assessment for children English language learners"
    },
    "000a51e0d37b8a557318559e19905d0f07f9ea00": {
      "paper_id": "000a51e0d37b8a557318559e19905d0f07f9ea00",
      "abstract": "Grit has been presented as a higher-order personality trait that is highly predictive of both success and performance and distinct from other traits such as conscientiousness. This paper provides a meta-analytic review of the grit literature with a particular focus on the structure of grit and the relation between grit and performance, retention, conscientiousness, cognitive ability, and demographic variables. Our results based on 584 effect sizes from 88 independent samples representing 66,807 individuals indicate that the higher-order structure of grit is not confirmed, that grit is only moderately correlated with performance and retention, and that grit is very strongly correlated with conscientiousness. We also find that the perseverance of effort facet has significantly stronger criterion validities than the consistency of interest facet and that perseverance of effort explains variance in academic performance even after controlling for conscientiousness. In aggregate our results suggest that interventions designed to enhance grit may only have weak effects on performance and success, that the construct validity of grit is in question, and that the primary utility of the grit construct may lie in the perseverance facet.",
      "title": "GRIT META-ANALYSIS 1 Much Ado about Grit"
    },
    "000a734fb46e54b49e5c8bffd0aaed925d87f2a2": {
      "paper_id": "000a734fb46e54b49e5c8bffd0aaed925d87f2a2",
      "abstract": "Information Technology and Communications (ICT) is presented as the main element in order to achieve more efficient and sustainable city resource management, while making sure that the needs of the citizens to improve their quality of life are satisfied. A key element will be the creation of new systems that allow the acquisition of context information, automatically and transparently, in order to provide it to decision support systems. In this paper, we present a novel distributed system for obtaining, representing and providing the flow and movement of people in densely populated geographical areas. In order to accomplish these tasks, we propose the design of a smart sensor network based on RFID communication technologies, reliability patterns and integration techniques. Contrary to other proposals, this system represents a comprehensive solution that permits the acquisition of user information in a transparent and reliable way in a non-controlled and heterogeneous environment. This knowledge will be useful in moving towards the design of smart cities in which decision support on transport strategies, business evaluation or initiatives in the tourism sector will be supported by real relevant information. As a final result, a case study will be presented which will allow the validation of the proposal.",
      "title": "A Computational Architecture Based on RFID Sensors for Traceability in Smart Cities"
    },
    "000a7757b7a4e19500249e434c171cf431038136": {
      "paper_id": "000a7757b7a4e19500249e434c171cf431038136",
      "abstract": "This paper presents a cost-effective scalable quasi-cyclic LDPC (QC-LDPC) decoder architecture for non-volatile memory systems (NVMS). A re-arranged architecture is proposed to eliminate the first-in-first-out (FIFO) memory in conventional decoders, where the FIFO size is linearly proportional to the codeword size. The area reduction is 18.5% compared to the conventional decoder architecture. The scalable datapaths of the proposed decoder reduce the re-design cost and enable the flexibility of using QC-LDPC codes for NVMS. A prototyping decoder with maximum codeword size of 9280 bits is implemented in TSMC 90nm CMOS technology, and the core area is only 2.52mm2 at 138.8MHz.",
      "title": "Cost-effective scalable QC-LDPC decoder designs for non-volatile memory systems"
    },
    "000c009765a276d166fc67595e107a9bc44f230d": {
      "paper_id": "000c009765a276d166fc67595e107a9bc44f230d",
      "abstract": "The data of interest are assumed to be represented as N-dimensional real vectors, and these vectors are compressible in some linear basis B, implying that the signal can be reconstructed accurately using only a small number M Lt N of basis-function coefficients associated with B. Compressive sensing is a framework whereby one does not measure one of the aforementioned N-dimensional signals directly, but rather a set of related measurements, with the new measurements a linear combination of the original underlying N-dimensional signal. The number of required compressive-sensing measurements is typically much smaller than N, offering the potential to simplify the sensing system. Let f denote the unknown underlying N-dimensional signal, and g a vector of compressive-sensing measurements, then one may approximate f accurately by utilizing knowledge of the (under-determined) linear relationship between f and g, in addition to knowledge of the fact that f is compressible in B. In this paper we employ a Bayesian formalism for estimating the underlying signal f based on compressive-sensing measurements g. The proposed framework has the following properties: i) in addition to estimating the underlying signal f, \"error bars\" are also estimated, these giving a measure of confidence in the inverted signal; ii) using knowledge of the error bars, a principled means is provided for determining when a sufficient number of compressive-sensing measurements have been performed; iii) this setting lends itself naturally to a framework whereby the compressive sensing measurements are optimized adaptively and hence not determined randomly; and iv) the framework accounts for additive noise in the compressive-sensing measurements and provides an estimate of the noise variance. In this paper we present the underlying theory, an associated algorithm, example results, and provide comparisons to other compressive-sensing inversion algorithms in the literature.",
      "title": "Bayesian Compressive Sensing"
    },
    "000f0e74dba9d98755aa7411e9a626505b1a4e8d": {
      "paper_id": "000f0e74dba9d98755aa7411e9a626505b1a4e8d",
      "abstract": "Since the Intergovernmental Panel on Climate Change (IPCC) was formed in 1988, it has engaged a substantial proportion of those individuals with relevant scientific expertise in the process of forming reasonable judgments about the effects of aggregate human activity on the composition of the earth\u2019s atmosphere and about the resulting implications for global climate. It is now widely agreed that in concert with other so-called \u201cgreenhouse gases,\u201d carbon dioxide (CO2) released from the burning of fossil fuels for energy is causing the earth\u2019s climate to change. Over the last century, the concentration of CO2 in the atmosphere increased from about 300 to 375 parts per million by volume (ppmv), and global average surface temperature increased by 0.4 to 0.8 C. In the absence of policies designed to substantially reduce global emissions, scenarios developed by the IPCC indicate that CO2 concentrations will reach 550 to 1000 ppmv in 2100 and that global average surface temperature will increase by an additional 1.5 to 6 C (IPCC 2001a).",
      "title": "Decarbonizing the Global Energy System: Implications for Energy Technology and Security"
    },
    "000f90380d768a85e2316225854fc377c079b5c4": {
      "paper_id": "000f90380d768a85e2316225854fc377c079b5c4",
      "abstract": "Semantic image segmentation is an essential component of modern autonomous driving systems, as an accurate understanding of the surrounding scene is crucial to navigation and action planning. Current state-of-the-art approaches in semantic image segmentation rely on pre-trained networks that were initially developed for classifying images as a whole. While these networks exhibit outstanding recognition performance (i.e., what is visible?), they lack localization accuracy (i.e., where precisely is something located?). Therefore, additional processing steps have to be performed in order to obtain pixel-accurate segmentation masks at the full image resolution. To alleviate this problem we propose a novel ResNet-like architecture that exhibits strong localization and recognition performance. We combine multi-scale context with pixel-level accuracy by using two processing streams within our network: One stream carries information at the full image resolution, enabling precise adherence to segment boundaries. The other stream undergoes a sequence of pooling operations to obtain robust features for recognition. The two streams are coupled at the full image resolution using residuals. Without additional processing steps and without pre-training, our approach achieves an intersection-over-union score of 71.8% on the Cityscapes dataset.",
      "title": "Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes"
    },
    "000fc8d392b4b5bf2bb0f2db83cb2c021bcbd749": {
      "paper_id": "000fc8d392b4b5bf2bb0f2db83cb2c021bcbd749",
      "abstract": "In this paper, we investigate the possibility to automatically generate sports news from live text commentary scripts. As a preliminary study, we treat this task as a special kind of document summarization based on sentence extraction. We formulate the task in a supervised learning to rank framework, utilizing both traditional sentence features for generic document summarization and novelly designed task-specific features. To tackle the problem of local redundancy, we also propose a probabilistic sentence selection algorithm. Experiments on our collected data from football live commentary scripts and corresponding sports news demonstrate the feasibility of this task. Evaluation results show that our methods are indeed appropriate for this task, outperforming several baseline methods in different aspects.",
      "title": "Towards Constructing Sports News from Live Text Commentary"
    },
    "00113e81ef3a179d74d988d72329d306eae78525": {
      "paper_id": "00113e81ef3a179d74d988d72329d306eae78525",
      "abstract": "Blockchain is a distributed, transparent, immutable ledger. Consensus protocol forms the core of blockchain. They decide how a blockchain works. With the advent of new possibilities in blockchain technology, researchers are keen to find a well-optimized Byzantine fault tolerant consensus protocol. Creating a global consensus protocol or tailoring a cross-platform plug and play software application for implementation of various consensus protocols are ideas of huge interest. Stellar Consensus Protocol (SCP) is considered to be a global consensus protocol and promises to be Byzantine Fault Tolerant (BFT) by bringing with it the concept of quorum slices and federated byzantine fault tolerance. This consensus's working and its comparison with other protocols that were earlier proposed are analyzed here. Also, hyperledger an open-source project by Linux Foundation which includes implementing the concept of practical byzantine fault tolerance and also a platform where various other consensus protocols and blockchain applications can be deployed in a plug and play manner is also being discussed here. This paper focuses on analyzing these consensus protocols already proposed and their feasibility and efficiency in meeting the characteristics they propose to provide.",
      "title": "Survey of consensus protocols on blockchain applications"
    },
    "00126e91b056848ac68a4f3ed4737de25f094f65": {
      "paper_id": "00126e91b056848ac68a4f3ed4737de25f094f65",
      "abstract": "We present a model rationalizing the economic value of digital tokens for launching peer-to-peer platforms: By using the blockchain to transparently distribute tokens before the platform begins operation, a token sale overcomes later coordination failures between transaction counterparties during the platform operation. This result follows from forward induction reasoning, under which the costly and observable action of token acquisition credibly communicates the intent to participate on the platform. Our theoretical framework demonstrates the applications of digital tokens to entrepreneurship, including initial coin offerings (ICOs), and offers guidance for both practitioners and regulators.",
      "title": "Initial Coin Offerings and Platform Building"
    }
  }